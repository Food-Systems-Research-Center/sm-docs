[
  {
    "objectID": "pages/tables.html",
    "href": "pages/tables.html",
    "title": "Metric Tables",
    "section": "",
    "text": "This page contains a couple of tables for exploring the secondary data metrics that have been collected. The first contains the metadata and links to sources. The second contains the actual data. This one is harder to navigate, as it will take a few filters to get results that are comprehensible. You can also just download a .csv from the table to work with.",
    "crumbs": [
      "Secondary Data",
      "Tables"
    ]
  },
  {
    "objectID": "pages/tables.html#metadata-table",
    "href": "pages/tables.html#metadata-table",
    "title": "Metric Tables",
    "section": "1 Metadata Table",
    "text": "1 Metadata Table\nUsing the table:\n\nClick column headers to sort\nGlobal search in the top right, or column search in each header\nChange page length and page through results at the bottom\nUse the download button to download a .csv file of the filtered table\nClick the arrow on the left of each row for details, including a URL to the data source.\n\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools\n)\n\n# Load full metadata table\nmetadata_all &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\n# Pick out variables to display\nmetadata &lt;- metadata_all %&gt;% \n  select(\n    metric,\n    'Variable Name' = variable_name,\n    definition,\n    dimension,\n    index,\n    indicator,\n    units,\n    'Year' = latest_year, # Renaming latest year as year, not including og year\n    source,\n    scope,\n    resolution,\n    url\n) %&gt;% \n  setNames(c(str_to_title(names(.))))\n\n###\nhtmltools::browsable(\n  tagList(\n    \n    tags$div(\n      style = \"display: flex; gap: 16px; margin-bottom: 20px; justify-content: center;\",\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Show/hide more columns\"),\n        onclick = \"Reactable.setHiddenColumns('metadata_table', prevColumns =&gt; {\n          return prevColumns.length === 0 ? ['Definition', 'Scope', 'Resolution', 'Url'] : []\n        })\"\n      ),\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Download as CSV\"),\n        onclick = \"Reactable.downloadDataCSV('metadata_table', 'sustainability_metadata.csv')\"\n      )\n    ),\n    \n    reactable(\n      metadata,\n      sortable = TRUE,\n      resizable = TRUE,\n      filterable = TRUE,\n      searchable = TRUE,\n      pagination = TRUE,\n      bordered = TRUE,\n      wrap = TRUE,\n      rownames = FALSE,\n      onClick = 'select',\n      striped = TRUE,\n      pageSizeOptions = c(5, 10, 25, 50, 100),\n      defaultPageSize = 5,\n      showPageSizeOptions = TRUE,\n      highlight = TRUE,\n      style = list(fontSize = \"14px\"),\n      compact = TRUE,\n      fullWidth = TRUE,\n      columns = list(\n        Metric = colDef(\n          minWidth = 200,\n          sticky = 'left'\n        ),\n        'Variable Name' = colDef(\n          minWidth = 150\n        ),\n        Definition = colDef(\n          minWidth = 250\n        ),\n        'Latest Year' = colDef(minWidth = 75),\n        Source = colDef(minWidth = 250),\n        Scope = colDef(show = FALSE),\n        Resolution = colDef(show = FALSE),\n        Url = colDef(\n          minWidth = 300,\n          show = FALSE\n        )\n      ),\n      defaultColDef = colDef(minWidth = 100),\n      elementId = \"metadata_table\",\n      details = function(index) {\n        div(\n          style = \"padding: 15px; border: 1px solid #ddd; margin: 10px 0;\n             background-color: #E0EEEE; border-radius: 10px; border-color: black;\n             box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\",\n          \n          tags$h4(\n            strong(\"Details\"), \n          ),\n          tags$p(\n            strong('Metric Name: '), \n            as.character(metadata_all[index, 'metric']),\n          ),\n          tags$p(\n            strong('Variable Name: '), \n            as.character(metadata_all[index, 'variable_name']),\n          ),\n          tags$p(\n            strong('Definition: '), \n            as.character(metadata_all[index, 'definition']),\n          ),\n          tags$p(\n            strong('Source: '), \n            as.character(metadata_all[index, 'source'])\n          ),\n          tags$p(\n            strong('Latest Year: '), \n            as.character(metadata_all[index, 'latest_year'])\n          ),\n          tags$p(\n            strong('All Years (cleaned, wrangled, and included): '), \n            as.character(metadata_all[index, 'year'])\n          ),\n          tags$p(\n            strong('Updates: '), \n            str_to_title(as.character(metadata_all[index, 'updates']))\n          ),\n          tags$p(\n            strong('URL: '), \n            tags$a(\n              href = as.character(metadata_all[index, 'url']),\n              target = '_blank',\n              as.character(metadata_all[index, 'url'])\n            )\n          )\n        )\n      }\n    )\n  )\n)\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV",
    "crumbs": [
      "Secondary Data",
      "Tables"
    ]
  },
  {
    "objectID": "pages/tables.html#data-table",
    "href": "pages/tables.html#data-table",
    "title": "Metric Tables",
    "section": "2 Data Table",
    "text": "2 Data Table\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools\n)\n\n# Load metrics and metadata\nmetadata_all &lt;- readRDS('data/sm_data.rds')[['metadata']]\nmetrics &lt;- readRDS('data/sm_data.rds')[['metrics']]\nfips_key &lt;- readRDS('data/sm_data.rds')[['fips_key']]\n\n# Value formatting function based on units\nsource('dev/format_values.R')\n\n# Join with metadata and county fips codes\nmetrics &lt;- metrics %&gt;% \n  left_join(metadata_all, by = join_by('variable_name')) %&gt;% \n  left_join(fips_key, by = join_by('fips')) %&gt;% \n  mutate(county_name = ifelse(is.na(county_name), state_name, county_name)) %&gt;% \n  format_values() %&gt;% \n  select(\n    metric,\n    'Variable Name' = variable_name,\n    definition,\n    year = year.x,\n    Area = county_name,\n    units,\n    value\n  ) %&gt;% \n  setNames(c(str_to_title(names(.)))) %&gt;% \n  dplyr::filter(!is.na(Value))\n\n\n## Reactable table\nhtmltools::browsable(\n  tagList(\n    \n    tags$div(\n      style = \"display: flex; gap: 16px; margin-bottom: 20px; justify-content: center;\",\n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Download as CSV\"),\n        onclick = \"Reactable.downloadDataCSV('metrics_table', 'sustainability_metrics.csv')\"\n      )\n    ),\n    \n    reactable(\n      metrics,\n      sortable = TRUE,\n      resizable = TRUE,\n      filterable = TRUE,\n      searchable = TRUE,\n      pagination = TRUE,\n      bordered = TRUE,\n      wrap = TRUE,\n      rownames = FALSE,\n      onClick = 'select',\n      striped = TRUE,\n      pageSizeOptions = c(5, 10, 25, 50, 100),\n      defaultPageSize = 5,\n      showPageSizeOptions = TRUE,\n      highlight = TRUE,\n      style = list(fontSize = \"14px\"),\n      compact = TRUE,\n      fullWidth = TRUE,\n      columns = list(\n        Metric = colDef(\n          minWidth = 125,\n          sticky = 'left'\n        ),\n        'Variable Name' = colDef(\n          minWidth = 125\n        ),\n        Definition = colDef(\n          minWidth = 250\n        ),\n        Units = colDef(minWidth = 100),\n        'Year' = colDef(minWidth = 100)\n      ),\n      defaultColDef = colDef(minWidth = 100),\n      elementId = \"metrics_table\"\n    )\n  )\n)\n\n\n\n\n\nDownload as CSV",
    "crumbs": [
      "Secondary Data",
      "Tables"
    ]
  },
  {
    "objectID": "pages/refine_environment.html",
    "href": "pages/refine_environment.html",
    "title": "Environment Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the environment dimension. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting. A final set of indicators to incorporate into the next RFP is still in the works!",
    "crumbs": [
      "Framework",
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/refine_environment.html#indicator-progression",
    "href": "pages/refine_environment.html#indicator-progression",
    "title": "Environment Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework for the dimension as described in the Wiltshire et al.Â paper.\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/wiltshire_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/matrix_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Refinement Meeting\nFinally, the tentative set of indicators following the indicator refinement meeting on November 22nd, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/env_meeting_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)",
    "crumbs": [
      "Framework",
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/refine_environment.html#survey",
    "href": "pages/refine_environment.html#survey",
    "title": "Environment Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the economic indicator refinement meeting on November 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/env_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    ends_with('GROUP'),\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not'\n  )) %&gt;% \n  .[-c(1:2), ]\n\nto_df &lt;- function(x) {\n  x %&gt;%\n    str_replace_all('PFAS, PFOS', 'PFAS/PFOS') %&gt;% \n    str_replace_all('soil loss/', 'Soil loss/') %&gt;% \n    str_split(',') %&gt;% \n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n    arrange(desc(freq))\n}\n\nindi_out &lt;- map(dat[1:4], to_df)\nidx_out &lt;- map(dat[5:8], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\n\n\nCode\nggplot(graph_table, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so âMust Includeâ is worth 3 points, âProbably Includeâ is worth 2 points, âProbably Not Includeâ is worth 1 point, and âMust Not Includeâ is worth 0 points. Note that the last column is the sum of proportions of âMust Includeâ and âProbably Includeâ. You can sort, search, expand, or page through the table below.\n\n\nCalled from: clean_refine_surveys(tables = ind_tables, scope = \"indicator\", \n    cat_must = \"indi_must\", cat_probably = \"indi_probably\", n_votes = 5)\ndebug: props &lt;- tables %&gt;% imap(~.x %&gt;% mutate(cat = .y)) %&gt;% bind_rows() %&gt;% \n    select(-score)\ndebug: prop_prob_or_must_include &lt;- props %&gt;% filter(cat %in% c(cat_must, \n    cat_probably)) %&gt;% group_by(.[[scope]]) %&gt;% summarize(prop_include = sum(freq)/n_votes) %&gt;% \n    arrange(desc(prop_include))\ndebug: prop_must_include &lt;- props %&gt;% filter(cat == cat_must) %&gt;% group_by(.[[scope]]) %&gt;% \n    summarize(prop_must = sum(freq)/n_votes) %&gt;% arrange(desc(prop_must))\ndebug: scores &lt;- tables %&gt;% bind_rows() %&gt;% group_by(.[[scope]]) %&gt;% \n    summarize(score = sum(score, na.rm = TRUE)) %&gt;% arrange(desc(score))\ndebug: scores_table &lt;- scores %&gt;% full_join(prop_must_include) %&gt;% full_join(prop_prob_or_must_include) %&gt;% \n    arrange(desc(score)) %&gt;% mutate(across(where(is.numeric), \n    ~ifelse(is.na(.x), 0, .x)), across(c(3:4), ~format(round(.x, \n    2), nsmall = 2))) %&gt;% setNames(c(str_to_sentence(scope), \n    \"Score\", \"Proportion Must Include\", \"Proportion Must OR Probably Include\"))\n\n\ndebug: return(scores_table)\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nidx_tables &lt;- map2(idx_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    sort_key = ifelse(str_detect(index, 'Carbon'), 5e6, sort_key),\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\",\n      'must_not'\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\",\n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nThe indices are going through the same treatment as indicators above - scored from 3 to 0. Note here that the âCarbon ($ GHGs/nutrients)â index seems to be missing a vote. So, it only has 12 points, but the proportion of votes for âMust Includeâ is 1.\n\n\nCalled from: clean_refine_surveys(tables = idx_tables, scope = \"index\", cat_must = \"idx_must\", \n    cat_probably = \"idx_probably\", n_votes = 5)\ndebug: props &lt;- tables %&gt;% imap(~.x %&gt;% mutate(cat = .y)) %&gt;% bind_rows() %&gt;% \n    select(-score)\ndebug: prop_prob_or_must_include &lt;- props %&gt;% filter(cat %in% c(cat_must, \n    cat_probably)) %&gt;% group_by(.[[scope]]) %&gt;% summarize(prop_include = sum(freq)/n_votes) %&gt;% \n    arrange(desc(prop_include))\ndebug: prop_must_include &lt;- props %&gt;% filter(cat == cat_must) %&gt;% group_by(.[[scope]]) %&gt;% \n    summarize(prop_must = sum(freq)/n_votes) %&gt;% arrange(desc(prop_must))\ndebug: scores &lt;- tables %&gt;% bind_rows() %&gt;% group_by(.[[scope]]) %&gt;% \n    summarize(score = sum(score, na.rm = TRUE)) %&gt;% arrange(desc(score))\ndebug: scores_table &lt;- scores %&gt;% full_join(prop_must_include) %&gt;% full_join(prop_prob_or_must_include) %&gt;% \n    arrange(desc(score)) %&gt;% mutate(across(where(is.numeric), \n    ~ifelse(is.na(.x), 0, .x)), across(c(3:4), ~format(round(.x, \n    2), nsmall = 2))) %&gt;% setNames(c(str_to_sentence(scope), \n    \"Score\", \"Proportion Must Include\", \"Proportion Must OR Probably Include\"))\n\n\ndebug: return(scores_table)",
    "crumbs": [
      "Framework",
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/refined_framework.html",
    "href": "pages/refined_framework.html",
    "title": "Refined Secondary Data Framework",
    "section": "",
    "text": "This page shows the partially refined framework as it stands after two dimension meetings: economics and environment. It also includes a selection of secondary data metrics to match those indicators. We have collected around 1500 metrics so far, although many of those are fluff. Effectively, we have around 250 meaningful metrics. Here, we are using a selection of 95 of them to make a preliminary framework for preliminary analyses. This is more than we have been planning for the refined framework, which will give us a chance to see how aggregate scores change with different combinations of metrics and under different methods of aggregation.\nThere are still a few gaps I would have liked to fill by now:\nAnother issue that is on our radar is that I have been trying to collect data at the county level whenever possible, giving us a sample of ~68 counties (depending on the year) with which to run PCA and exploratory analyses. Unfortunately, some crucial data is only available at the state level. We can aggregate back up to the state if need be, but then we are left with only 6 units of analysis. The longer-term solution I think will be to start around back again and collect all data at the state level across all states. This will let us use all the metrics at our disposal while still having a big enough sample size to do some empirical work.",
    "crumbs": [
      "Secondary Data Rework",
      "Framework"
    ]
  },
  {
    "objectID": "pages/refined_framework.html#partially-refined-framework",
    "href": "pages/refined_framework.html#partially-refined-framework",
    "title": "Refined Secondary Data Framework",
    "section": "1 Partially Refined Framework",
    "text": "1 Partially Refined Framework\nHere is the framework with a selection of secondary metrics, split into each dimension for ease of reading.\n\n1.1 Environment\nWe are missing any soils data in the environment dimension. We have a reasonable spread of metrics for carbon, ghg, and nutrients, although the stocks are only coming from the TreeMap 2016 dataset. Belowground carbon would be great to add here if I can find it. Fluxes are from a great EPA dataset, filtered down to agricultural emissions only. Here, I have aggregated by the type of emission, but we also have very specific breakdowns. Species and habitat is not great - I have more rare species and habitat layers, but I havenât sorted out how to make sense of them at this scale.\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'environment',\n  include_metrics = TRUE,\n  y_limits = c(-2, 3.25),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Economics\nIt has definitely been easier to find economics data than other dimensions. Worth noting here is that the access to land indicator is not ideal. Iâm using value and farm size as a proxy for access. Use of crop in surance is also a proxy, since I could not find direct insurance claim data from FSA. So for now, we are just using the presidential and ag secretary declarations of disasters that allow for insurance claims.\n\n\nCode\nget_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'economics',\n  include_metrics = TRUE,\n  y_limits = c(-1.5, 3.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Production\nThe production dimension looks slim at first glance, but is better than it looks. Agricultural exports are a pretty robust dataset at the state level from ERS, although the import data only includes the values of the top five imports for each state - not ideal. Crop diversity is based on the Cropland Data Layer, a USDA NASS estimate of crop types, which I used to calculate Shannon diversity at the county level. The rest of the metrics come from NASS. Production is an area in which I feel better about using NASS data than usual.\n\n\nCode\nget_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'production',\n  include_metrics = TRUE,\n  y_limits = c(-1.75, 3),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.4 Health\nThe Food Environment Atlas has lots of data on access and nutrition, which accounts for much of the food security data, along with NASS. The âfood security tbdâ index is just pulled out because this is already an index of food security that encompasses access and affordability. I will use this to explore how redundant they are. I also threw in a slew of metrics for physical health. The Factor and Outcome Z-Scores are already composite indices from the County Health Rankings, a great dataset. This is another area I want to explore with PCA to see how much unique variation these little metrics bring compared to a composite.\n\n\nCode\nget_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'health',\n  include_metrics = TRUE,\n  y_limits = c(-1.9, 3),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.5 Social\nThe social dimension is admittedly slim, but it could have been a lot worse. The County Health Rankings dataset brings a few useful metrics here, like social associations and disconnected youth. Census participation and voter turnout are proxies for participatory governance in food systems - I canât imagine finding something much more specific than that at this point. I also plan on replacing mean producer age with a diversity index for age structure among producers.\n\n\nCode\nget_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'social',\n  include_metrics = TRUE,\n  y_limits = c(-1.9, 3),\n  palette = \"ggthemes::stata_s2color\"\n)",
    "crumbs": [
      "Secondary Data Rework",
      "Framework"
    ]
  },
  {
    "objectID": "pages/refined_framework.html#metadata",
    "href": "pages/refined_framework.html#metadata",
    "title": "Refined Secondary Data Framework",
    "section": "2 Metadata",
    "text": "2 Metadata\nHere is the metadata for this refined set of secondary metrics.\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools\n)\n\n# Load full metadata table\nmetadata &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\n# Pull var names out of frame df, extract from all metadata\nvars &lt;- frame$variable_name\nmetadata &lt;- filter(metadata, variable_name %in% vars)\n\n# Pick out variables to display\nmeta &lt;- metadata %&gt;% \n  select(\n    metric,\n    'Variable Name' = variable_name,\n    definition,\n    dimension,\n    index,\n    indicator,\n    units,\n    'Year' = latest_year, # Renaming latest year as year, not including og year\n    source,\n    scope,\n    resolution,\n    url\n) %&gt;% \n  setNames(c(str_to_title(names(.))))\n\n###\nhtmltools::browsable(\n  tagList(\n    \n    tags$div(\n      style = \"display: flex; gap: 16px; margin-bottom: 20px; justify-content: center;\",\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Show/hide more columns\"),\n        onclick = \"Reactable.setHiddenColumns('metadata_table', prevColumns =&gt; {\n          return prevColumns.length === 0 ? ['Definition', 'Scope', 'Resolution', 'Url'] : []\n        })\"\n      ),\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Download as CSV\"),\n        onclick = \"Reactable.downloadDataCSV('metadata_table', 'sustainability_metadata.csv')\"\n      )\n    ),\n    \n    reactable(\n      meta,\n      sortable = TRUE,\n      resizable = TRUE,\n      filterable = TRUE,\n      searchable = TRUE,\n      pagination = TRUE,\n      bordered = TRUE,\n      wrap = TRUE,\n      rownames = FALSE,\n      onClick = 'select',\n      striped = TRUE,\n      pageSizeOptions = c(5, 10, 25, 50, 100),\n      defaultPageSize = 5,\n      showPageSizeOptions = TRUE,\n      highlight = TRUE,\n      style = list(fontSize = \"14px\"),\n      compact = TRUE,\n      fullWidth = TRUE,\n      columns = list(\n        Metric = colDef(\n          minWidth = 200,\n          sticky = 'left'\n        ),\n        'Variable Name' = colDef(\n          minWidth = 150\n        ),\n        Definition = colDef(\n          minWidth = 250\n        ),\n        'Latest Year' = colDef(minWidth = 75),\n        Source = colDef(minWidth = 250),\n        Scope = colDef(show = FALSE),\n        Resolution = colDef(show = FALSE),\n        Url = colDef(\n          minWidth = 300,\n          show = FALSE\n        )\n      ),\n      defaultColDef = colDef(minWidth = 100),\n      elementId = \"metadata_table\",\n      details = function(index) {\n        div(\n          style = \"padding: 15px; border: 1px solid #ddd; margin: 10px 0;\n             background-color: #E0EEEE; border-radius: 10px; border-color: black;\n             box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\",\n          \n          tags$h4(\n            strong(\"Details\"), \n          ),\n          tags$p(\n            strong('Metric Name: '), \n            as.character(metadata[index, 'metric']),\n          ),\n          tags$p(\n            strong('Variable Name: '), \n            as.character(metadata[index, 'variable_name']),\n          ),\n          tags$p(\n            strong('Definition: '), \n            as.character(metadata[index, 'definition']),\n          ),\n          tags$p(\n            strong('Source: '), \n            as.character(metadata[index, 'source'])\n          ),\n          tags$p(\n            strong('Latest Year: '), \n            as.character(metadata[index, 'latest_year'])\n          ),\n          tags$p(\n            strong('All Years (cleaned, wrangled, and included): '), \n            as.character(metadata[index, 'year'])\n          ),\n          tags$p(\n            strong('Updates: '), \n            str_to_title(as.character(metadata[index, 'updates']))\n          ),\n          tags$p(\n            strong('URL: '), \n            tags$a(\n              href = as.character(metadata[index, 'url']),\n              target = '_blank',\n              as.character(metadata[index, 'url'])\n            )\n          )\n        )\n      }\n    )\n  )\n)\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV",
    "crumbs": [
      "Secondary Data Rework",
      "Framework"
    ]
  },
  {
    "objectID": "pages/framework.html",
    "href": "pages/framework.html",
    "title": "Framework",
    "section": "",
    "text": "Just including some visualizations of the framework here for now. These include all the indicators currently in the matrix.",
    "crumbs": [
      "Framework",
      "Overview"
    ]
  },
  {
    "objectID": "pages/framework.html#radial-plot",
    "href": "pages/framework.html#radial-plot",
    "title": "Framework",
    "section": "1 Radial Plot",
    "text": "1 Radial Plot\n\n\nCode\n## Load packages\npacman::p_load(\n  ggraph,\n  igraph,\n  dplyr,\n  RColorBrewer,\n  viridisLite\n)\n\n\n## Load data and add an origin level\ndat &lt;- readRDS('data/trees/tree_dat.rds') %&gt;% \n  mutate(Framework = 'Sustainability') %&gt;% \n  select(Framework, Dimension:Indicator)\n\n\n## Make edges\n# include groupings by dimension, then combine them\nedges &lt;- list()\nedges$sm_dim &lt;- dat %&gt;% \n  select(Framework, Dimension) %&gt;% \n  unique() %&gt;% \n  rename(from = Framework, to = Dimension) %&gt;% \n  mutate(group = to)\nedges$dim_ind &lt;- dat %&gt;% \n  select(Dimension, Index) %&gt;% \n  unique() %&gt;% \n  rename(from = Dimension, to = Index) %&gt;% \n  mutate(group = from)\nedges$ind_ind &lt;- dat %&gt;% \n  select(Index, Indicator) %&gt;% \n  unique() %&gt;% \n  rename(from = Index, to = Indicator) %&gt;% \n  mutate(group = edges$dim_ind$from[match(.$from, edges$dim_ind$to)])\nedges &lt;- bind_rows(edges)\n\n\n## Make vertices\n# Each line is a single vertex (dimension, index, or indicator)\n# We are just giving them random values to control point size for now\nvertices = data.frame(\n  name = unique(c(as.character(edges$from), as.character(edges$to))) , \n  value = runif(nrow(edges) + 1)\n) \n\n# Add the dimension groupings to the vertices as well\nvertices$group = edges$group[match(vertices$name, edges$to)]\n\n# Calculate the angles to arrange indicator labels\nvertices$id = NA\nmyleaves = which(is.na(match(vertices$name, edges$from)))\nnleaves = length(myleaves)\nvertices$id[myleaves] = seq(1:nleaves)\nvertices$angle = 90 - 360 * vertices$id / nleaves\n\n# Calculate alignment of indicator labels\nvertices$hjust &lt;- ifelse(vertices$angle &lt; -90, 1, 0)\n\n# Flip label angles around 180 degrees if they are facing the wrong way\nvertices$angle &lt;- ifelse(vertices$angle &lt; -90, vertices$angle + 180, vertices$angle)\n\n\n## Create graph\n# Make ggraph object from edges and vertices\ngraph &lt;- graph_from_data_frame(edges, vertices = vertices)\n\n# Plot the graph\nggraph(graph, layout = 'dendrogram', circular = TRUE) +\n  \n  # Color edges by dimension\n  geom_edge_diagonal(aes(color = group), width = 0.5) +\n  \n  # Create text for indicators using angles, hjust, and dimension groupings\n  geom_node_text(\n    aes(\n      x = x * 1.04,\n      y = y * 1.04,\n      filter = leaf,\n      label = name,\n      angle = angle,\n      hjust = hjust,\n      colour = group\n    ),\n    size = 2.7,\n    alpha = 1\n  ) +\n  \n  # Make the points for indicators based on dimension groupings\n  # geom_node_point(aes(\n  #   filter = leaf,\n  #   x = x * 1.07,\n  #   y = y * 1.07,\n  #   colour = group,\n  #   size = value,\n  #   alpha = 0.2\n  # )) +\n  \n  # Label the dimensions within the graph\n  geom_node_label(\n    aes(label = ifelse(name == group, name, NA)),\n    label.padding = unit(0.2, \"lines\"),\n    label.r = unit(0.3, \"lines\"),\n    label.size = 0.1,\n    size = 3\n  ) +\n  \n  # Various formatting options\n  scale_colour_manual(values = brewer.pal(5, 'Set1')) +\n  scale_edge_color_manual(values = brewer.pal(5, 'Set1')) +\n  scale_size_continuous(range = c(0.1, 7)) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.margin = unit(c(0, 0, 0, 0), \"cm\")\n  ) +\n  expand_limits(x = c(-2, 2), y = c(-2, 2))\n\n\n\n\n\nRadial dendrogram of Sustainability Metrics framework",
    "crumbs": [
      "Framework",
      "Overview"
    ]
  },
  {
    "objectID": "pages/framework.html#cladogram",
    "href": "pages/framework.html#cladogram",
    "title": "Framework",
    "section": "2 Cladogram",
    "text": "2 Cladogram\nA slightly more readable version of the diagram above.\n\n\nCode\npacman::p_load(\n  ggtree,\n  dplyr,\n  ape,\n  data.tree,\n  viridisLite,\n  stringr\n)\n\n## Load data and add an origin level\ndat &lt;- readRDS('data/trees/tree_dat.rds') %&gt;% \n  mutate(Framework = 'Sustainability') %&gt;% \n  select(Framework, Dimension:Indicator) %&gt;% \n  mutate(across(\n    everything(), \n    ~ str_trim(str_replace_all(., ';|%|/|\\\\.|\\\"|,|\\\\(|\\\\)', '_'))\n  ))\n\ndat$pathString &lt;- paste(\n  dat$Framework,\n  dat$Dimension,\n  dat$Index,\n  dat$Indicator,\n  sep = '/'\n)\ntree &lt;- as.Node(dat)\n\n# Convert the data.tree structure to Newick format\ntree_newick &lt;- ToNewick(tree)\n\n# Read the Newick tree into ape\nphylo_tree &lt;- read.tree(text = tree_newick)\n\n# Make all edge lengths 1\nphylo_tree$edge.length &lt;- rep(1, length(phylo_tree$edge.length))\n\n# Add a space to end of node labels so it isn't cut off\nphylo_tree$node.label &lt;- paste0(phylo_tree$node.label, ' ')\n\n# Plot it\nplot(\n  phylo_tree, \n  type = 'c',\n  cex = 0.75,\n  edge.width = 2,\n  show.tip.label = TRUE,\n  label.offset = 0,\n  no.margin = TRUE,\n  tip.color = 'black',\n  edge.color = viridis(181),\n  x.lim = c(-0.1, 5)\n)\n\nnodelabels(\n  phylo_tree$node.label,\n  cex = 0.8,\n  bg = 'white'\n)\n\n\n\n\n\nCladogram of Sustainability Metrics framework",
    "crumbs": [
      "Framework",
      "Overview"
    ]
  },
  {
    "objectID": "pages/data_environment_overview.html",
    "href": "pages/data_environment_overview.html",
    "title": "Environment: Overview",
    "section": "",
    "text": "The first plot shows all the environment indicators from both the current studies and the original framework in the y-axis. Purple indicates that the indicator is only being used in the current studies, orange that it is only included in the Wiltshire framework, and green that the indicator is used in both the framework and current studies.\nThe x-axis shows the number of secondary data metrics that have been collected to represent those indicators. You can see that there are some indicators for which there exist many data, but many indicators for which I have found little to represent them.\nNASS figures are used to cover on-farm water use, energy efficiency, and acres in conservation practices. I used the National Aquatic Resource Surveys aggregated at the state level to measure water quality. Land use diversity is pretty well represented by Multi-Resolution Land Characteristics LULC layers, which I also aggregated at the county level. Greenhouse gas emissions come from EPA figures by state, broken down by economic sector. Finally, the USFS TreeMap dataset accounts for aboveground biomass and would do reasonably well in tree vigor. There is more to pull out here than I have so far.\nOtherwise, if anyone has ideas for secondary datasets to cover the rest of the indicators, please do let me know.\nCode\npacman::p_load(\n  dplyr,\n  ggplot2,\n  stringr,\n  plotly,\n  RColorBrewer\n)\n\n## Load data for tree and metrics\nenv_tree &lt;- readRDS('data/trees/env_tree.rds')\n\nmeta &lt;- readRDS('data/sm_data.rds')[['metadata']] %&gt;% \n  filter(dimension == 'environment')\n\n# Format to match Wiltshire framework\nmeta &lt;- meta %&gt;% \n  mutate(\n    indicator = str_to_sentence(indicator),\n    indicator = case_when(\n      str_detect(indicator, '^Above') ~ 'Aboveground biomass',\n      str_detect(indicator, '^Water') ~ 'Water use / irrigation efficiency',\n      TRUE ~ indicator\n    )\n  ) \n\n# Counts of secondary data metrics\ncounts &lt;- meta %&gt;% \n  group_by(indicator) %&gt;% \n  dplyr::summarize(count = n())\n\n# Join to Wiltshire framework\ncolors &lt;- RColorBrewer::brewer.pal(n = 3, name = 'Dark2')\ndat &lt;- full_join(env_tree, counts, by = join_by(Indicator == indicator)) %&gt;% \n  mutate(\n    count = ifelse(is.na(count), 0, count),\n    label_color = case_when(\n      Use == 'both' ~ colors[1],\n      Use == 'wiltshire_only' ~ colors[2],\n      Use == 'current_only' ~ colors[3]\n    )\n  )\n\n# Plot\ndat %&gt;%\n  ggplot(aes(x = Indicator, y = count)) +\n  geom_col(\n    color = 'black',\n    fill = 'grey'\n  ) +\n  geom_point(\n    data = dat,\n    aes(x = 1, y = 1, color = Use),\n    inherit.aes = FALSE,\n    alpha = 0,\n    size = -1\n  ) +\n  scale_color_manual(\n    name = \"Indicator Use:\",\n    values = c(\n      \"both\" = colors[1],\n      \"current_only\" = colors[3],\n      \"wiltshire_only\" = colors[2]\n    ),\n    labels = c(\n      'Both',\n      'Current Only',\n      'Framework Only'\n    )\n  ) +\n  theme_classic() +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.text.y = element_text(color = dat$label_color),\n    axis.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    legend.title = element_text(size = 12),\n    legend.position = \"bottom\",\n    plot.margin = margin(t = 10, r = 75, b = 10, l = 10)\n  ) +\n  guides(\n    color = guide_legend(override.aes = list(size = 4, alpha = 1))\n  ) +\n  coord_flip() +\n  labs(y = 'Secondary Data Count')\n\n\n\n\n\nBar Plot of Indicators",
    "crumbs": [
      "Secondary Data",
      "Environment",
      "Overview"
    ]
  },
  {
    "objectID": "pages/data_environment_overview.html#distribution-plots",
    "href": "pages/data_environment_overview.html#distribution-plots",
    "title": "Environment: Overview",
    "section": "1 Distribution Plots",
    "text": "1 Distribution Plots\n\n1.1 By County\nNote that while most of the available secondary data is at the county level, the environment dimension includes a fair amount at the state level as well. This includes greenhouse gas emissions and water quality surveys. For now, Iâll just show these separately, but some creative aggregation will have to happen eventually.\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr\n)\nsource('dev/data_pipeline_functions.R')\nsource('dev/filter_fips.R')\nmetrics &lt;- readRDS('data/sm_data.rds')[['metrics']]\nmetadata &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\n# Use metadata to get help filter by dimension\nenv_meta &lt;- metadata %&gt;%\n  filter(dimension == 'environment')\n\n# Filter to economics dimension\nenv_metrics &lt;- metrics %&gt;%\n  filter(variable_name %in% env_meta$variable_name)\n\n# env_metrics$variable_name %&gt;% unique\n# get_str(env_metrics)\n\n# Filter to latest year and new (post-2024) counties\n# And pivot wider so it is easier to get correlations\nenv_county &lt;- env_metrics %&gt;%\n  filter_fips(scope = 'counties') %&gt;%\n  get_latest_year() %&gt;%\n  select(fips, variable_name, value) %&gt;%\n  mutate(variable_name = str_split_i(variable_name, '_', 1)) %&gt;%\n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;%\n  unnest(!fips) %&gt;%\n  mutate(across(c(2:last_col()), as.numeric))\n\n# Save temp file for use in analysis script\nsaveRDS(env_county, 'data/temp/env_county.rds')\n\n## Plot\nplots &lt;- map(names(env_county)[-1], \\(var){\n  if (is.character(env_county[[var]])) {\n    env_county %&gt;%\n      ggplot(aes(x = !!sym(var))) +\n      geom_bar(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else if (is.numeric(env_county[[var]])) {\n    env_county %&gt;%\n      ggplot(aes(x = !!sym(var))) +\n      geom_density(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else {\n    return(NULL)\n  }\n})\n\n\n# Arrange them in 4 columns\nggarrange(\n  plotlist = plots,\n  ncol = 4,\n  nrow = 11\n)\n\n\n$`1`\n\n\n\n\n\nDistributions of economic metrics at the county level.\n\n\n\n\n\n$`2`\n\n\n\n\n\nDistributions of economic metrics at the county level.\n\n\n\n\n\nattr(,\"class\")\n[1] \"list\"      \"ggarrange\"\n\n\n\n\n1.2 By State\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr,\n  stringr\n)\n\nconflicted::conflicts_prefer(\n  dplyr::select(),\n  dplyr::mutate(),\n  dplyr::summarize(),\n  dplyr::rename(),\n  .quiet = TRUE\n)\n\nstate_codes &lt;- readRDS('data/sm_data.rds')[['fips_key']] %&gt;%\n  dplyr::select(fips, state_code)\n\nenv_state &lt;- env_metrics %&gt;%\n  filter_fips(scope = 'states') %&gt;%\n  get_latest_year() %&gt;%\n  dplyr::select(fips, variable_name, value) %&gt;%\n  dplyr::mutate(variable_name = stringr::str_split_i(variable_name, '_', 1)) %&gt;% \n  tidyr::complete(fips, variable_name) %&gt;% \n  tidyr::pivot_wider(\n    id_cols = fips,\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;% \n  dplyr::left_join(state_codes, by = 'fips') %&gt;% \n  dplyr::mutate(across(!c(fips, state_code), as.numeric))\n\n# Save temp data file for use in analysis script\nsaveRDS(env_state, 'data/temp/env_state.rds')\n\n# Variables to map. Take out some that didn't come through well.\nvars &lt;- names(env_state)[-1] %&gt;%\n  stringr::str_subset(\n    'lakesAcidCond|lakesCylsperEpaCond|lakesMicxEpaCond|state_code|waterIrrSrcOffFarmExp|waterIrrReclaimedAcreFt|waterIrrReclaimedOpenAcres',\n    negate = TRUE\n  ) %&gt;% \n  stringr::str_subset('^CH4(?!FromAg)|^N2O(?!FromAg)|^CO2(?!FromAg)|^SubSector', negate = TRUE)\n\n## Plot\nstate_plots &lt;- purrr::map(vars, \\(var){\n  env_state %&gt;%\n    ggplot(aes(y = !!sym(var), x = state_code, color = state_code)) +\n    geom_point(\n      alpha = 0.5,\n      size = 3\n    ) +\n    theme_classic() +\n    theme(\n      plot.margin = unit(c(rep(0.5, 4)), 'cm'),\n      legend.position = 'none'\n    ) +\n    labs(\n      x = 'State'\n    )\n})\n\n# Arrange them in 4 columns\nggpubr::ggarrange(\n  plotlist = state_plots,\n  ncol = 4,\n  nrow = 22\n)\n\n\n\n\n\nDistributions of environmental variables at state level",
    "crumbs": [
      "Secondary Data",
      "Environment",
      "Overview"
    ]
  },
  {
    "objectID": "pages/data_environment_overview.html#bivariate-plots",
    "href": "pages/data_environment_overview.html#bivariate-plots",
    "title": "Environment: Overview",
    "section": "2 Bivariate Plots",
    "text": "2 Bivariate Plots\nUsing a selection of variables at the county level. The variable names are a bit hard to fit in here, but from left to right across the top they are LULC diversity, mean live above-ground forest biomass, conservation income per farm, conservatino easement acres per farm, conservation tillage: no-till acres per farm, conservation tillage: excluding no-till acres per farm, and cover cropping: excluding CRP acres per farm.\n\n\nCode\npacman::p_load(\n  GGally\n)\n\n# Neat function for mapping colors to ggpairs plots\n# https://stackoverflow.com/questions/45873483/ggpairs-plot-with-heatmap-of-correlation-values\nmap_colors &lt;- function(data,\n                       mapping,\n                       method = \"p\",\n                       use = \"pairwise\",\n                       ...) {\n  # grab data\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n\n  # calculate correlation\n  corr &lt;- cor(x, y, method = method, use = use)\n  colFn &lt;- colorRampPalette(c(\"blue\", \"white\", \"red\"), interpolate = 'spline')\n  fill &lt;- colFn(100)[findInterval(corr, seq(-1, 1, length = 100))]\n\n  # correlation plot\n  ggally_cor(data = data, mapping = mapping, color = 'black', ...) +\n    theme_void() +\n    theme(panel.background = element_rect(fill = fill))\n}\n\nlower_function &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(color = \"blue\", fill = \"grey\", ...) +\n    theme_bw()\n}\n\n# Rename variables to be shorter\nenv_county %&gt;%\n  select(\n    LULC = lulcDiversity,\n    # Biomass = meanAboveGrndForBiomass,\n    consIncomePF,\n    consEasementAcresPF,\n    consTillNoTillAcresPF,\n    consTillExclNoTillAcresPF,\n    coverCropExclCrpAcresPF\n  ) %&gt;%\n  ggpairs(\n    upper = list(continuous = map_colors),\n    lower = list(continuous = lower_function),\n    axisLabels = 'show'\n  ) +\n  theme(\n    strip.text = element_text(size = 5),\n    axis.text = element_text(size = 5),\n    legend.text = element_text(size = 5)\n  )\n\n\n\n\n\n\n\n\n\nIt looks like there are a few non-linear relationships, conservation income per farm in particular, but for the most part, linear relationships do a decent job here.",
    "crumbs": [
      "Secondary Data",
      "Environment",
      "Overview"
    ]
  },
  {
    "objectID": "pages/data_environment_overview.html#sec-correlations",
    "href": "pages/data_environment_overview.html#sec-correlations",
    "title": "Environment: Overview",
    "section": "3 Correlations",
    "text": "3 Correlations\nOnly showing correlations by county because we donât have enough observations to run it by state.\n\n\nCode\npacman::p_load(\n  dplyr,\n  tidyr,\n  tibble,\n  stringr,\n  purrr,\n  tidyr,\n  ggplot2,\n  plotly,\n  reshape,\n  Hmisc,\n  viridisLite\n)\n\n# get_str(env_county)\n\ncor &lt;- env_county %&gt;%\n  select(-fips) %&gt;%\n  as.matrix() %&gt;%\n  rcorr()\n\n# Melt correlation values and rename columns\ncor_r &lt;- melt(cor$r) %&gt;%\n  setNames(c('var_1', 'var_2', 'value'))\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Make heatmap with custom text aesthetic for tooltip\nplot &lt;- cor_r %&gt;%\n  ggplot(aes(var_1, var_2, fill = value, text = paste0(\n    'Var 1: ', var_1, '\\n',\n    'Var 2: ', var_2, '\\n',\n    'Correlation: ', format(round(value, 3), nsmall = 3), '\\n',\n    'P-Value: ', format(round(p.value, 3), nsmall = 3)\n  ))) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  theme(axis.text.x = element_text(hjust = 1, angle = 45)) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = 'Correlation'\n  )\n\n# Convert to interactive plotly figure with text tooltip\nggplotly(\n  plot,\n  tooltip = 'text',\n  width = 1000,\n  height = 800\n)\n\n\n\n\nInteractive correlation plot of metrics by county",
    "crumbs": [
      "Secondary Data",
      "Environment",
      "Overview"
    ]
  },
  {
    "objectID": "pages/data_environment_analysis.html",
    "href": "pages/data_environment_analysis.html",
    "title": "Environment: Analysis",
    "section": "",
    "text": "This section will serve as a first pass at using some methods in the literature to aggregate metrics. I should say at the start that we have a pretty narrow selection of metrics to work with so far, which do not do a great job at capturing the breadth of the dimension. Iâm also working with just the county-level data here. This provides some opportunities to use data-driven analyses like PCA, but it is worth noting that these will not get us to the holistic, system-wide measurements of sustainability we are after without including some normative judgments as to how to combine geographic areas as well as our five dimensions. So, letâs just go through the motions here, see how the process unfolds, and note anything worth digging into more down the road.",
    "crumbs": [
      "Secondary Data",
      "Environment",
      "Analysis"
    ]
  },
  {
    "objectID": "pages/data_environment_analysis.html#imputation",
    "href": "pages/data_environment_analysis.html#imputation",
    "title": "Environment: Analysis",
    "section": "1 Imputation",
    "text": "1 Imputation\nPCA requires complete data, so we either have to impute, delete, or use PPCA. Iâm choosing to impute with missing forest here as it is pretty good at handling MAR and non-linear data, but PPCA is certainly worth exploring.\n\n\nCode\npacman::p_load(\n  missForest,\n  tibble\n)\nsource('dev/filter_fips.R')\nenv_county &lt;- readRDS('data/temp/env_county.rds')\n\n# Wrangle dataset. Need all numeric vars or factor vars. And can't be tibble\n# Also removing character vars - can't use these in PCA\n# Using old Connecticut counties - some lulc data is missing for them though\ndat &lt;- env_county %&gt;%\n  filter_fips('old') %&gt;%\n  select(fips, where(is.numeric)) %&gt;%\n  column_to_rownames('fips') %&gt;%\n  as.data.frame()\n# get_str(dat)\n# skimr::skim(dat)\n\n# Remove variables with most missing data - too much to impute.\n# Also remove the proportional LULC values - keeping diversity though\ndat &lt;- dat %&gt;%\n  select(-matches('consIncome'), -matches('^lulcProp'))\n\n# Impute missing variables\nset.seed(42)\nmf_out &lt;- dat %&gt;%\n  missForest(\n    ntree = 200,\n    mtry = 10,\n    verbose = FALSE,\n    variablewise = FALSE\n  )\n\n# Save imputed dataset\nimp &lt;- mf_out$ximp\n\n# Print OOB\nmf_out$OOBerror\n\n\n      NRMSE \n0.001503807",
    "crumbs": [
      "Secondary Data",
      "Environment",
      "Analysis"
    ]
  },
  {
    "objectID": "pages/data_environment_analysis.html#standardization",
    "href": "pages/data_environment_analysis.html#standardization",
    "title": "Environment: Analysis",
    "section": "2 Standardization",
    "text": "2 Standardization\nCentering and scaling to give every variable a mean of 0 and SD of 1.\n\n\nCode\ndat &lt;- map_dfc(imp, ~ scale(.x, center = TRUE, scale = TRUE))\n\n\nNow that we have standardized variables, we have to make normative decisions about what constitutes a good or bad value. This will certainly be a collaborative process where we seek input from teams to come to some kind of consensus once we have primary data. But until then, Iâm going to make some heroic assumptions that LULC diversity is good, above ground forest biomass is good, conservation practices and easements are good, and fertilizer expenses are bad. Open to thoughts here as always.\nWith that, we can recode our normalized variables accordingly.\n\n\nCode\nnormed &lt;- dat %&gt;%\n  mutate(across(c(matches('^fert')), ~ -.x))",
    "crumbs": [
      "Secondary Data",
      "Environment",
      "Analysis"
    ]
  },
  {
    "objectID": "pages/data_environment_analysis.html#component-extraction",
    "href": "pages/data_environment_analysis.html#component-extraction",
    "title": "Environment: Analysis",
    "section": "3 Component Extraction",
    "text": "3 Component Extraction\nDetermine the number of components to extract using a few tools: very simple structure (VSS), Velicerâs minimum average partial (MAP) test, parallel analysis, and a scree plot.\n\n\nCode\npacman::p_load(\n  psych\n)\nVSS(normed)\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.7  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.89  with  3  factors\n\nThe Velicer MAP achieves a minimum of 0.05  with  7  factors \nBIC achieves a minimum of  -856.8  with  6  factors\nSample Size adjusted BIC achieves a minimum of  437.79  with  8  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq\n1 0.61 0.00 0.126 629  2866\n2 0.70 0.86 0.092 593  2249\n3 0.70 0.89 0.088 558  1974\n4 0.66 0.88 0.057 524  1456\n5 0.66 0.86 0.054 491  1228\n6 0.65 0.86 0.053 459  1073\n7 0.57 0.82 0.049 428   949\n8 0.55 0.82 0.052 398   858\n                                                                                                                                                                                                                                                                                          prob\n1 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000017\n2 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000093430007676563637860484012875872394943144172430038452148437500000000000000000000000000000000\n3 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000444418161123225061766989218980938858294393867254257202148437500000000000000000000000000000000000000000000000000000000000000000\n4 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000126741522878445080264615124621130348714359570294618606567382812500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n5 0.000000000000000000000000000000000000000000000000000000000000000094542742590778233880560721402730450790841132402420043945312500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n6 0.000000000000000000000000000000000000000000000000003802932090020860391756107876659598332480527460575103759765625000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n7 0.000000000000000000000000000000000000000017137959719205147570992436856229801378503907471895217895507812500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n8 0.000000000000000000000000000000000007470808978643554096668588648810782615328207612037658691406250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n  sqresid  fit RMSEA  BIC SABIC complex eChisq  SRMR eCRMS  eBIC\n1    93.3 0.61  0.23  221  2201     1.0   4945 0.235 0.242  2300\n2    34.2 0.86  0.20 -244  1623     1.2   1571 0.133 0.141  -922\n3    17.9 0.93  0.19 -373  1384     1.4    765 0.093 0.101 -1581\n4    10.9 0.95  0.16 -748   902     1.5    384 0.066 0.074 -1819\n5     7.1 0.97  0.15 -837   709     1.7    222 0.050 0.058 -1842\n6     4.7 0.98  0.14 -857   588     1.8    135 0.039 0.047 -1795\n7     3.4 0.99  0.13 -851   497     1.9     87 0.031 0.039 -1712\n8     2.5 0.99  0.13 -815   438     2.0     58 0.026 0.033 -1615\n\n\nCode\nfa.parallel(normed)\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  4 \n\n\nCode\npca_out &lt;- pca(normed, nfactors = 3, rotate = 'varimax')\nplot(pca_out$values)\nabline(h = 1)\n\n\n\n\n\n\n\n\n\nThis scree plot shows the eigenvalues (unit variance explained) of each principal component (y-axis) against each component (x-axis). The first few components explain lots of variance, but there is a decent elbow around the fourth component.\nVSS suggests 1 or 2, MAP suggests 8, parallel analysis shows 3. Iâm going with 3 here, which will be explained further below.",
    "crumbs": [
      "Secondary Data",
      "Environment",
      "Analysis"
    ]
  },
  {
    "objectID": "pages/data_environment_analysis.html#principal-components-analysis",
    "href": "pages/data_environment_analysis.html#principal-components-analysis",
    "title": "Environment: Analysis",
    "section": "4 Principal Components Analysis",
    "text": "4 Principal Components Analysis\nNow we letâs look run the PCA.\n\n\nCode\n(pca_out &lt;- pca(normed, nfactors = 3, rotate = 'varimax'))\n\n\nPrincipal Components Analysis\nCall: principal(r = r, nfactors = nfactors, residuals = residuals, \n    rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, \n    missing = missing, impute = impute, oblique.scores = oblique.scores, \n    method = method, use = use, cor = cor, correct = 0.5, weight = NULL)\nStandardized loadings (pattern matrix) based upon correlation matrix\n                            RC1   RC2   RC3   h2   u2 com\ndroughtWeeksSevere        -0.19  0.13 -0.80 0.70 0.30 1.2\ndroughtWeeksExtreme       -0.10 -0.02 -0.79 0.64 0.36 1.0\nlulcDiversity              0.21  0.51  0.35 0.43 0.57 2.1\nforestCarbonLive          -0.13  0.77  0.33 0.71 0.29 1.4\nforestCarbonDeadStanding  -0.06  0.28  0.32 0.18 0.82 2.1\nforestCarbonDeadDown      -0.17  0.75  0.22 0.63 0.37 1.3\nforestCanopyCover         -0.31  0.78 -0.13 0.72 0.28 1.4\nforestLiveTreeVolume      -0.29  0.76 -0.08 0.67 0.33 1.3\nforestLiveTrees           -0.12  0.19  0.54 0.35 0.65 1.4\nforestDeadTrees            0.32 -0.25  0.57 0.49 0.51 2.0\nforestStandHeight         -0.33  0.80 -0.06 0.75 0.25 1.3\nalleyCropSilvapastureNOps  0.38  0.62  0.27 0.60 0.40 2.1\nconsEasementAcres          0.19  0.56  0.50 0.60 0.40 2.2\nconsEasementAcresPF        0.23  0.14  0.76 0.64 0.36 1.3\nconsEasementNOps           0.09  0.81 -0.10 0.68 0.32 1.1\nconsTillExclNoTillAcres    0.92  0.01  0.11 0.86 0.14 1.0\nconsTillExclNoTillAcresPF  0.85 -0.13  0.21 0.78 0.22 1.2\nconsTillExclNoTillNOps     0.55  0.67 -0.09 0.76 0.24 2.0\nconsTillNoTillAcres        0.78  0.19  0.26 0.71 0.29 1.4\nconsTillNoTillAcresPF      0.62 -0.10  0.42 0.58 0.42 1.8\nconsTillNoTillNOps         0.40  0.75 -0.13 0.74 0.26 1.6\ncoverCropExclCrpAcres      0.89  0.01  0.03 0.80 0.20 1.0\ncoverCropExclCrpAcresPF    0.84 -0.10  0.19 0.75 0.25 1.1\ncoverCropExclCrpNOps       0.50  0.70 -0.16 0.77 0.23 1.9\ndrainedDitchesAcres        0.92  0.01  0.05 0.84 0.16 1.0\ndrainedDitchesAcresPF      0.91 -0.09  0.10 0.84 0.16 1.0\ndrainedDitchesNOps         0.60  0.37 -0.01 0.50 0.50 1.7\ndrainedTileAcres           0.69  0.04  0.18 0.50 0.50 1.1\ndrainedTileAcresPF         0.64 -0.09  0.28 0.50 0.50 1.4\ndrainedTileNOps            0.80  0.27  0.09 0.72 0.28 1.2\nprecisionAgNOps            0.79  0.28 -0.20 0.74 0.26 1.4\nrotateIntenseGrazeNOps     0.42  0.67  0.15 0.65 0.35 1.8\nfertExpenseTotal          -0.87 -0.09  0.10 0.77 0.23 1.0\nfertExpenseOpsWithExp     -0.35 -0.74  0.26 0.75 0.25 1.7\nbiomassHarvestNOps         0.17  0.70  0.00 0.52 0.48 1.1\nnFsaSecDisasters          -0.06  0.51  0.16 0.29 0.71 1.2\nnFsaPresDisasters          0.16 -0.01  0.83 0.71 0.29 1.1\n\n                        RC1  RC2  RC3\nSS loadings           10.86 8.47 4.56\nProportion Var         0.29 0.23 0.12\nCumulative Var         0.29 0.52 0.65\nProportion Explained   0.45 0.35 0.19\nCumulative Proportion  0.45 0.81 1.00\n\nMean item complexity =  1.4\nTest of the hypothesis that 3 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n with the empirical chi square  796.55  with prob &lt;  0.00000000012 \n\nFit based upon off diagonal values = 0.94\n\n\nRecommendations for creating composite indices are to extract components that each have eigenvalues &gt; 1, explained variance &gt; 0.10, and such that the proportion of explained variance for the total set is &gt; 0.60 (Nicoletti 2000; OECD 2008).\nOur total cumulative variance is explained is 0.74, and our component that explains the least variance is RC4 with 0.11. Note that extracting four or more components here gives us a component with less than 0.10, so this is why we are sticking to three. The first component (RC1) explains 38% of the variance in the data. The second component is respectable at 0.26, while the third is barely above the threshold at 0.11.\nLooking at the metrics, we can see that the first component loads mostly onto the conservation practices, no-till acres, cover cropping, drainage, and total fertilizer expenses. The second component leads onto mean above-ground biomass (although there is cross-loading with the first component), operations with silvapasture, operations with easements, rotational grazing operations, and operations with fertilizer expenses. This seems to be catching more of the population-related metrics. The last component only loads onto a few metrics: easement acres, easement acres per farm, and silvapasture operations (which has some heavy cross-loading).",
    "crumbs": [
      "Secondary Data",
      "Environment",
      "Analysis"
    ]
  },
  {
    "objectID": "pages/data_environment_analysis.html#aggregation",
    "href": "pages/data_environment_analysis.html#aggregation",
    "title": "Environment: Analysis",
    "section": "5 Aggregation",
    "text": "5 Aggregation\nHere, we follow Nicoletti and calculate the normalized sum of square factor loadings, which represent the proportion of total unit variance of the metrics that is explained by the component.\n\n\nCode\n## Get metric weights following Nicoletti 2000\n# Pull out metric loadings\nloadings &lt;- pca_out$weights %&gt;%\n  as.data.frame()\n\n# For each set of loadings, get squares, and then normalized proportions\nsq_loadings &lt;- map(loadings, ~ .x^2)\nmetric_weights &lt;- map(sq_loadings, ~ .x / sum(.x))\nhead(as.data.frame(metric_weights))\n\n\n            RC1           RC2        RC3\n1 0.00106789689 0.00424768529 0.14056643\n2 0.00490624619 0.00003622943 0.14266640\n3 0.00005098337 0.02724382619 0.02230611\n4 0.01582561729 0.07313818044 0.02701340\n5 0.00552223350 0.00951183857 0.02431250\n6 0.01475639416 0.07134055229 0.01255785\n\n\nNow we can use these to weight metrics and aggregate them into a component score for each county.\n\n\nCode\n# Component scores for each component across each county\ncomponent_scores &lt;- map(metric_weights, \\(x) {\n  as.matrix(normed) %*% x\n}) %&gt;%\n  as.data.frame()\nhead(component_scores)\n\n\n         RC1        RC2         RC3\n1 -0.4386941 -0.3235887 -0.13690759\n2  0.1270114  0.2766361 -0.34432215\n3  0.1036931  0.7519137  0.06999345\n4 -0.3461699 -0.4572497 -0.47618428\n5 -0.3372682  0.1206856 -0.41866790\n6  0.0373482  0.4286833  0.18077161\n\n\nAn alternative method here is regression scores, which are native to PCA. Iâll calculate these as well to compare to the component scores above.\n\n\nCode\n# Get regression scores from pca output\nregression_scores &lt;- as.data.frame(pca_out$scores)\nhead(regression_scores)\n\n\n         RC1        RC2        RC3\n1 -0.7190967 -0.4003725  0.2095507\n2  0.4926787  0.8706437 -1.1005301\n3  0.1217356  1.4927953 -0.0865495\n4 -0.4298152 -0.6620397 -0.3905350\n5 -0.0664264  0.4162461 -0.7080689\n6  0.5686187  0.9639500 -0.8366983\n\n\nRunning a correlation to see how similar they are:\n\n\nCode\ncoefs &lt;- map2_dbl(component_scores, regression_scores, \\(x, y) cor(x, y)) %&gt;%\n  round(3)\ncat(paste0(\n  'Pearson Correlation Coefficients:\\n',\n  'RC1: ', coefs[1], '\\n',\n  'RC2: ', coefs[2], '\\n',\n  'RC3: ', coefs[3]\n))\n\n\nPearson Correlation Coefficients:\nRC1: 0.932\nRC2: 0.969\nRC3: 0.427\n\n\nIt looks like they are reasonably similar, although RC2 and RC3 have substantially lower correlation coefficients. It will be worth noting this and coming back to explore the differences at some point.\nFor now, letâs keep following Nicoletti and aggregate the component scores into a single variable.\n\n\nCode\nsum_sq_loadings &lt;- map_dbl(sq_loadings, ~ sum(.x))\n(factor_weights &lt;- map_dbl(sum_sq_loadings, ~ .x / (sum(sum_sq_loadings))))\n\n\n      RC1       RC2       RC3 \n0.2210869 0.2647765 0.5141366 \n\n\nCurious that the component that accounted for the most variance is weighted the lowest. Worth doing a dive here at some point and figuring out why that is.\nWe will use these to weight each component to combine them.\n\n\nCode\ndimension_scores &lt;- component_scores %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    dimension_score = sum(RC1, RC2, RC3),\n    across(everything(), ~ round(.x, 3))\n  ) %&gt;%\n  bind_cols(rownames(imp)) %&gt;%\n  select(fips = 5, everything())\nhead(dimension_scores)\n\n\n# A tibble: 6 Ã 5\n# Rowwise: \n  fips     RC1    RC2    RC3 dimension_score\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;\n1 09001 -0.439 -0.324 -0.137          -0.899\n2 09003  0.127  0.277 -0.344           0.059\n3 09005  0.104  0.752  0.07            0.926\n4 09007 -0.346 -0.457 -0.476          -1.28 \n5 09009 -0.337  0.121 -0.419          -0.635\n6 09011  0.037  0.429  0.181           0.647\n\n\nNow that we have all three component scores and the dimension score, letâs take a look at a map. Select the data to display with the layer button on the left.\n\n\nCode\npacman::p_load(\n  mapview,\n  leaflet,\n  leafpop\n)\nmap_dat &lt;- readRDS('data/sm_data.rds')[['ne_counties_2021']] %&gt;%\n  right_join(dimension_scores) %&gt;%\n  left_join(fips_key) %&gt;%\n  select(\n    fips,\n    RC1:RC3,\n    'Dimension Score' = dimension_score,\n    County = county_name,\n    State = state_name,\n    geometry\n  )\n\nmap_dat %&gt;%\n  mapview(\n    zcol = c(\n      'Dimension Score',\n      'RC1',\n      'RC2',\n      'RC3'\n    ),\n    burst = FALSE,\n    hide = c(FALSE, rep(TRUE, 3)),\n    popup = popupTable(\n      map_dat,\n      zcol = names(map_dat)[-length(map_dat)],\n      row.numbers = FALSE,\n      feature.id = FALSE\n  )\n)\n\n\n\n\n\n\nKeep in mind there are lots of caveats with this very preliminary analysis, the most egregious being a set of metrics that does not well represent the dimension it purports to measure. Missing data and various branching paths of decisions in the index scoring also deserve further scrutiny.\nStill, there is plenty to look at here as a first pass at aggregating dimension scores. The first component, RC1, was heavily influenced by the geography - it loads the strongest onto metrics measuring acres or acres per farm. I presume this is why Aroostook county shows up so high on this scale. RC2 loaded strongly onto the number of operations using various conservation practices (easements, no-till, rotational grazing). It seems to track a little bit with county size, but is highest near relatively urban areas. RC3 was most associated with conservation easement acres and easement acres per farm, and consequently seems to track with rural areas.\nI donât think that the dimension score inspires much confidence as it is now. The weighting method for combining components is hard to interpret intuitively, and I think more expert driven normative decisions might make more sense at that point. On the bright side, it is a good expedition into the kinds of ambiguous decisions that will need to be made to aggregate this data across the whole system.",
    "crumbs": [
      "Secondary Data",
      "Environment",
      "Analysis"
    ]
  },
  {
    "objectID": "pages/comparison.html",
    "href": "pages/comparison.html",
    "title": "Comparison of Aggregation Methods",
    "section": "",
    "text": "In the last page we created six sets of scores by state based on combinations of three normalization methods (z-scores, min max, box cox) and two aggregation methods (arithmetic, geometric). Here, we will explore differences between them in terms of state distributions and rankings.\nNote that each set of spider plots are scaled to the minimum and maximum of any single state in that dimension, given the normalization and aggregation methods. This means in the case of min-max normalization, for example, raw metrics are scaled from 0 to 1, arithmetic and geometric means consolidate values to dimension scores, and these sets of dimension scores are scaled on the plot from the lowest to the highest value of any state. A âperfectâ score here means that it is the best of any state. Plots show dimension values for Vermont in green. The dotted purple polygon behind it is the US average by state. Arithmetic means are on the left, and geometric on the right.\nBe aware that spider/radar charts can be hard to interpret, and sometimes misleading The Radar Chart and its Caveats. The order of variables makes a big impact on the area of chart, and area is not a terribly reliable way to show differences, as it increases quadratically as variables increase linearly. Will explore some other ways to show this information, but using these for now as they are quite popular in the literature for sustainability metrics.",
    "crumbs": [
      "Secondary Data Rework",
      "Comparison"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table",
    "href": "pages/comparison.html#arithmetic-table",
    "title": "Comparison of Aggregation Methods",
    "section": "1.1 Arithmetic Table",
    "text": "1.1 Arithmetic Table\n\n\nCode\nget_reactable(dat, 'minmax_arithmetic')",
    "crumbs": [
      "Secondary Data Rework",
      "Comparison"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table",
    "href": "pages/comparison.html#geometric-table",
    "title": "Comparison of Aggregation Methods",
    "section": "1.2 Geometric Table",
    "text": "1.2 Geometric Table\n\n\nCode\nget_reactable(dat, 'minmax_geometric')",
    "crumbs": [
      "Secondary Data Rework",
      "Comparison"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table-1",
    "href": "pages/comparison.html#arithmetic-table-1",
    "title": "Comparison of Aggregation Methods",
    "section": "2.1 Arithmetic Table",
    "text": "2.1 Arithmetic Table\n\n\nCode\nget_reactable(dat, 'zscore_arithmetic')",
    "crumbs": [
      "Secondary Data Rework",
      "Comparison"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table-1",
    "href": "pages/comparison.html#geometric-table-1",
    "title": "Comparison of Aggregation Methods",
    "section": "2.2 Geometric Table",
    "text": "2.2 Geometric Table\n\n\nCode\nget_reactable(dat, 'zscore_geometric')",
    "crumbs": [
      "Secondary Data Rework",
      "Comparison"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table-2",
    "href": "pages/comparison.html#arithmetic-table-2",
    "title": "Comparison of Aggregation Methods",
    "section": "3.1 Arithmetic Table",
    "text": "3.1 Arithmetic Table\n\n\nCode\nget_reactable(dat, 'boxcox_arithmetic')",
    "crumbs": [
      "Secondary Data Rework",
      "Comparison"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table-2",
    "href": "pages/comparison.html#geometric-table-2",
    "title": "Comparison of Aggregation Methods",
    "section": "3.2 Geometric Table",
    "text": "3.2 Geometric Table\n\n\nCode\nget_reactable(dat, 'boxcox_geometric')",
    "crumbs": [
      "Secondary Data Rework",
      "Comparison"
    ]
  },
  {
    "objectID": "pages/aggregation.html",
    "href": "pages/aggregation.html",
    "title": "Metric Aggregation",
    "section": "",
    "text": "Exploring methods of aggregating data into index and dimension scores.",
    "crumbs": [
      "Secondary Data Rework",
      "Aggregation"
    ]
  },
  {
    "objectID": "pages/aggregation.html#indicator-means",
    "href": "pages/aggregation.html#indicator-means",
    "title": "Metric Aggregation",
    "section": "6.1 Indicator Means",
    "text": "6.1 Indicator Means\n\n\nCode\n# We need to attach these back to framework from metadata\n# Filter frame from earlier down to our current metrics\nfiltered_frame &lt;- frame %&gt;% \n  filter(variable_name %in% names(normed[[1]])) %&gt;% \n  select(variable_name, indicator, index, dimension)\n# get_str(filtered_frame)\n\n# Make a list where we hold scores for indicators, indices, and dimensions\nscores &lt;- list()\n\n# Function for geometric mean\ngeometric_mean = function(x, na.rm = TRUE){\n  exp(sum(log(x[x &gt; 0]), na.rm = na.rm) / length(x))\n}\n\n# Get indicator scores across all three normalization methods\nindicator_scores &lt;- map(normed, \\(df) {\n  \n  # For each df, calculate indicator means\n  indicators_out &lt;- map(unique(filtered_frame$indicator), \\(ind) {\n  \n    # Column name based on indicator\n    ind_snake &lt;- ind\n    \n    # Split into groups by indicator, with one or more metrics each\n    variables &lt;- filtered_frame %&gt;% \n      filter(indicator == ind) %&gt;% \n      pull(variable_name) %&gt;% \n      unique()\n    indicator_metrics &lt;- df %&gt;% \n      select(all_of(variables))\n    \n    # Get arithmetic and geo means for each indicator\n    dfs &lt;- list()\n    dfs$arithmetic &lt;- indicator_metrics %&gt;%\n      rowwise() %&gt;%\n      mutate(\n        !!sym(ind_snake) := mean(c_across(everything())),\n      ) %&gt;%\n      select(!!sym(ind_snake))\n    dfs$geometric &lt;- indicator_metrics %&gt;% \n      rowwise() %&gt;% \n      mutate(\n        !!sym(ind_snake) := geometric_mean(c_across(everything())),\n      ) %&gt;%\n      select(!!sym(ind_snake))\n    return(dfs) \n  })\n  \n  # Rearrange so we put each aggregation method (arith, geo) together\n  norm_out &lt;- list()\n  norm_out$arithmetic &lt;- map(indicators_out, ~ {\n    .x[grep(\"arithmetic\", names(.x))]\n  }) %&gt;% \n    bind_cols()\n  norm_out$geometric &lt;- map(indicators_out, ~ {\n    .x[grep(\"geometric\", names(.x))]\n  }) %&gt;% \n    bind_cols()\n  return(norm_out) \n})\n  \n# get_str(indicator_scores, 4)",
    "crumbs": [
      "Secondary Data Rework",
      "Aggregation"
    ]
  },
  {
    "objectID": "pages/aggregation.html#index-means",
    "href": "pages/aggregation.html#index-means",
    "title": "Metric Aggregation",
    "section": "6.2 Index Means",
    "text": "6.2 Index Means\n\n\nCode\n# For each set of indicator scores, calculate index scores\n# get_str(indicator_scores, 4)\nindices &lt;- unique(filtered_frame$index)\n\n# Choose aggregation function based on agg_type\nagg_function &lt;- function(x, agg_type) {\n   if (agg_type == 'geometric') {\n    geometric_mean(x)\n  } else if (agg_type == 'arithmetic') {\n    mean(x)\n  }\n}\n\nindex_scores &lt;- map(indicator_scores, \\(norm_type) {\n  imap(norm_type, \\(agg_df, agg_type) {\n    map(indices, \\(index_) {\n      # Get names of indicators for this index\n      index_indicators &lt;- filtered_frame %&gt;% \n        filter(index == index_) %&gt;% \n        pull(indicator) %&gt;% \n        unique()\n      # Get DF of indicators for this index\n      index_indicator_df &lt;- agg_df %&gt;% \n        select(all_of(index_indicators))\n      # Get arithmetic or geometric mean, based on agg_type\n      index_indicator_df %&gt;% \n        rowwise() %&gt;% \n        # mutate(mean = across(everything(), agg_function(agg_type)))\n        mutate(!!sym(index_) := agg_function(c_across(everything()), agg_type)) %&gt;% \n        select(!!sym(index_))\n    }) %&gt;% \n      bind_cols()\n  })\n})\n# get_str(index_scores, 4)",
    "crumbs": [
      "Secondary Data Rework",
      "Aggregation"
    ]
  },
  {
    "objectID": "pages/data_economics.html",
    "href": "pages/data_economics.html",
    "title": "Economics",
    "section": "",
    "text": "Shown in the diagram below are a total of 45 indicators within the economics dimension. Indices are labeled within the diagram. 17 indicators are both included in the Wiltshire et al.Â framework as well as being studied by one or more teams (red), 9 are included in the Wiltshire et al.Â but not currently belong studied (green), while 19 were not in the original framework, but have been added by one or more teams (blue).\nThe points beside each indicator name represent the number of secondary data metrics that have been aggregated for each indicator. Sources include USDA NASS, BLS, ERS, Census Bureau, and others. The quality and appropriateness of these metrics vary widely - I do not mean to suggest that having more of them means an indicator is more accurately better represented. For more information on the data sources, head to the Tables page to see metadata.\nOne other point to note here is that I removed several dozen metrics from BLS wage labor data broken down by NAICS industry code so as not to inflate that indicator relative to the others.\n\n\nCode\n## Load packages\npacman::p_load(\n  ggraph,\n  igraph,\n  dplyr,\n  RColorBrewer,\n  viridisLite,\n  ggrepel,\n  stringr\n)\n\nconflicted::conflicts_prefer(\n  dplyr::as_data_frame(),\n  .quiet = TRUE\n)\n\n## Load data for tree and metrics\ndat &lt;- readRDS('data/trees/econ_tree.rds') %&gt;% \n  select(Dimension:Source)\nmetadata_all &lt;- readRDS('data/sm_data.rds')[['metadata']]\nmeta &lt;- metadata_all %&gt;% \n  filter(\n    dimension == 'economics'\n  )\n\n# Rename metadata so it fits into formatting of tree data\n# This is quite not ideal - Note to harmonize this properly later\nmeta &lt;- meta %&gt;% \n  mutate(\n    indicator = str_to_sentence(indicator),\n    indicator = case_when(\n      str_detect(indicator, '^Assets') ~ 'Balance sheet (assets and liabilities)',\n      str_detect(indicator, '^Business failure') ~ 'Business failure rate of food business',\n      str_detect(indicator, '^Direct') ~ '% direct-to-consumer sales',\n      str_detect(indicator, '^Job avail') ~ 'Availability of good-paying jobs in food systems',\n      str_detect(indicator, '^Local sales') ~ '% local sales',\n      str_detect(indicator, '^Operator salary') ~ 'Operator salary / wage',\n      str_detect(indicator, '^Total sales') ~ 'Total sales / revenue',\n      str_detect(indicator, '^Wealth/income') ~ 'Wealth / income distribution',\n      TRUE ~ indicator\n    )\n  ) \n\n# Join counts of secondary data metrics to original dataset\n# Remove the NAICS variables - there are so many of them, don't add much\ncounts &lt;- meta %&gt;% \n  filter(str_detect(variable_name, '^lq|lvl|Lvl|Naics', negate = TRUE)) %&gt;% \n  group_by(indicator) %&gt;% \n  dplyr::summarize(count = n())\n\n\n## Make edges\n# include groupings by dimension, then combine them\nedges &lt;- list()\nedges$dim_ind &lt;- dat %&gt;% \n  select(Dimension, Index) %&gt;% \n  unique() %&gt;% \n  dplyr::rename(from = Dimension, to = Index) %&gt;% \n  mutate(group = to)\nedges$ind_ind &lt;- dat %&gt;% \n  select(Index, Indicator) %&gt;% \n  unique() %&gt;% \n  dplyr::rename(from = Index, to = Indicator) %&gt;% \n  mutate(group = from)\nedges &lt;- bind_rows(edges)\n\n# Add column for use (will use in colors of text?)\nedges$group &lt;- c(rep(NA, 10), dat$Source)\n\n\n## Make vertices\n# Each line is a single vertex (dimension, index, or indicator)\n# We are just giving them random values to control point size for now\nvertices = data.frame(\n  name = unique(c(as.character(edges$from), as.character(edges$to)))\n) %&gt;% \n  left_join(counts, by = join_by(name == indicator)) %&gt;% \n  dplyr::rename('value' = count)\n\n# Add the dimension groupings to the vertices as well\nvertices$group = edges$group[match(vertices$name, edges$to)]\n\n# Calculate the angles to arrange indicator labels\nvertices$id = NA\nmyleaves = which(is.na(match(vertices$name, edges$from)))\nnleaves = length(myleaves)\nvertices$id[myleaves] = seq(1:nleaves)\nvertices$angle = 90 - 360 * vertices$id / nleaves\n\n# Calculate alignment of indicator labels\nvertices$hjust &lt;- ifelse(vertices$angle &lt; -90, 1, 0)\n\n# Flip label angles around 180 degrees if they are facing the wrong way\nvertices$angle &lt;- ifelse(vertices$angle &lt; -90, vertices$angle + 180, vertices$angle)\n\n\n## Create graph\n# Make ggraph object from edges and vertices\ngraph &lt;- graph_from_data_frame(edges, vertices = vertices)\n\n# Plot the graph\nggraph(graph, layout = 'dendrogram', circular = TRUE) +\n  \n  # Color edges by dimension\n  geom_edge_diagonal(color = 'black', width = 0.5) +\n  \n  # Create text for indicators using angles, hjust, and dimension groupings\n  geom_node_text(\n    aes(\n      x = x * 1.15,\n      y = y * 1.15,\n      filter = leaf,\n      label = name,\n      angle = angle,\n      hjust = hjust,\n      colour = group\n    ),\n    size = 3,\n    alpha = 1\n  ) +\n  \n  # Label indices within graph\n  geom_label_repel(\n    aes(\n      x = x,\n      y = y,\n      label = ifelse(name %in% unique(dat$Index), name, NA)\n    ),\n    label.padding = unit(0.15, \"lines\"),\n    label.r = unit(0.3, \"lines\"),\n    label.size = 0.05,\n    size = 2.25,\n    force = 0.1,    \n    force_pull = 1, \n    max.overlaps = 10 \n  ) +\n  \n  # Make the points for indicators based on secondary metric count\n  geom_node_point(\n    aes(\n      filter = leaf,\n      x = x * 1.07,\n      y = y * 1.07,\n      colour = group,\n      size = value\n    ),\n    alpha = 0.4\n  ) +\n  \n  # Various formatting options\n  scale_colour_manual(values = brewer.pal(3, 'Set1')) +\n  # scale_size_continuous(range = c(0.1, 7)) +\n  theme_void() +\n  theme(\n    plot.margin = unit(c(0, 0, 0, 0), \"cm\")\n  ) +\n  scale_colour_manual(\n    name = \"Indicator Use\",\n    values = brewer.pal(3, 'Set1'),\n    labels = c(\"Both\", \"Current Only\", \"Wiltshire Only\")\n  ) +\n  expand_limits(x = c(-2.5, 2.5), y = c(-2.5, 2.5))\n\n\n\n\n\nRadial dendrogram of Sustainability Metrics framework",
    "crumbs": [
      "Secondary Data",
      "Economics"
    ]
  },
  {
    "objectID": "pages/data_economics.html#dimension-overview",
    "href": "pages/data_economics.html#dimension-overview",
    "title": "Economics",
    "section": "",
    "text": "Shown in the diagram below are a total of 45 indicators within the economics dimension. Indices are labeled within the diagram. 17 indicators are both included in the Wiltshire et al.Â framework as well as being studied by one or more teams (red), 9 are included in the Wiltshire et al.Â but not currently belong studied (green), while 19 were not in the original framework, but have been added by one or more teams (blue).\nThe points beside each indicator name represent the number of secondary data metrics that have been aggregated for each indicator. Sources include USDA NASS, BLS, ERS, Census Bureau, and others. The quality and appropriateness of these metrics vary widely - I do not mean to suggest that having more of them means an indicator is more accurately better represented. For more information on the data sources, head to the Tables page to see metadata.\nOne other point to note here is that I removed several dozen metrics from BLS wage labor data broken down by NAICS industry code so as not to inflate that indicator relative to the others.\n\n\nCode\n## Load packages\npacman::p_load(\n  ggraph,\n  igraph,\n  dplyr,\n  RColorBrewer,\n  viridisLite,\n  ggrepel,\n  stringr\n)\n\nconflicted::conflicts_prefer(\n  dplyr::as_data_frame(),\n  .quiet = TRUE\n)\n\n## Load data for tree and metrics\ndat &lt;- readRDS('data/trees/econ_tree.rds') %&gt;% \n  select(Dimension:Source)\nmetadata_all &lt;- readRDS('data/sm_data.rds')[['metadata']]\nmeta &lt;- metadata_all %&gt;% \n  filter(\n    dimension == 'economics'\n  )\n\n# Rename metadata so it fits into formatting of tree data\n# This is quite not ideal - Note to harmonize this properly later\nmeta &lt;- meta %&gt;% \n  mutate(\n    indicator = str_to_sentence(indicator),\n    indicator = case_when(\n      str_detect(indicator, '^Assets') ~ 'Balance sheet (assets and liabilities)',\n      str_detect(indicator, '^Business failure') ~ 'Business failure rate of food business',\n      str_detect(indicator, '^Direct') ~ '% direct-to-consumer sales',\n      str_detect(indicator, '^Job avail') ~ 'Availability of good-paying jobs in food systems',\n      str_detect(indicator, '^Local sales') ~ '% local sales',\n      str_detect(indicator, '^Operator salary') ~ 'Operator salary / wage',\n      str_detect(indicator, '^Total sales') ~ 'Total sales / revenue',\n      str_detect(indicator, '^Wealth/income') ~ 'Wealth / income distribution',\n      TRUE ~ indicator\n    )\n  ) \n\n# Join counts of secondary data metrics to original dataset\n# Remove the NAICS variables - there are so many of them, don't add much\ncounts &lt;- meta %&gt;% \n  filter(str_detect(variable_name, '^lq|lvl|Lvl|Naics', negate = TRUE)) %&gt;% \n  group_by(indicator) %&gt;% \n  dplyr::summarize(count = n())\n\n\n## Make edges\n# include groupings by dimension, then combine them\nedges &lt;- list()\nedges$dim_ind &lt;- dat %&gt;% \n  select(Dimension, Index) %&gt;% \n  unique() %&gt;% \n  dplyr::rename(from = Dimension, to = Index) %&gt;% \n  mutate(group = to)\nedges$ind_ind &lt;- dat %&gt;% \n  select(Index, Indicator) %&gt;% \n  unique() %&gt;% \n  dplyr::rename(from = Index, to = Indicator) %&gt;% \n  mutate(group = from)\nedges &lt;- bind_rows(edges)\n\n# Add column for use (will use in colors of text?)\nedges$group &lt;- c(rep(NA, 10), dat$Source)\n\n\n## Make vertices\n# Each line is a single vertex (dimension, index, or indicator)\n# We are just giving them random values to control point size for now\nvertices = data.frame(\n  name = unique(c(as.character(edges$from), as.character(edges$to)))\n) %&gt;% \n  left_join(counts, by = join_by(name == indicator)) %&gt;% \n  dplyr::rename('value' = count)\n\n# Add the dimension groupings to the vertices as well\nvertices$group = edges$group[match(vertices$name, edges$to)]\n\n# Calculate the angles to arrange indicator labels\nvertices$id = NA\nmyleaves = which(is.na(match(vertices$name, edges$from)))\nnleaves = length(myleaves)\nvertices$id[myleaves] = seq(1:nleaves)\nvertices$angle = 90 - 360 * vertices$id / nleaves\n\n# Calculate alignment of indicator labels\nvertices$hjust &lt;- ifelse(vertices$angle &lt; -90, 1, 0)\n\n# Flip label angles around 180 degrees if they are facing the wrong way\nvertices$angle &lt;- ifelse(vertices$angle &lt; -90, vertices$angle + 180, vertices$angle)\n\n\n## Create graph\n# Make ggraph object from edges and vertices\ngraph &lt;- graph_from_data_frame(edges, vertices = vertices)\n\n# Plot the graph\nggraph(graph, layout = 'dendrogram', circular = TRUE) +\n  \n  # Color edges by dimension\n  geom_edge_diagonal(color = 'black', width = 0.5) +\n  \n  # Create text for indicators using angles, hjust, and dimension groupings\n  geom_node_text(\n    aes(\n      x = x * 1.15,\n      y = y * 1.15,\n      filter = leaf,\n      label = name,\n      angle = angle,\n      hjust = hjust,\n      colour = group\n    ),\n    size = 3,\n    alpha = 1\n  ) +\n  \n  # Label indices within graph\n  geom_label_repel(\n    aes(\n      x = x,\n      y = y,\n      label = ifelse(name %in% unique(dat$Index), name, NA)\n    ),\n    label.padding = unit(0.15, \"lines\"),\n    label.r = unit(0.3, \"lines\"),\n    label.size = 0.05,\n    size = 2.25,\n    force = 0.1,    \n    force_pull = 1, \n    max.overlaps = 10 \n  ) +\n  \n  # Make the points for indicators based on secondary metric count\n  geom_node_point(\n    aes(\n      filter = leaf,\n      x = x * 1.07,\n      y = y * 1.07,\n      colour = group,\n      size = value\n    ),\n    alpha = 0.4\n  ) +\n  \n  # Various formatting options\n  scale_colour_manual(values = brewer.pal(3, 'Set1')) +\n  # scale_size_continuous(range = c(0.1, 7)) +\n  theme_void() +\n  theme(\n    plot.margin = unit(c(0, 0, 0, 0), \"cm\")\n  ) +\n  scale_colour_manual(\n    name = \"Indicator Use\",\n    values = brewer.pal(3, 'Set1'),\n    labels = c(\"Both\", \"Current Only\", \"Wiltshire Only\")\n  ) +\n  expand_limits(x = c(-2.5, 2.5), y = c(-2.5, 2.5))\n\n\n\n\n\nRadial dendrogram of Sustainability Metrics framework",
    "crumbs": [
      "Secondary Data",
      "Economics"
    ]
  },
  {
    "objectID": "pages/data_economics.html#distributions",
    "href": "pages/data_economics.html#distributions",
    "title": "Economics",
    "section": "2 Distributions",
    "text": "2 Distributions\nWe are taking out the abundant but largely redundant BLS NAICS wage data variables to leave us with a more approachable set of 46 variables to explore here. First just show univariate distributions by county.\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr\n)\n\nsource('dev/data_pipeline_functions.R')\nsource('dev/filter_fips.R')\nmetrics &lt;- readRDS('data/sm_data.rds')[['metrics']]\nmetadata &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\n# Use metadata to get help filter by dimension\necon_meta &lt;- metadata %&gt;% \n  filter(dimension == 'economics')\n\n# Filter to economics dimension\necon_metrics &lt;- metrics %&gt;% \n  filter(variable_name %in% econ_meta$variable_name)\n\n# Filter to latest year and new (post-2024) counties\n# Also remove NAICS variables to leave us with an approachable number\n# And pivot wider so it is easier to get correlations\necon_metrics_latest &lt;- econ_metrics %&gt;%\n  filter_fips(scope = 'new') %&gt;% \n  get_latest_year() %&gt;% \n  filter(\n    str_detect(\n      variable_name, \n      'Naics|NAICS|^lq|^avgEmpLvl|expHiredLaborPercOpExp', \n      negate = TRUE\n    )\n  )\n\n# Pivot wider for easier correlations below\necon_metrics_latest &lt;- econ_metrics_latest %&gt;% \n  select(fips, variable_name, value) %&gt;% \n  unique() %&gt;% \n  mutate(variable_name = str_split_i(variable_name, '_', 1)) %&gt;% \n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;% \n  unnest(!fips) %&gt;% \n  mutate(across(c(civLaborForce:last_col()), as.numeric))\n\n\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr\n)\n\nplots &lt;- map(names(econ_metrics_latest)[-1], \\(var){\n  if (is.character(econ_metrics_latest[[var]])) {\n    econ_metrics_latest %&gt;% \n      ggplot(aes(x = !!sym(var))) + \n      geom_bar(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else if (is.numeric(econ_metrics_latest[[var]])) {\n    econ_metrics_latest %&gt;% \n      ggplot(aes(x = !!sym(var))) + \n      geom_density(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else {\n    return(NULL)\n  }\n}) \n\n# Arrange them in 4 columns\nggarrange(\n  plotlist = plots,\n  ncol = 4,\n  nrow = 12\n)\n\n\n$`1`\n\n\n\n\n\nDistributions of economic metrics at the county level.\n\n\n\n\n\n$`2`\n\n\n\n\n\nDistributions of economic metrics at the county level.\n\n\n\n\n\nattr(,\"class\")\n[1] \"list\"      \"ggarrange\"",
    "crumbs": [
      "Secondary Data",
      "Economics"
    ]
  },
  {
    "objectID": "pages/data_economics.html#correlation-heatmap",
    "href": "pages/data_economics.html#correlation-heatmap",
    "title": "Economics",
    "section": "3 Correlation Heatmap",
    "text": "3 Correlation Heatmap\nThrowing those same variables into a correlation matrix. Hover to see variable names, Pearson correlation, and p-values.\n\n\nCode\npacman::p_load(\n  dplyr,\n  tidyr,\n  tibble,\n  stringr,\n  purrr,\n  tidyr,\n  ggplot2,\n  plotly,\n  reshape,\n  Hmisc,\n  viridisLite\n)\n\n# Arrange variables in some halfway reasonable order\ncor_dat &lt;- econ_metrics_latest %&gt;% \n  select(\n    matches('Code_|metro'),\n    matches('employ|abor|Worker'),\n    matches('Sales'),\n    matches('Earn|Income'),\n    everything(),\n    -fips,\n    -matches('expHiredLaborPercOpExp') # This one didn't come through\n  )\n\n# Make a correlation matrix using all the selected variables\ncor &lt;- cor_dat %&gt;% \n  as.matrix() %&gt;% \n  rcorr()\n\n# Melt correlation values and rename columns\ncor_r &lt;- melt(cor$r) %&gt;% \n  setNames(c('var_1', 'var_2', 'value'))\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Make heatmap with custom text aesthetic for tooltip\nplot &lt;- cor_r %&gt;% \n  ggplot(aes(var_1, var_2, fill = value, text = paste0(\n    'Var 1: ', var_1, '\\n',\n    'Var 2: ', var_2, '\\n',\n    'Correlation: ', format(round(value, 3), nsmall = 3), '\\n',\n    'P-Value: ', format(round(p.value, 3), nsmall = 3)\n  ))) + \n  geom_tile() + \n  scale_fill_viridis_c() + \n  theme(axis.text.x = element_text(hjust = 1, angle = 45)) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = 'Correlation'\n  )\n\n# Convert to interactive plotly figure with text tooltip\nggplotly(\n  plot, \n  tooltip = 'text',\n  width = 1000,\n  height = 800\n)\n\n\n\n\nInteractive Correlation Plot",
    "crumbs": [
      "Secondary Data",
      "Economics"
    ]
  },
  {
    "objectID": "pages/data_economics.html#pca",
    "href": "pages/data_economics.html#pca",
    "title": "Economics",
    "section": "4 PCA",
    "text": "4 PCA\nPCA is a popular tool in this area for exploring unique variation with many collinear variables. It is a way to reduce the dimensionality of the data into fewer, more interpretable principal components.\nIt also requires complete data, which we do not have. So we either have to run a probabililistic PCA or run imputations. Iâm using a random forest algorithm to impute data here as a first pass (Stekhoven and BÃ¼hlmann 2012). This really warrants a deeper dive into the type and severity of missingness though, and PPCA is likely the better option in the end.\n\n\nCode\npacman::p_load(\n  missForest\n)\n\n# Wrangle dataset. Need all numeric vars or factor vars. And can't be tibble\n# Also removing character vars - can't use these in PCA\ndat &lt;- econ_metrics_latest %&gt;%\n  select(where(is.numeric)) %&gt;%\n  as.data.frame()\n# get_str(dat)\n\n# Check missing variables\n# skimr::skim(dat)\n\n# Impute missing variables\nset.seed(42)\nmf_out &lt;- dat %&gt;%\n  missForest(\n    ntree = 200,\n    mtry = 10,\n    verbose = FALSE,\n    variablewise = FALSE\n  )\n\n\n  removed variable(s) 15 due to the missingness of all entries\n\n\nCode\n# Save imputed dataset\nimp &lt;- mf_out$ximp\n\n# Print OOB\nmf_out$OOBerror\n\n\n    NRMSE \n0.1909033 \n\n\nOut of bag error is shown as normalized root mean square error. Now we can explore how many composite factors is appropriate for the data.\n\n\nCode\npacman::p_load(\n  psych\n)\nVSS(imp)\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.66  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.84  with  2  factors\n\nThe Velicer MAP achieves a minimum of 0.04  with  6  factors \nBIC achieves a minimum of  -1186.13  with  5  factors\nSample Size adjusted BIC achieves a minimum of  1669.02  with  8  factors\n\nStatistics by number of factors \n  vss1 vss2   map  dof chisq\n1 0.51 0.00 0.120 1175  5057\n2 0.66 0.84 0.064 1126  4210\n3 0.64 0.81 0.058 1078  3726\n4 0.65 0.84 0.052 1031  3401\n5 0.60 0.83 0.044  985  2970\n6 0.57 0.81 0.041  940  2807\n7 0.55 0.79 0.042  896  2671\n8 0.54 0.79 0.044  853  2582\n                                                                                                                                                                                                                                                                                                prob\n1 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n2 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n3 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000016\n4 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000319536676019325987076549755450116663269\n5 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000009241128965980816158988508934868377764360047876834869384765625000000000000000000000000000000\n6 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000061530117101705700134034460191401194606442004442214965820312500000000000000000000000000000000000000000000\n7 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000108915515911629524881815145320729243394453078508377075195312500000000000000000000000000000000000000000000000000000\n8 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000046276694390055251623720955489460493481601588428020477294921875000000000000000000000000000000000000000000000000000000\n  sqresid  fit RMSEA   BIC SABIC complex eChisq  SRMR eCRMS  eBIC\n1     162 0.51  0.22    99  3799     1.0   8837 0.230 0.235  3879\n2      54 0.84  0.20  -541  3004     1.3   2350 0.119 0.124 -2401\n3      34 0.90  0.19  -823  2572     1.6   1283 0.088 0.094 -3266\n4      22 0.93  0.18  -950  2297     1.7    708 0.065 0.071 -3642\n5      17 0.95  0.17 -1186  1916     1.7    478 0.054 0.060 -3678\n6      15 0.96  0.17 -1159  1801     1.9    383 0.048 0.055 -3583\n7      12 0.96  0.17 -1110  1712     2.0    305 0.043 0.050 -3476\n8      11 0.97  0.17 -1017  1669     2.0    248 0.039 0.046 -3351\n\n\nCode\nfa.parallel(imp)\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  4 \n\n\nVSS gives a wide range from 2 to 8, MAP shows 7, parallel analysis shows 4. I tend to trust PA the most, so letâs go with 4.\n\n\nCode\n(pca_out &lt;- pca(imp, nfactors = 4))\n\n\nPrincipal Components Analysis\nCall: principal(r = r, nfactors = nfactors, residuals = residuals, \n    rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, \n    missing = missing, impute = impute, oblique.scores = oblique.scores, \n    method = method, use = use, cor = cor, correct = 0.5, weight = NULL)\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                             RC1   RC2   RC4   RC3    h2    u2\ncivLaborForce                               0.23  0.25  0.89  0.01 0.902 0.098\nemployed                                    0.23  0.26  0.88  0.00 0.900 0.100\nunemployed                                  0.24  0.21  0.91  0.10 0.934 0.066\nunemploymentRate                            0.12 -0.12  0.17  0.84 0.763 0.237\nmedHhIncome                                 0.06  0.89  0.30 -0.08 0.896 0.104\nmedHhIncomePercState                        0.06  0.83  0.04 -0.21 0.740 0.260\ngini                                       -0.21  0.03  0.34  0.48 0.392 0.608\nunemployment                                0.12 -0.12  0.17  0.85 0.772 0.228\nchildrenInPoverty                          -0.03 -0.84  0.12  0.34 0.834 0.166\nincomeInequality                           -0.06 -0.14  0.62  0.39 0.557 0.443\nmedianHouseholdIncome                       0.06  0.89  0.35 -0.04 0.912 0.088\nchildrenEligibleForFreeOrReducedPriceLunch  0.09 -0.79 -0.06  0.36 0.761 0.239\nbroadbandAccess                            -0.06  0.79  0.25 -0.18 0.722 0.278\ngenderPayGap                               -0.17  0.04 -0.14  0.22 0.098 0.902\nnCSA                                        0.19 -0.04  0.40 -0.23 0.249 0.751\nnFarmersMarket                              0.34  0.20  0.82 -0.05 0.831 0.169\nnOnFarmMarket                               0.02  0.07  0.12 -0.02 0.021 0.979\nagTourSalesPerc                            -0.14  0.42  0.14  0.58 0.554 0.446\nd2cSalesPerc                               -0.28  0.53  0.34  0.08 0.477 0.523\nlocalSalesPerc                             -0.19  0.55  0.34  0.13 0.468 0.532\nnAnaerDigestion                             0.54 -0.15 -0.36  0.05 0.444 0.556\nnCompost                                    0.35  0.18  0.80  0.07 0.802 0.198\nnFoodHubs                                   0.17 -0.04  0.01 -0.04 0.034 0.966\nnMeatProcess                                0.18  0.12  0.78  0.02 0.662 0.338\nmedianEarnMaleFood                          0.00  0.16  0.19 -0.34 0.176 0.824\nmedianEarnFemaleFood                       -0.25  0.19  0.07  0.52 0.372 0.628\nwomenEarnPercMaleFood                      -0.17 -0.08 -0.16  0.55 0.363 0.637\nmedianEarnMaleFarm                         -0.17  0.31 -0.04  0.26 0.197 0.803\nmedianEarnFemaleFarm                       -0.13  0.63 -0.10  0.32 0.522 0.478\nwomenEarnPercMaleFarm                      -0.06  0.53  0.05  0.18 0.326 0.674\nexpHiredLabor                               0.95  0.05  0.18 -0.07 0.935 0.065\nnOpsHiredLaborExp                           0.87 -0.03  0.27 -0.26 0.902 0.098\nnHiredWorkers                               0.91  0.04  0.33 -0.11 0.949 0.051\nnOpsHiredLabor                              0.87 -0.03  0.27 -0.26 0.902 0.098\nnWorkersGE150                               0.83  0.13  0.42 -0.11 0.892 0.108\nnOpsWorkersGE150                            0.88  0.05  0.31 -0.20 0.906 0.094\nnWorkersLE150                               0.88 -0.03  0.26 -0.05 0.838 0.162\nnOpsWorkersLE150                            0.88 -0.05  0.24 -0.26 0.903 0.097\nnMigrantWorkers                             0.60 -0.18 -0.11  0.16 0.434 0.566\nnOpsMigrantWorkers                          0.79  0.07  0.10 -0.09 0.652 0.348\nnUnpaidWorkers                              0.64 -0.17  0.36 -0.37 0.698 0.302\nnOpsUnpaidWorkers                           0.68 -0.17  0.28 -0.39 0.719 0.281\nexpHiredLaborPF                             0.61  0.17 -0.03  0.18 0.436 0.564\nexpPF                                       0.77  0.02 -0.30  0.29 0.759 0.241\nfarmIncomePF                                0.31  0.67  0.27  0.10 0.621 0.379\nacresOperated                               0.72 -0.41 -0.36 -0.05 0.821 0.179\nacresPF                                     0.36 -0.56 -0.54  0.13 0.753 0.247\nmedianAcresPF                               0.26 -0.64 -0.48 -0.02 0.713 0.287\nlandValPF                                   0.23  0.51 -0.12  0.13 0.343 0.657\nlandValPerAcre                             -0.15  0.50  0.42  0.26 0.517 0.483\n                                           com\ncivLaborForce                              1.3\nemployed                                   1.3\nunemployed                                 1.3\nunemploymentRate                           1.2\nmedHhIncome                                1.3\nmedHhIncomePercState                       1.1\ngini                                       2.2\nunemployment                               1.2\nchildrenInPoverty                          1.4\nincomeInequality                           1.8\nmedianHouseholdIncome                      1.3\nchildrenEligibleForFreeOrReducedPriceLunch 1.4\nbroadbandAccess                            1.3\ngenderPayGap                               2.7\nnCSA                                       2.1\nnFarmersMarket                             1.5\nnOnFarmMarket                              1.8\nagTourSalesPerc                            2.1\nd2cSalesPerc                               2.3\nlocalSalesPerc                             2.1\nnAnaerDigestion                            1.9\nnCompost                                   1.5\nnFoodHubs                                  1.2\nnMeatProcess                               1.2\nmedianEarnMaleFood                         2.0\nmedianEarnFemaleFood                       1.8\nwomenEarnPercMaleFood                      1.4\nmedianEarnMaleFarm                         2.6\nmedianEarnFemaleFarm                       1.7\nwomenEarnPercMaleFarm                      1.3\nexpHiredLabor                              1.1\nnOpsHiredLaborExp                          1.4\nnHiredWorkers                              1.3\nnOpsHiredLabor                             1.4\nnWorkersGE150                              1.6\nnOpsWorkersGE150                           1.4\nnWorkersLE150                              1.2\nnOpsWorkersLE150                           1.3\nnMigrantWorkers                            1.4\nnOpsMigrantWorkers                         1.1\nnUnpaidWorkers                             2.4\nnOpsUnpaidWorkers                          2.1\nexpHiredLaborPF                            1.3\nexpPF                                      1.6\nfarmIncomePF                               1.8\nacresOperated                              2.1\nacresPF                                    2.8\nmedianAcresPF                              2.2\nlandValPF                                  1.7\nlandValPerAcre                             2.7\n\n                        RC1  RC2  RC4  RC3\nSS loadings           11.15 8.18 7.72 4.32\nProportion Var         0.22 0.16 0.15 0.09\nCumulative Var         0.22 0.39 0.54 0.63\nProportion Explained   0.36 0.26 0.25 0.14\nCumulative Proportion  0.36 0.62 0.86 1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 4 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.07 \n with the empirical chi square  757.14  with prob &lt;  1 \n\nFit based upon off diagonal values = 0.96\n\n\nCode\nplot(pca_out$values)\nabline(h = 1)\n\n\n\n\n\n\n\n\n\nFrom the scree plot and eigenvalues it looks like the first three components bear lots of unique variance, but after that there is no clear elbow where a qualitative decision can be made to choose a certain number of components. The Kaiser-Guttman rule suggests keeping any compents with an eigenvalue &gt; 1 (at the horizontal line), but we can see here that this is a rather dubious distinction.\nIf we look at the output from the PCA call, we can see how closely each variable (row) correlates with each component (columns 1-4). The variables most associated with Component #1 are the farm labor variables - numbers of workers, labor expenses, etc. They also tend to be raw figures, and probably have more to do with population than anything else. Component #2 is made up mostly of generic employment figures - total civilian labor force, total employed, total unemployed. These are not specific to food systems. Component #3 has a curious collection of median earnings variables and âper farmâ variables like acres per farm, income per farm, and local and direct-to-consumer sales. Component #4 does not represent much unique variance, and loooks like a grab bag of variables.\nA couple of early takeaways here are that the raw figures that are tied to population probably shouldnât be mixed with other variables like proportions. We could try normalizing all the variables so that raw variables are not disproportionately weighted. But it might make more sense to avoid raw counts and dollar amounts entirely.",
    "crumbs": [
      "Secondary Data",
      "Economics"
    ]
  },
  {
    "objectID": "pages/data_environment_maps.html",
    "href": "pages/data_environment_maps.html",
    "title": "Environment: Maps",
    "section": "",
    "text": "Taking a quick tour through some of the spatial data here. Most of these metrics will also be available to peruse on the Shiny app, with the exception of those that are hard to aggregate, like biodiversity hotspots."
  },
  {
    "objectID": "pages/data_environment_maps.html#land-use",
    "href": "pages/data_environment_maps.html#land-use",
    "title": "Environment: Maps",
    "section": "1 Land Use",
    "text": "1 Land Use\nThis is the MRLC 30m LULC layer from 2023. Below the map, you can find a table with codes and descriptions. Sort or expand to see all the values.\n\n\nCode\nlulc &lt;- readRDS('data/sm_data.rds')[['mrlc_lulc_ne']]\n# sm_data &lt;- readRDS('data/sm_data.rds')\ncounties &lt;- readRDS('data/sm_data.rds')[['ne_counties_2024']]\n\nlulc_map &lt;- lulc %&gt;% \n  mapview(\n    layer.name = 'LULC'\n  ) + \n  mapview(\n    counties,\n    alpha.regions = 0,\n    color = 'black',\n    col.regions = 'black',\n    lwd = 1.25,\n    layer.name = 'Counties'\n  )\n\nlulc_map@map %&gt;% \n  addFullscreenControl()\n\n\n\n\nLand use land cover map\n\n\n\n\nCode\npacman::p_load(\n  reactable,\n  dplyr,\n  stringr\n)\n\nmeta &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\nlulc_codes &lt;- meta %&gt;% \n  filter(\n    str_detect(variable_name, '^lulc'),\n    str_detect(variable_name, 'NoData|Diversity', negate = TRUE)\n  ) %&gt;% \n  select(definition) %&gt;% \n  mutate(\n    Value = c(11, 12, 21, 22, 23, 24, 31, 41, 42, 43, 52, 71, 81, 82, 90, 95),\n    Class = c(\n      rep('Water', 2),\n      rep('Developed', 4),\n      'Barren',\n      rep('Forest', 3),\n      'Shrubland',\n      'Herbaceous',\n      rep('Planted/Cultivated', 2),\n      rep('Wetlands', 2)\n    ),\n    Type = c(\n      'Open Water',\n      'Ice or Snow',\n      'Developed, Open Space',\n      'Developed, Low Intensity',\n      'Developed, Medium Intensity',\n      'Developed, High Intensity',\n      'Barren Land (Rock / Sand / Clay)',\n      'Deciduous Forest',\n      'Evergreen Forest',\n      'Mixed Forest',\n      'Shrub / Scrub',\n      'Grassland / Herbaceous',\n      'Pasture / Hay',\n      'Cultivated Crops',\n      'Woody Wetlands',\n      'Emergent Herbaceous Wetlands'\n    )\n  ) %&gt;% \n  select(\n    Value,\n    Class,\n    Type,\n    Description = definition\n  )\n\nreactable(\n  lulc_codes,\n  sortable = TRUE,\n  resizable = TRUE,\n  filterable = TRUE,\n  searchable = FALSE,\n  pagination = TRUE,\n  bordered = TRUE,\n  wrap = TRUE,\n  rownames = FALSE,\n  striped = TRUE,\n  defaultPageSize = 5,\n  showPageSizeOptions = FALSE,\n  highlight = TRUE,\n  style = list(fontSize = \"14px\"),\n  compact = TRUE,\n  columns = list(\n    Value = colDef(minWidth = 40),\n    Class = colDef(minWidth = 100),\n    Type = colDef(minWidth = 100),\n    Description = colDef(minWidth = 500)\n  )\n)"
  },
  {
    "objectID": "pages/data_environment_maps.html#land-use-diversity",
    "href": "pages/data_environment_maps.html#land-use-diversity",
    "title": "Environment: Maps",
    "section": "2 Land Use Diversity",
    "text": "2 Land Use Diversity\nLULC Diversity is derived from the MRLC LULC layer above. LULC types are aggregated by category (water, developed, barren, forest, shrubland, herbaceous, cultivated, wetlands) and Shannon diversity is calculated for each county. It makes for an interesting metric, but Iâm not sure it makes for a strong normative metric. If anyone has thoughts on what the ârightâ amount of LULC diversity is, Iâd love to hear from you.\n\n\nCode\ndiv &lt;- readRDS('data/sm_data.rds')[['lulc_div']]\n\ndiv_map &lt;- mapview(\n  div,\n  zcol = 'lulc_div',\n  label = 'county_name',\n  layer.name = 'LULC Diversity',\n  popup = popupTable(\n    div,\n    zcol = c(\n      'county_name',\n      'lulc_div'\n    ),\n    row.numbers = FALSE,\n    feature.id = FALSE\n  )\n)\n\ndiv_map@map %&gt;% \n  addFullscreenControl()\n\n\n\n\nLand Use Land Cover Diversity"
  },
  {
    "objectID": "pages/data_environment_maps.html#rare-threatened-and-endangered-species",
    "href": "pages/data_environment_maps.html#rare-threatened-and-endangered-species",
    "title": "Environment: Maps",
    "section": "3 Rare, Threatened and Endangered Species",
    "text": "3 Rare, Threatened and Endangered Species\nThe Vermont ANR Biofinder has lots of great layers. Technical abstracts for these layers can be found here. Below is a map of rare, threatened, and endangered species polygons statewide. Note that these are lumped together into a multi-polygon to save some space, but the individual polygons didnât provide a whole lot useful information anyway.\n\n\nCode\npacman::p_load(\n  mapview,\n  leaflet.extras,\n  sf\n)\nrte &lt;- readRDS('data/sm_data.rds')[['biofinder_rte_spp']] %&gt;% \n  summarize()\nrte_map &lt;- mapview(\n  rte,\n  layer.name = 'RTE Species',\n  col.regions = '#154734'\n)\nrte_map@map %&gt;%\n  addFullscreenControl()\n\n\n\n\nMap of Rare, Threatened, and Endangered Species"
  },
  {
    "objectID": "pages/data_environment_maps.html#uncommon-species",
    "href": "pages/data_environment_maps.html#uncommon-species",
    "title": "Environment: Maps",
    "section": "4 Uncommon Species",
    "text": "4 Uncommon Species\nBiofinder also lists uncommon species as those facing a âmoderate risk of extinction or extirpation due to restricted range, relatively few populations (often 80 or fewer), recent widespread declines, and other factors.â Same as above, these are lumped together into a single polygon for convenience.\n\n\nCode\nuncommon &lt;- readRDS('data/sm_data.rds')[['biofinder_uncommon_spp']] %&gt;% \n  summarize()\nuncommon_map &lt;- mapview(\n  uncommon,\n  layer.name = 'Uncommon Species',\n  col.regions = '#154734'\n)\nuncommon_map@map %&gt;%\n  addFullscreenControl()\n\n\n\n\nMap of Uncommon Species Distributions"
  },
  {
    "objectID": "pages/data_environment_maps.html#forest-biomass",
    "href": "pages/data_environment_maps.html#forest-biomass",
    "title": "Environment: Maps",
    "section": "5 Forest Biomass",
    "text": "5 Forest Biomass\nThe TreeMap 2016 dataset is quite comprehensive national survey of forest health and diversity. Updates are infrequent, but this is the best layer Iâve found to address biomass. The raster is at 30m.\n\n\nCode\ntreemap &lt;- readRDS('data/sm_data.rds')[['treemap_biomass']]\ncounties &lt;- readRDS('data/sm_data.rds')[['ne_counties_2024']]\n\ntreemap_map &lt;- treemap %&gt;%\n  mapview(\n    layer.name = 'Biomass (tons per acre)',\n    col.regions = viridis(n = 256)\n  ) +\n  mapview(\n    counties,\n    alpha.regions = 0,\n    color = 'black',\n    col.regions = 'black',\n    lwd = 1.25,\n    layer.name = 'Counties'\n  )\ntreemap_map@map %&gt;%\n  addFullscreenControl()\n\n\n\n\nMap of aboveground forest biomass\n\n\nShown below is the mean live above-ground biomass aggregated by county so that it plays well with other metrics. Note that it is measured in tons per acre of forest, non-forest cells were removed from analysis. So, it is not showing density of forest, just biomass in existing forest. This is why the more urban counties still show a reasonable density of live biomass. There is lots more that can be pulled out of this dataset, like dead/down carbon, tree stocking, live canopy cover, height, volume, tree per acre, etc. More info can be found here.\n\n\nCode\npacman::p_load(\n  mapview,\n  dplyr,\n  sf,\n  viridisLite,\n  leaflet,\n  leafpop,\n  stars\n)\n\nbiomass &lt;- readRDS('data/sm_data.rds')[['mean_biomass']]\nbiomass_map &lt;- mapview(\n  biomass,\n  zcol = 'mean_biomass',\n  layer.name = 'Mean Live Above&lt;br&gt;Ground Biomass&lt;br&gt;(tons per acre)',\n  label = 'county_name',\n  popup = popupTable(\n    biomass,\n    zcol = c(\n      'county_name',\n      'mean_biomass'\n    ),\n    feature.id = FALSE,\n    row.numbers = FALSE\n  )\n)"
  },
  {
    "objectID": "pages/data_production.html",
    "href": "pages/data_production.html",
    "title": "Production",
    "section": "",
    "text": "The first plot shows all the production indicators from both the current studies and the original framework in the y-axis. Orange indicates that the indicator is only being used in the current studies, purple that it is only included in the Wiltshire framework, and green that the indicator is used in both the framework and current studies.\nThe x-axis shows the number of secondary data metrics that have been collected to represent those indicators. You can see that there are some indicators for which there exist many data, but many indicators for which I have found little to represent them.\nValue-added market indicators are pulled from various NASS, as are the total quantity of food and forest products and production inputs. There is plenty more that might be pulled from NASS here. Imports and exports are from the Economic Research Service. The exports data are far more detailed than the imports. The former are disaggregated by category at the state level (fresh fruit, processed fruit, dairyâ¦) which is why there are a heap of metrics for it. The import data is weak - I could only find the value of the top five agricultural imports for each state, not a total. Recalls are from FDA records, but I have not any helpful information the impact of recalls in terms of food safety. Crop diversity is represented in the richness indicator by the Cropland CROS data set, which provides estimates of the area of farmland devoted to specific crops across the US. I have disaggregated these at the county and state levels here.\nYou can see there is plenty more in the frameworks that are not represented by secondary data here, particularly related to the consumer side - marketability, nutrition, food waste, and safety. I suspect some of these indicators will migrate toward other dimensions in the refinement process as well. But this does help identify some gaps in the data.\nCode\npacman::p_load(\n  dplyr,\n  ggplot2,\n  stringr,\n  plotly,\n  RColorBrewer\n)\n\n# Load production tree with use notes\nprod_tree &lt;- read.csv('data/trees/prod_tree_with_use.csv')\n\n# Counts of secondary data metrics\ncounts &lt;- meta %&gt;% \n  group_by(Indicator) %&gt;% \n  dplyr::summarize(count = n())\n\n# Join to Wiltshire framework\ncolors &lt;- RColorBrewer::brewer.pal(n = 3, name = 'Dark2')\ndat &lt;- full_join(prod_tree, counts, by = join_by(Indicator == Indicator)) %&gt;% \n  arrange(Indicator) %&gt;% \n  mutate(\n    count = ifelse(is.na(count), 0, count),\n    label_color = case_when(\n      Use == 'both' ~ colors[1],\n      Use == 'wiltshire' ~ colors[3],\n      Use == 'current' ~ colors[2]\n    )\n  )\n# [1] \"#1B9E77\" \"#D95F02\" \"#7570B3\"\n\n# Plot\ndat %&gt;%\n  ggplot(aes(x = Indicator, y = count)) +\n  geom_col(\n    color = 'black',\n    fill = 'grey'\n  ) +\n  geom_point(\n    data = dat,\n    aes(x = 1, y = 1, color = Use),\n    inherit.aes = FALSE,\n    alpha = 0,\n    size = -1\n  ) +\n  scale_color_manual(\n    name = \"Indicator Use:\",\n    values = c(\n      \"both\" = colors[1],\n      \"wiltshire\" = colors[2],\n      \"current\" = colors[3]\n    ),\n    labels = c(\n      'Both',\n      'Wiltshire Only',\n      'Current Only'\n    )\n  ) +\n  theme_classic() +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.text.y = element_text(color = rev(dat$label_color)),\n    axis.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    legend.title = element_text(size = 12),\n    legend.position = \"bottom\",\n    plot.margin = margin(t = 10, r = 75, b = 10, l = 10)\n  ) +\n  guides(\n    color = guide_legend(override.aes = list(size = 4, alpha = 1))\n  ) +\n  coord_flip() +\n  labs(y = 'Secondary Data Count')\n\n\n\n\n\nBar Plot of Indicators\nOtherwise, I wonât be diving into the usual PCA exploration for the production dataset because we have collected enough metrics to put together a mostly full, mostly coherent example framework with which we can try aggregating data. This should be coming in January.",
    "crumbs": [
      "Secondary Data",
      "Production"
    ]
  },
  {
    "objectID": "pages/data_production.html#crop-diversity",
    "href": "pages/data_production.html#crop-diversity",
    "title": "Production",
    "section": "1 Crop Diversity",
    "text": "1 Crop Diversity\nI wanted to highlight this cropland data layer from USDA NASS in collaboration with USGS, NRCS, and FSA, among other agencies. Itâs a crop-specific LULC layer derived from satellite imagery and ground-truthing. It seems to be about the best thrust at crop diversity across regions that Iâve found, but it also is certainly tailored toward primary crops, and may not represent New England very well. Iâd love to hear thoughts on how useful this would be in New England.\n\n\nCode\npacman::p_load(\n  mapview,\n  sf,\n  stars,\n  leaflet,\n  leaflet.extras,\n  leafpop\n)\ncounties_sf &lt;- readRDS('data/sm_data.rds')[['ne_counties_2024']]\nfips_key &lt;- readRDS('data/sm_data.rds')[['fips_key']]\ncrop &lt;- readRDS('data/sm_data.rds')[['cropland_cros']]\n\ncounties &lt;- left_join(counties_sf, fips_key)\n\ndiv_map &lt;- mapview(\n  crop,\n  zcol = '2023_30m_cdls',\n  layer.name = 'Cropland Data Layer'\n) + \n  mapview(\n    counties,\n    label = 'county_name',\n    alpha.regions = 0\n  )\n\ndiv_map@map %&gt;% \n  addFullscreenControl()\n\n\n\n\nCrop diversity in New England\n\n\nI went on to use this layer to calculate Shannon diversity for crop types at the county and state levels. Here is what it looks like:\n\n\nCode\npacman::p_load(\n  mapview,\n  leaflet,\n  stringr,\n  sf\n)\nsource('dev/data_pipeline_functions.R')\n\ndat &lt;- readRDS('data/sm_data.rds')\n\ndiv &lt;- dat$metrics %&gt;% \n  filter(\n    variable_name == 'cropDiversity',\n    str_length(fips) == 5\n  ) %&gt;% \n  get_latest_year() %&gt;% \n  mutate(value = round(as.numeric(value), 3))\n\ndiv &lt;- left_join(dat$ne_counties_2021, div)\nmapview(\n  div,\n  zcol = 'value',\n  label = 'value',\n  layer.name = 'Crop Diversity'\n)\n\n\n\n\nShannon diversity for crop production at county level.\n\n\nSimilarly, we could pull crop richness out of this dataset, but I have a feeling that the bias toward commodity crops would make that a bit more problematic.",
    "crumbs": [
      "Secondary Data",
      "Production"
    ]
  },
  {
    "objectID": "pages/data_production.html#distribution-plots",
    "href": "pages/data_production.html#distribution-plots",
    "title": "Production",
    "section": "2 Distribution Plots",
    "text": "2 Distribution Plots\n\n2.1 By County\nNote that while most of the available secondary data is at the county level, the environment dimension includes a fair amount at the state level as well. This includes greenhouse gas emissions and water quality surveys. For now, Iâll just show these separately, but some creative aggregation will have to happen eventually.\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr\n)\nsource('dev/data_pipeline_functions.R')\nsource('dev/filter_fips.R')\nmetrics &lt;- readRDS('data/sm_data.rds')[['metrics']]\nmetadata &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\n# Use metadata to get help filter by dimension\nprod_meta &lt;- metadata %&gt;%\n  filter(dimension == 'production')\n\n# Filter to economics dimension\nprod_metrics &lt;- metrics %&gt;%\n  filter(variable_name %in% prod_meta$variable_name)\n\n# env_metrics$variable_name %&gt;% unique\n# get_str(env_metrics)\n\n# Filter to latest year and new (post-2024) counties\n# And pivot wider so it is easier to get correlations\nprod_county &lt;- prod_metrics %&gt;%\n  filter_fips(scope = 'counties') %&gt;%\n  get_latest_year() %&gt;%\n  select(fips, variable_name, value) %&gt;%\n  mutate(variable_name = str_split_i(variable_name, '_', 1)) %&gt;%\n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;%\n  unnest(!fips) %&gt;%\n  mutate(across(c(2:last_col()), as.numeric))\n\n# Save temp file for use in analysis script\nsaveRDS(prod_county, 'data/temp/prod_county.rds')\n\n## Plot\nplots &lt;- map(names(prod_county)[-1], \\(var){\n  if (is.character(prod_county[[var]])) {\n    env_county %&gt;%\n      ggplot(aes(x = !!sym(var))) +\n      geom_bar(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else if (is.numeric(prod_county[[var]])) {\n    prod_county %&gt;%\n      ggplot(aes(x = !!sym(var))) +\n      geom_density(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else {\n    return(NULL)\n  }\n})\n\n\n# Arrange them in 4 columns\nggarrange(\n  plotlist = plots,\n  ncol = 3,\n  nrow = 4\n)\n\n\n\n\n\nDistributions of production metrics at the county level.\n\n\n\n\n\n\n2.2 By State\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr\n)\n\nstate_codes &lt;- readRDS('data/sm_data.rds')[['fips_key']] %&gt;%\n  select(fips, state_code)\n\nprod_state &lt;- prod_metrics %&gt;%\n  filter_fips(scope = 'state') %&gt;%\n  get_latest_year() %&gt;%\n  select(fips, variable_name, value) %&gt;%\n  mutate(variable_name = str_split_i(variable_name, '_', 1)) %&gt;%\n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;%\n  unnest(!fips) %&gt;%\n  mutate(across(c(2:last_col()), as.numeric)) %&gt;%\n  left_join(state_codes, by = 'fips')\n\n# Save temp data file for use in analysis script\nsaveRDS(prod_state, 'data/temp/prod_state.rds')\n\n# Variables to map. \nvars &lt;- names(prod_state)[-c(1, 43)]\n\n## Plot\nplots &lt;- map(vars, \\(var){\n  prod_state %&gt;%\n    ggplot(aes(y = !!sym(var), x = state_code, color = state_code)) +\n    geom_point(\n      alpha = 0.5,\n      size = 3\n    ) +\n    theme_classic() +\n    theme(\n      plot.margin = unit(c(rep(0.5, 4)), 'cm'),\n      legend.position = 'none'\n    ) +\n    labs(\n      x = 'State'\n    )\n})\n\n# Arrange them in 4 columns\nggarrange(\n  plotlist = plots,\n  ncol = 4,\n  nrow = 11\n)\n\n\n\n\n\nDistributions of production variables at state level",
    "crumbs": [
      "Secondary Data",
      "Production"
    ]
  },
  {
    "objectID": "pages/data_production.html#bivariate-plots",
    "href": "pages/data_production.html#bivariate-plots",
    "title": "Production",
    "section": "3 Bivariate Plots",
    "text": "3 Bivariate Plots\nUsing a selection of variables at the county level.\n\n\nCode\npacman::p_load(\n  GGally\n)\n\n# Neat function for mapping colors to ggpairs plots\n# https://stackoverflow.com/questions/45873483/ggpairs-plot-with-heatmap-of-correlation-values\nmap_colors &lt;- function(data,\n                       mapping,\n                       method = \"p\",\n                       use = \"pairwise\",\n                       ...) {\n  # grab data\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n\n  # calculate correlation\n  corr &lt;- cor(x, y, method = method, use = use)\n  colFn &lt;- colorRampPalette(c(\"blue\", \"white\", \"red\"), interpolate = 'spline')\n  fill &lt;- colFn(100)[findInterval(corr, seq(-1, 1, length = 100))]\n\n  # correlation plot\n  ggally_cor(data = data, mapping = mapping, color = 'black', ...) +\n    theme_void() +\n    theme(panel.background = element_rect(fill = fill))\n}\n\nlower_function &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(color = \"blue\", fill = \"grey\", ...) +\n    theme_bw()\n}\n\n# Rename variables to be shorter\nprod_county %&gt;%\n  select(-fips) %&gt;% \n  ggpairs(\n    upper = list(continuous = map_colors),\n    lower = list(continuous = lower_function),\n    axisLabels = 'show'\n  ) +\n  theme(\n    strip.text = element_text(size =  5),\n    axis.text = element_text(size =   5),\n    legend.text = element_text(size = 5)\n  )",
    "crumbs": [
      "Secondary Data",
      "Production"
    ]
  },
  {
    "objectID": "pages/data_production.html#sec-correlations",
    "href": "pages/data_production.html#sec-correlations",
    "title": "Production",
    "section": "4 Correlations",
    "text": "4 Correlations\nOnly showing correlations by county because we donât have enough observations to run it by state.\n\n\nCode\npacman::p_load(\n  dplyr,\n  tidyr,\n  tibble,\n  stringr,\n  purrr,\n  tidyr,\n  ggplot2,\n  plotly,\n  reshape,\n  Hmisc,\n  viridisLite\n)\n\n# get_str(env_county)\n\ncor &lt;- prod_county %&gt;%\n  select(-fips) %&gt;%\n  as.matrix() %&gt;%\n  rcorr()\n\n# Melt correlation values and rename columns\ncor_r &lt;- melt(cor$r) %&gt;%\n  setNames(c('var_1', 'var_2', 'value'))\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Make heatmap with custom text aesthetic for tooltip\nplot &lt;- cor_r %&gt;%\n  ggplot(aes(var_1, var_2, fill = value, text = paste0(\n    'Var 1: ', var_1, '\\n',\n    'Var 2: ', var_2, '\\n',\n    'Correlation: ', format(round(value, 3), nsmall = 3), '\\n',\n    'P-Value: ', format(round(p.value, 3), nsmall = 3)\n  ))) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  theme(axis.text.x = element_text(hjust = 1, angle = 45)) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = 'Correlation'\n  )\n\n# Convert to interactive plotly figure with text tooltip\nggplotly(\n  plot,\n  tooltip = 'text',\n  width = 800,\n  height = 500\n)\n\n\n\n\nInteractive correlation plot of metrics by county",
    "crumbs": [
      "Secondary Data",
      "Production"
    ]
  },
  {
    "objectID": "pages/home.html",
    "href": "pages/home.html",
    "title": "Sustainability Metrics",
    "section": "",
    "text": "Caution\n\n\n\n\n\nThe Sustainability Metrics project, as well as this site itself, are works in progress. All data and analyses shown here are preliminary. If you have any questions, comments, or suggestions about this site or the accompanying Shiny app, feel free to reach out to Chris at christopher.donovan@uvm.edu.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/home.html#sec-intro",
    "href": "pages/home.html#sec-intro",
    "title": "Sustainability Metrics",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n\n\nIntervale Center, Burlington, Vermont. Copyright: Sally McCay, UVM Photo.\n\n\nResilient food systems are increasingly recognized as essential, not only in meeting human needs, but in doing so within planetary bounds (Conijn et al. 2018). Approximately 42% of worldâs population depend on agriculture for employment, which is a challenging endeavor in the face of farm consolidation, changing consumption patterns, and climate change (Giller et al. 2021; Aznar-SÃ¡nchez et al. 2019). Food systems themselves are responsible for one-third of greenhouse gas emissions, while anthropogenic climate change has reduced agricultural output by 21% in the last 60 years (Crippa et al. 2021; Ortiz-Bobea et al. 2021).\nMonitoring and adaptively managing the sustainability of food systems is thus vital. However, there is little consensus on how to define, let alone measure food system sustainability (Allen and Prosperi 2016; BÃ©nÃ© et al. 2019). And while there is an abundance of research at the global level (Bathaei and Å treimikienÄ 2023; Chaudhary, Gustafson, and Mathys 2018), there exist gaps in understanding at the local, regional, and landscape levels (Dale et al. 2012).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/home.html#sustainability-metrics",
    "href": "pages/home.html#sustainability-metrics",
    "title": "Sustainability Metrics",
    "section": "2 Sustainability Metrics",
    "text": "2 Sustainability Metrics\n\n\n\nSpread from the Climate Kitchen harvest dinner. Photo credit: Colleen Goodhue, FSRC.\n\n\nThe Sustainability Metrics project is an effort to develop both the conceptual and methodological frameworks to define and measure regional food system sustainability in New England. The framework could be used to monitor sustainability over time and inform interventions at the policy and farm levels, creating a healthier and more resilient food system for both social and ecological ends.\nThe project is led by the Food Systems Research Center at the University of Vermont in partnership with, and funded by, the USDA ARS Food Systems Research Unit in Burlington, Vermont. Five teams of researchers and numerous community partners are currently conducting primary research on the development and measurement of indicators for food system sustainability. You can find more information about this work at the UVM FSRC Sustainability Metrics website. For now, what you will find here is a growing collection of secondary data, visualizations, and exploratory analyses to help support the project.\nMetadata and citations will be provided throughout the document, but it is worth appreciating the work of the folks at USDA AMS Food and Agriculture Mapper and Explorer in particular, as many of the data shown here were cleaned and compiled in their data warehouse. Considerable inspiration was also taken from the Food Systems Dashboard, developed by the Global Alliance for Improved Nutrition.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/home.html#about-fsrc",
    "href": "pages/home.html#about-fsrc",
    "title": "Sustainability Metrics",
    "section": "3 About FSRC",
    "text": "3 About FSRC\nThe Food Systems Research Center at the University of Vermont is transforming the research landscape by funding collaborative projects that put people and the planet first, break down traditional academic silos and are integrated with and responsive to the needs of the communities we serve, including decision-makers, farmers, and food systems actors.\nRooted in the belief that no one group can find the answers alone, FSRC empowers researchers to work together across disciplines to address critical issues like soil health, food security, and climate resilience. Instead of funding research that leads to short-term fixes, our commitment is to give researchers the freedom, resources, and time they need to do relevant research that will inform policies, practices, and programs that will long outlast their work.\nFSRC considers the relationship of food systems across scales from local to global and is a partnership between UVM and the U.S. Department of Agriculture (USDA) Agricultural Research Service (ARS). FSRCâs transdisciplinary approach prioritizes research that studies food systems as a whole, including the networks of people, institutions, physical infrastructure, and natural resources through which food is grown, processed, distributed, sold, prepared, and eaten.\nLearn more about us at the Food Systems Research Center website.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/home.html#license",
    "href": "pages/home.html#license",
    "title": "Sustainability Metrics",
    "section": "4 License",
    "text": "4 License\n\n    This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. \n\n\n    The code is licensed under the GNU General Public License v3.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/refine_economics.html",
    "href": "pages/refine_economics.html",
    "title": "Economic Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the economics dimensions. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting. A final set of indicators to incorporate into the next RFP is still in the works!",
    "crumbs": [
      "Framework",
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refine_economics.html#indicator-progression",
    "href": "pages/refine_economics.html#indicator-progression",
    "title": "Economic Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework as described in the Wiltshire et al.Â paper.\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_wiltshire_tree.csv',\n  dimension_in = 'economics',\n  include_metrics = FALSE,\n  y_limits = c(-1.5, 2.1)\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_tree.csv',\n  dimension_in = 'economics',\n  y_limits = c(-1.5, 2.1)\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Refinement Meeting\nFinally, the tentative set of indicators following the indicator refinement meeting on November 15th, 2024\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_meeting_tree.csv',\n  dimension_in = 'economics',\n  y_limits = c(-1.5, 2.1)\n)",
    "crumbs": [
      "Framework",
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refine_economics.html#survey",
    "href": "pages/refine_economics.html#survey",
    "title": "Economic Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the economic indicator refinement meeting on November 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/econ_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    starts_with('Q'),\n    -ends_with('RANK')\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    paste0('add_indi_', 1:3),\n    'notes',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not',\n    paste0('add_idx_', 1:3),\n    'idx_notes',\n    'final_notes'\n  )) %&gt;% \n  .[-c(1:2), ]\n\ngroups &lt;- select(dat, indi_must:indi_must_not, idx_must:idx_probably_not)\n\nto_df &lt;- function(x) {\n  x %&gt;% \n    str_split(',') %&gt;% \n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n    arrange(desc(freq))\n}\n\nindi_out &lt;- map(groups[1:4], to_df)\nidx_out &lt;- map(groups[5:7], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\n\n\nCode\nggplot(graph_table, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so âMust Includeâ is worth 3 points, âProbably Includeâ is worth 2 points, âProbably Not Includeâ is worth 1 point, and âMust Not Includeâ is worth 0 points. Note that the last column is the sum of proportions of âMust Includeâ and âProbably Includeâ. You can sort, search, expand, or page through the table below.\n\n\nCode\n# Add category to tables\nprops &lt;- ind_tables %&gt;% \n  imap(~ .x %&gt;% mutate(cat = .y)) %&gt;% \n  bind_rows() %&gt;% \n  select(-score)\n \n# Get proportion of probably include OR must include\nprop_prob_or_must_include &lt;- props %&gt;% \n  filter(cat %in% c('indi_must', 'indi_probably')) %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(prop_include = sum(freq) / 6) %&gt;% \n  arrange(desc(prop_include))\n\n# Get proportion of must include\nprop_must_include &lt;- props %&gt;% \n  filter(cat == 'indi_must') %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(prop_must = sum(freq) / 6) %&gt;% \n  arrange(desc(prop_must))\n\n# Add up weighted scores\nind_scores &lt;- ind_tables %&gt;% \n  bind_rows() %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(score = sum(score, na.rm = TRUE)) %&gt;% \n  arrange(desc(score))\n\n# Join everything together\nscores_table &lt;- ind_scores %&gt;% \n  full_join(prop_must_include) %&gt;% \n  full_join(prop_prob_or_must_include) %&gt;% \n  arrange(desc(score)) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    across(c(3:4), ~ format(round(.x, 2), nsmall = 2))\n  ) %&gt;% \n  setNames(c('Indicator', 'Score', 'Proportion Must Include', 'Proportion Must OR Probably Include'))\n\n\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\nidx_out &lt;- map(groups[5:7], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:1)\nidx_tables &lt;- map2(idx_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:probably_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')[2:4]\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 16),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\"\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nThe indices are going through the same treatment as indicators above - scored from 3 to 0. Note that there were no indices rated as âMust Not Includeâ.",
    "crumbs": [
      "Framework",
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/sm-explorer.html",
    "href": "pages/sm-explorer.html",
    "title": "SM-Explorer",
    "section": "",
    "text": "Caution\n\n\n\n\n\nThe SM-Explorer is a work in progress. There are a small heap of bugs Iâm already aware of, and about a hundred things Iâd still like to add. If/when you find things that arenât working properly, please feel free to let Chris know!\n\n\n\nThis is a Shiny app that allows for interactive exploration of metrics, mostly at the county level. It includes a map page, a bivariate plot explorer, and a metadata table much like what is included in this Quarto doc. It tends to work best if you open it in its own page using the button below:\n\n\n\n\nGo To SM-Explorer\n\n\n\n\nYou can also just use it here in the window. Note that some functions (like the full screen button) wonât work here.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Secondary Data",
      "SM-Explorer"
    ]
  },
  {
    "objectID": "pages/min_max_dive.html",
    "href": "pages/min_max_dive.html",
    "title": "Variable Selection and Regression",
    "section": "",
    "text": "On this page we will take our min-max normalized, geometrically averaged scores, which look like the most reliable and approachable so far, and take a deeper dive into variable selection, regression, and PCA. From the dimension meetings, it sounds like we may have some indicators with a couple of metrics, and potentially others with dozens. Because of this, and because of our focus on developing sensible indicators, I think it will be best to do any weighting at the indicator level or above. This also reduces our variable count substantially in relation to our state count of 51, opening more doors for PCA.\nIt is worth emphasizing at the top that the metrics that are making up this secondary data framework are not a great representation of the system. There are some important holes, as well as a heap of metrics that are serving as rather uninspiring proxies. So, extrapolation of these results beyond the confines of the exercise is not recommended. The purpose here is to explore strengths and tradeoffs in methods for aggregating the data. As primary data come in and make up the bulk of the framework and secondary data are used to fill in the gaps, this should start becoming more interpretable."
  },
  {
    "objectID": "pages/min_max_dive.html#economics",
    "href": "pages/min_max_dive.html#economics",
    "title": "Variable Selection and Regression",
    "section": "3.1 Economics",
    "text": "3.1 Economics\n\n3.1.1 Linear Model\nFirst we can try a plain old linear model to see how economics loads onto its indicators.\n\n\nCode\n# Reduce data down to dimen_economics and all indicators\necon_dat &lt;- select(dat, dimen_economics, starts_with('indic')) %&gt;% \n  setNames(c(names(.) %&gt;% str_remove('indic_|dimen_')))\nget_str(econ_dat)\n\n\nrowws_df [51 Ã 28] (S3: rowwise_df/tbl_df/tbl/data.frame)\n $ economics                            : num [1:51] 0.0933 0.1706 0.1077 0.10..\n $ access_to_land                       : num [1:51] 0.0948 0.0366 0.025 0.155..\n $ wealth_income_distribution           : num [1:51] 0.336 0.36 0.343 0.342 0...\n $ operations_diversification           : num [1:51] 0.01315 0.39392 0.05679 0..\n $ income_stability                     : num [1:51] 0.0496 0.3594 0.3734 0.07..\n $ stocks                               : num [1:51] 0.1621 0.3616 0.0181 0.31..\n $ embodied                             : num [1:51] 0.05821 0.00648 0.14482 0..\n $ complexity                           : num [1:51] 0.5363 0.3491 0.053 0.476..\n $ health                               : num [1:51] 0.2028 0.4019 0.121 0.328..\n $ quantity                             : num [1:51] 0.1788 0.7638 0.1694 0.10..\n $ educational_attainment               : num [1:51] 0.2323 0.4223 0.4479 0.15..\n $ food_access                          : num [1:51] 0.2805 0.3153 0.0677 0.24..\n $ access_to_culturally_appropriate_food: num [1:51] 0.1563 0.3824 0.0818 0.07..\n $ dietary_quality                      : num [1:51] 0.4706 0.6929 0.2826 0.66..\n $ food_affordability                   : num [1:51] 0.798 0.362 0.306 0.768 0..\n $ food_security_tbd                    : num [1:51] 0.861 0.481 0.444 0.926 0..\n $ housing_supply_and_quality           : num [1:51] 0.46 0.596 0.508 0.345 0...\n $ physical_health_tbd                  : num [1:51] 0.368 0.511 0.377 0.327 0..\n $ richness                             : num [1:51] 0.5496 0.4202 0.6537 0.37..\n $ production_inputs                    : num [1:51] 0.2 1 0.381 0.6952 0.4667..\n $ total_quantity_food_products         : num [1:51] 0.9903 0.6528 0.4495 0.72..\n $ total_quantity_forest_products       : num [1:51] 0.635061 0.000087 0.00048..\n $ value_added_market                   : num [1:51] 0.01481 0.30841 0.04466 0..\n $ social_connectedness                 : num [1:51] 0.425 0.545 0.222 0.489 0..\n $ diverse_representation               : num [1:51] 0.498 0.449 0.284 0.648 0..\n $ gender_diversity                     : num [1:51] 0.274 0.9229 1 0.4825 0.4..\n $ age_diversity                        : num [1:51] 0.623 0.245 0.887 0.415 0..\n $ participatory_governance             : num [1:51] 0.3192 0.514 0.4967 0.011..\n\n\nCode\nlm &lt;- lm(economics ~ ., data = econ_dat)\nsummary(lm)\n\n\n\nCall:\nlm(formula = economics ~ ., data = econ_dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.046012 -0.009026  0.000915  0.010218  0.027461 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           -0.043017   0.052920  -0.813 0.424621    \naccess_to_land                         0.271718   0.064231   4.230 0.000317 ***\nwealth_income_distribution             0.194485   0.054301   3.582 0.001580 ** \noperations_diversification             0.070557   0.037680   1.873 0.073903 .  \nincome_stability                       0.077955   0.035707   2.183 0.039484 *  \nstocks                                -0.036121   0.039646  -0.911 0.371699    \nembodied                               0.023358   0.043587   0.536 0.597177    \ncomplexity                            -0.009612   0.030643  -0.314 0.756588    \nhealth                                 0.067347   0.047412   1.420 0.168888    \nquantity                               0.020691   0.026151   0.791 0.436898    \neducational_attainment                -0.045740   0.028220  -1.621 0.118686    \nfood_access                           -0.018855   0.037256  -0.506 0.617607    \naccess_to_culturally_appropriate_food  0.062033   0.029151   2.128 0.044273 *  \ndietary_quality                       -0.001036   0.022305  -0.046 0.963346    \nfood_affordability                    -0.118349   0.068263  -1.734 0.096354 .  \nfood_security_tbd                      0.058557   0.053843   1.088 0.288059    \nhousing_supply_and_quality             0.098798   0.038582   2.561 0.017476 *  \nphysical_health_tbd                    0.040881   0.056823   0.719 0.479113    \nrichness                               0.080891   0.027118   2.983 0.006651 ** \nproduction_inputs                     -0.097837   0.025241  -3.876 0.000765 ***\ntotal_quantity_food_products          -0.026093   0.025395  -1.028 0.314869    \ntotal_quantity_forest_products         0.016282   0.021888   0.744 0.464466    \nvalue_added_market                     0.106278   0.035569   2.988 0.006574 ** \nsocial_connectedness                   0.124042   0.047764   2.597 0.016119 *  \ndiverse_representation                 0.041104   0.026992   1.523 0.141443    \ngender_diversity                      -0.035572   0.027458  -1.296 0.207990    \nage_diversity                         -0.010597   0.020113  -0.527 0.603303    \nparticipatory_governance               0.003441   0.020826   0.165 0.870221    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02191 on 23 degrees of freedom\nMultiple R-squared:  0.9623,    Adjusted R-squared:  0.918 \nF-statistic: 21.73 on 27 and 23 DF,  p-value: 0.00000000007309\n\n\nWe can see that most of the economics indicators (access to land, wealth and income distribution, income stability) are significant predictors, while operations diversification is close. But some surprises are access to culturally appropriate food (school food authorities serving culturally relevant food), housing supply and quality, as well as a few production indicators, like richness (crop diversity), production inputs, and value-added markets. Social connectedness from the social dimension also makes it on the list. The largest coefficients by a wide margin are for access to land and wealth and income distribution.\n\n\n3.1.2 Splitting Data\nHere we split out data into a 60/40 training/test set for cross validation with GLMnet and Random Forest models. Note that we are pushing the limits of our sample size. But this should help protect against overfitting.\n\n\nCode\n# Split data 60/40\nset.seed(42)\nindices &lt;- createDataPartition(econ_dat$economics, p = 0.60, list = FALSE)\ntraining_data &lt;- econ_dat[indices, ]\ntesting_data &lt;- econ_dat[-indices,]\n\nmy_folds &lt;- createFolds(training_data$economics, k = 5, list = TRUE)\n\n# Control\nmy_control &lt;- trainControl(\n  method = 'cv',\n  number = 5,\n  verboseIter = TRUE,\n  index = my_folds\n)\n\n# Check for zero variance or near zero variance indicators\nnearZeroVar(dat, names = TRUE, saveMetrics = TRUE)\n# All clear\n\n\n\n\n3.1.3 GLMnet\nHere we use a GLMnet to find an optimal balance between a ridge regression, which penalizes variables based on the magnitude of coefficients, and lasso regression, which adds a penalty based on the absolute value of coefficients. We use a tuning grid to find optimal values of alpha (0 = ridge, 1 = lasso) and lambda (the penalty parameter). Both this and the random forest model are particularly good at prediction, but also provide a metric for variable importance that can help us interpret our indicators.\n\n\nCode\nset.seed(42)\necon_glmnet &lt;- train(\n  economics ~ .,\n  data = training_data, \n  tuneGrid = expand.grid(\n    alpha = seq(0.1, 1, length = 5),\n    lambda = seq(0.0001, 0.1, length = 100)\n  ),\n  method = \"glmnet\",\n  trControl = my_control,\n  preProcess = c('zv', 'center', 'scale')\n)\n\n\n+ Fold1: alpha=0.100, lambda=0.1 \n- Fold1: alpha=0.100, lambda=0.1 \n+ Fold1: alpha=0.325, lambda=0.1 \n- Fold1: alpha=0.325, lambda=0.1 \n+ Fold1: alpha=0.550, lambda=0.1 \n- Fold1: alpha=0.550, lambda=0.1 \n+ Fold1: alpha=0.775, lambda=0.1 \n- Fold1: alpha=0.775, lambda=0.1 \n+ Fold1: alpha=1.000, lambda=0.1 \n- Fold1: alpha=1.000, lambda=0.1 \n+ Fold2: alpha=0.100, lambda=0.1 \n- Fold2: alpha=0.100, lambda=0.1 \n+ Fold2: alpha=0.325, lambda=0.1 \n- Fold2: alpha=0.325, lambda=0.1 \n+ Fold2: alpha=0.550, lambda=0.1 \n- Fold2: alpha=0.550, lambda=0.1 \n+ Fold2: alpha=0.775, lambda=0.1 \n- Fold2: alpha=0.775, lambda=0.1 \n+ Fold2: alpha=1.000, lambda=0.1 \n- Fold2: alpha=1.000, lambda=0.1 \n+ Fold3: alpha=0.100, lambda=0.1 \n- Fold3: alpha=0.100, lambda=0.1 \n+ Fold3: alpha=0.325, lambda=0.1 \n- Fold3: alpha=0.325, lambda=0.1 \n+ Fold3: alpha=0.550, lambda=0.1 \n- Fold3: alpha=0.550, lambda=0.1 \n+ Fold3: alpha=0.775, lambda=0.1 \n- Fold3: alpha=0.775, lambda=0.1 \n+ Fold3: alpha=1.000, lambda=0.1 \n- Fold3: alpha=1.000, lambda=0.1 \n+ Fold4: alpha=0.100, lambda=0.1 \n- Fold4: alpha=0.100, lambda=0.1 \n+ Fold4: alpha=0.325, lambda=0.1 \n- Fold4: alpha=0.325, lambda=0.1 \n+ Fold4: alpha=0.550, lambda=0.1 \n- Fold4: alpha=0.550, lambda=0.1 \n+ Fold4: alpha=0.775, lambda=0.1 \n- Fold4: alpha=0.775, lambda=0.1 \n+ Fold4: alpha=1.000, lambda=0.1 \n- Fold4: alpha=1.000, lambda=0.1 \n+ Fold5: alpha=0.100, lambda=0.1 \n- Fold5: alpha=0.100, lambda=0.1 \n+ Fold5: alpha=0.325, lambda=0.1 \n- Fold5: alpha=0.325, lambda=0.1 \n+ Fold5: alpha=0.550, lambda=0.1 \n- Fold5: alpha=0.550, lambda=0.1 \n+ Fold5: alpha=0.775, lambda=0.1 \n- Fold5: alpha=0.775, lambda=0.1 \n+ Fold5: alpha=1.000, lambda=0.1 \n- Fold5: alpha=1.000, lambda=0.1 \n\n\nAggregating results\nSelecting tuning parameters\nFitting alpha = 0.1, lambda = 0.00313 on full training set\n\n\nCode\nimportance &lt;- varImp(econ_glmnet, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nCode\n# Predict\n# p &lt;- predict(econ_glmnet, testing_data)\n# postResample(pred = p, obs = testing_data$economics)\n\n\nThe optimal hyperparameters from the tuning grid were alpha = 0.1 (mostly ridge regression) and lambda = 0.00313. The variable importance plot is on a relative scale of 0 (unimportant) to 100 (most important) in terms of predictive power. Curiously, it is showing that the value added market indicator from the production dimension is a better predictor of economics than any economics indicator.\n\n\n3.1.4 Random Forest\nNow we can try a random forest, which is particularly good at handling non-linear relationships. Here we use the RMSE to determine the optimal combination of mtry (the number of variables selected at each node in the decision tree), the split rule, and the minimum node size.\n\n\nCode\nset.seed(42)\necon_rf &lt;- train(\n  economics ~ .,\n  data = training_data, \n  tuneLength = 7,\n  method = \"ranger\",\n  trControl = my_control,\n  importance = 'impurity'\n)\n\n\n+ Fold1: mtry= 2, min.node.size=5, splitrule=variance \n- Fold1: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold1: mtry= 6, min.node.size=5, splitrule=variance \n- Fold1: mtry= 6, min.node.size=5, splitrule=variance \n+ Fold1: mtry=10, min.node.size=5, splitrule=variance \n- Fold1: mtry=10, min.node.size=5, splitrule=variance \n+ Fold1: mtry=14, min.node.size=5, splitrule=variance \n- Fold1: mtry=14, min.node.size=5, splitrule=variance \n+ Fold1: mtry=18, min.node.size=5, splitrule=variance \n- Fold1: mtry=18, min.node.size=5, splitrule=variance \n+ Fold1: mtry=22, min.node.size=5, splitrule=variance \n- Fold1: mtry=22, min.node.size=5, splitrule=variance \n+ Fold1: mtry=27, min.node.size=5, splitrule=variance \n- Fold1: mtry=27, min.node.size=5, splitrule=variance \n+ Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry= 6, min.node.size=5, splitrule=extratrees \n- Fold1: mtry= 6, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=10, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=10, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=18, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=18, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=22, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=22, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=27, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=27, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry= 2, min.node.size=5, splitrule=variance \n- Fold2: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold2: mtry= 6, min.node.size=5, splitrule=variance \n- Fold2: mtry= 6, min.node.size=5, splitrule=variance \n+ Fold2: mtry=10, min.node.size=5, splitrule=variance \n- Fold2: mtry=10, min.node.size=5, splitrule=variance \n+ Fold2: mtry=14, min.node.size=5, splitrule=variance \n- Fold2: mtry=14, min.node.size=5, splitrule=variance \n+ Fold2: mtry=18, min.node.size=5, splitrule=variance \n- Fold2: mtry=18, min.node.size=5, splitrule=variance \n+ Fold2: mtry=22, min.node.size=5, splitrule=variance \n- Fold2: mtry=22, min.node.size=5, splitrule=variance \n+ Fold2: mtry=27, min.node.size=5, splitrule=variance \n- Fold2: mtry=27, min.node.size=5, splitrule=variance \n+ Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry= 6, min.node.size=5, splitrule=extratrees \n- Fold2: mtry= 6, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=10, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=10, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=18, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=18, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=22, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=22, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=27, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=27, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry= 2, min.node.size=5, splitrule=variance \n- Fold3: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold3: mtry= 6, min.node.size=5, splitrule=variance \n- Fold3: mtry= 6, min.node.size=5, splitrule=variance \n+ Fold3: mtry=10, min.node.size=5, splitrule=variance \n- Fold3: mtry=10, min.node.size=5, splitrule=variance \n+ Fold3: mtry=14, min.node.size=5, splitrule=variance \n- Fold3: mtry=14, min.node.size=5, splitrule=variance \n+ Fold3: mtry=18, min.node.size=5, splitrule=variance \n- Fold3: mtry=18, min.node.size=5, splitrule=variance \n+ Fold3: mtry=22, min.node.size=5, splitrule=variance \n- Fold3: mtry=22, min.node.size=5, splitrule=variance \n+ Fold3: mtry=27, min.node.size=5, splitrule=variance \n- Fold3: mtry=27, min.node.size=5, splitrule=variance \n+ Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry= 6, min.node.size=5, splitrule=extratrees \n- Fold3: mtry= 6, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=10, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=10, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=18, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=18, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=22, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=22, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=27, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=27, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry= 2, min.node.size=5, splitrule=variance \n- Fold4: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold4: mtry= 6, min.node.size=5, splitrule=variance \n- Fold4: mtry= 6, min.node.size=5, splitrule=variance \n+ Fold4: mtry=10, min.node.size=5, splitrule=variance \n- Fold4: mtry=10, min.node.size=5, splitrule=variance \n+ Fold4: mtry=14, min.node.size=5, splitrule=variance \n- Fold4: mtry=14, min.node.size=5, splitrule=variance \n+ Fold4: mtry=18, min.node.size=5, splitrule=variance \n- Fold4: mtry=18, min.node.size=5, splitrule=variance \n+ Fold4: mtry=22, min.node.size=5, splitrule=variance \n- Fold4: mtry=22, min.node.size=5, splitrule=variance \n+ Fold4: mtry=27, min.node.size=5, splitrule=variance \n- Fold4: mtry=27, min.node.size=5, splitrule=variance \n+ Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry= 6, min.node.size=5, splitrule=extratrees \n- Fold4: mtry= 6, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=10, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=10, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=18, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=18, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=22, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=22, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=27, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=27, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry= 2, min.node.size=5, splitrule=variance \n- Fold5: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold5: mtry= 6, min.node.size=5, splitrule=variance \n- Fold5: mtry= 6, min.node.size=5, splitrule=variance \n+ Fold5: mtry=10, min.node.size=5, splitrule=variance \n- Fold5: mtry=10, min.node.size=5, splitrule=variance \n+ Fold5: mtry=14, min.node.size=5, splitrule=variance \n- Fold5: mtry=14, min.node.size=5, splitrule=variance \n+ Fold5: mtry=18, min.node.size=5, splitrule=variance \n- Fold5: mtry=18, min.node.size=5, splitrule=variance \n+ Fold5: mtry=22, min.node.size=5, splitrule=variance \n- Fold5: mtry=22, min.node.size=5, splitrule=variance \n+ Fold5: mtry=27, min.node.size=5, splitrule=variance \n- Fold5: mtry=27, min.node.size=5, splitrule=variance \n+ Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry= 6, min.node.size=5, splitrule=extratrees \n- Fold5: mtry= 6, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=10, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=10, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=18, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=18, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=22, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=22, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=27, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=27, min.node.size=5, splitrule=extratrees \n\n\nAggregating results\nSelecting tuning parameters\nFitting mtry = 18, splitrule = extratrees, min.node.size = 5 on full training set\n\n\nCode\n# econ_rf\n# plot(econ_rf)\n\nimportance &lt;- varImp(econ_rf, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nCode\n# Predict\n# p &lt;- predict(model_mf, testing_data)\n# postResample(pred = p, obs = testing_data$rebl_tpm)\n\n\nThe random forest model is also picking out the value-added market indicator as the best predictor of economics dimension scores, followed closely by operations diversification, wealth and income distribution, and income stability."
  },
  {
    "objectID": "pages/min_max_dive.html#component-extraction",
    "href": "pages/min_max_dive.html#component-extraction",
    "title": "Variable Selection and Regression",
    "section": "2.1 Component Extraction",
    "text": "2.1 Component Extraction\n\n\nCode\npacman::p_load(\n  psych\n)\n\n# Filter down to just indicators for PCA\npca_dat &lt;- dat %&gt;% \n  select(starts_with('indic')) %&gt;% \n  setNames(c(str_remove(names(.), 'indic_')))\nget_str(pca_dat)\n\n\nrowws_df [51 Ã 27] (S3: rowwise_df/tbl_df/tbl/data.frame)\n $ access_to_land                       : num [1:51] 0.0948 0.0366 0.025 0.155..\n $ wealth_income_distribution           : num [1:51] 0.336 0.36 0.343 0.342 0...\n $ operations_diversification           : num [1:51] 0.01315 0.39392 0.05679 0..\n $ income_stability                     : num [1:51] 0.0496 0.3594 0.3734 0.07..\n $ stocks                               : num [1:51] 0.1621 0.3616 0.0181 0.31..\n $ embodied                             : num [1:51] 0.05821 0.00648 0.14482 0..\n $ complexity                           : num [1:51] 0.5363 0.3491 0.053 0.476..\n $ health                               : num [1:51] 0.2028 0.4019 0.121 0.328..\n $ quantity                             : num [1:51] 0.1788 0.7638 0.1694 0.10..\n $ educational_attainment               : num [1:51] 0.2323 0.4223 0.4479 0.15..\n $ food_access                          : num [1:51] 0.2805 0.3153 0.0677 0.24..\n $ access_to_culturally_appropriate_food: num [1:51] 0.1563 0.3824 0.0818 0.07..\n $ dietary_quality                      : num [1:51] 0.4706 0.6929 0.2826 0.66..\n $ food_affordability                   : num [1:51] 0.798 0.362 0.306 0.768 0..\n $ food_security_tbd                    : num [1:51] 0.861 0.481 0.444 0.926 0..\n $ housing_supply_and_quality           : num [1:51] 0.46 0.596 0.508 0.345 0...\n $ physical_health_tbd                  : num [1:51] 0.368 0.511 0.377 0.327 0..\n $ richness                             : num [1:51] 0.5496 0.4202 0.6537 0.37..\n $ production_inputs                    : num [1:51] 0.2 1 0.381 0.6952 0.4667..\n $ total_quantity_food_products         : num [1:51] 0.9903 0.6528 0.4495 0.72..\n $ total_quantity_forest_products       : num [1:51] 0.635061 0.000087 0.00048..\n $ value_added_market                   : num [1:51] 0.01481 0.30841 0.04466 0..\n $ social_connectedness                 : num [1:51] 0.425 0.545 0.222 0.489 0..\n $ diverse_representation               : num [1:51] 0.498 0.449 0.284 0.648 0..\n $ gender_diversity                     : num [1:51] 0.274 0.9229 1 0.4825 0.4..\n $ age_diversity                        : num [1:51] 0.623 0.245 0.887 0.415 0..\n $ participatory_governance             : num [1:51] 0.3192 0.514 0.4967 0.011..\n\n\nCode\n# Explore how many factors to extract\nVSS(pca_dat, n = 8, rotate = 'varimax')\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.54  with  5  factors\nVSS complexity 2 achieves a maximimum of 0.74  with  7  factors\n\nThe Velicer MAP achieves a minimum of 0.04  with  5  factors \nBIC achieves a minimum of  -707.01  with  2  factors\nSample Size adjusted BIC achieves a minimum of  43.19  with  8  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq                  prob sqresid  fit RMSEA  BIC SABIC\n1 0.42 0.00 0.046 324   594 0.0000000000000000041    38.4 0.42 0.126 -680   337\n2 0.49 0.59 0.039 298   465 0.0000000020261430581    27.4 0.59 0.103 -707   229\n3 0.51 0.65 0.040 273   407 0.0000002377063332005    21.1 0.68 0.096 -666   191\n4 0.53 0.70 0.039 249   346 0.0000454455701748708    15.2 0.77 0.085 -633   149\n5 0.54 0.73 0.037 226   282 0.0067253171779996842    11.0 0.83 0.067 -607   103\n6 0.52 0.73 0.038 204   238 0.0531261528725437693     8.6 0.87 0.053 -564    76\n7 0.50 0.74 0.042 183   208 0.0996491767252502686     7.0 0.89 0.048 -512    63\n8 0.50 0.74 0.042 163   172 0.2932560548556843449     5.6 0.92 0.027 -469    43\n  complex eChisq  SRMR eCRMS eBIC\n1     1.0    977 0.165 0.172 -297\n2     1.3    609 0.130 0.142 -562\n3     1.5    440 0.111 0.126 -633\n4     1.7    282 0.089 0.105 -697\n5     1.7    171 0.069 0.086 -718\n6     1.8    115 0.057 0.074 -687\n7     2.0     89 0.050 0.069 -631\n8     2.1     60 0.041 0.060 -581\n\n\nCode\nfa.parallel(pca_dat)\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  5 \n\n\nBoth MAP and Parallel Analysis determine that there should be 5 components.\n\n\nCode\npca_out &lt;- pca(pca_dat, nfactors = 5)\nplot(pca_out$values)\nabline(h = 1)\n\n\n\n\n\n\n\n\n\nThe scree plot is not all that convincing, though. One could make an argument for seven components."
  },
  {
    "objectID": "pages/min_max_dive.html#run-pca",
    "href": "pages/min_max_dive.html#run-pca",
    "title": "Variable Selection and Regression",
    "section": "2.2 Run PCA",
    "text": "2.2 Run PCA\nLetâs go ahead with MAP and PA, which are generally more reliable than scree plots.\n\n\nCode\npca_out\n\n\nPrincipal Components Analysis\nCall: principal(r = r, nfactors = nfactors, residuals = residuals, \n    rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, \n    missing = missing, impute = impute, oblique.scores = oblique.scores, \n    method = method, use = use, cor = cor, correct = 0.5, weight = NULL)\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                        RC1   RC2   RC3   RC5   RC4   h2   u2\naccess_to_land                        -0.06 -0.03  0.65 -0.31 -0.24 0.57 0.43\nwealth_income_distribution             0.68  0.05  0.09  0.34 -0.03 0.58 0.42\noperations_diversification             0.85 -0.19 -0.09 -0.08  0.19 0.81 0.19\nincome_stability                       0.69 -0.21 -0.09  0.26 -0.32 0.70 0.30\nstocks                                 0.04 -0.11  0.03  0.75 -0.11 0.59 0.41\nembodied                              -0.47 -0.22  0.56  0.14  0.07 0.61 0.39\ncomplexity                             0.11  0.23 -0.50  0.27  0.60 0.75 0.25\nhealth                                 0.34 -0.36 -0.03  0.68  0.07 0.72 0.28\nquantity                               0.12  0.05 -0.02  0.20  0.76 0.64 0.36\neducational_attainment                 0.37 -0.72  0.03  0.00 -0.09 0.66 0.34\nfood_access                            0.11 -0.06  0.31  0.67  0.11 0.57 0.43\naccess_to_culturally_appropriate_food  0.66 -0.05  0.20  0.07 -0.05 0.48 0.52\ndietary_quality                        0.45 -0.22 -0.24 -0.04 -0.02 0.31 0.69\nfood_affordability                    -0.13  0.89 -0.10 -0.03 -0.02 0.82 0.18\nfood_security_tbd                     -0.07  0.88  0.01 -0.06 -0.11 0.79 0.21\nhousing_supply_and_quality             0.41  0.16 -0.38  0.11  0.08 0.36 0.64\nphysical_health_tbd                   -0.08  0.64 -0.10  0.10  0.12 0.45 0.55\nrichness                              -0.03  0.11 -0.06  0.15 -0.79 0.66 0.34\nproduction_inputs                     -0.11  0.38  0.57  0.11  0.11 0.50 0.50\ntotal_quantity_food_products          -0.46  0.03 -0.55 -0.10 -0.11 0.54 0.46\ntotal_quantity_forest_products        -0.17  0.35 -0.25  0.57 -0.02 0.55 0.45\nvalue_added_market                     0.80 -0.09  0.10 -0.16  0.12 0.70 0.30\nsocial_connectedness                   0.00  0.53  0.00 -0.12  0.01 0.29 0.71\ndiverse_representation                 0.05  0.03  0.69  0.22  0.19 0.56 0.44\ngender_diversity                       0.60 -0.09 -0.18  0.09 -0.04 0.41 0.59\nage_diversity                          0.27  0.11 -0.24  0.15 -0.60 0.52 0.48\nparticipatory_governance              -0.10 -0.70 -0.23  0.10  0.21 0.61 0.39\n                                      com\naccess_to_land                        1.8\nwealth_income_distribution            1.5\noperations_diversification            1.3\nincome_stability                      2.0\nstocks                                1.1\nembodied                              2.5\ncomplexity                            2.8\nhealth                                2.1\nquantity                              1.2\neducational_attainment                1.5\nfood_access                           1.6\naccess_to_culturally_appropriate_food 1.2\ndietary_quality                       2.1\nfood_affordability                    1.1\nfood_security_tbd                     1.1\nhousing_supply_and_quality            2.5\nphysical_health_tbd                   1.2\nrichness                              1.1\nproduction_inputs                     2.0\ntotal_quantity_food_products          2.1\ntotal_quantity_forest_products        2.3\nvalue_added_market                    1.2\nsocial_connectedness                  1.1\ndiverse_representation                1.4\ngender_diversity                      1.3\nage_diversity                         2.0\nparticipatory_governance              1.5\n\n                       RC1  RC2  RC3  RC5  RC4\nSS loadings           4.36 3.97 2.69 2.43 2.31\nProportion Var        0.16 0.15 0.10 0.09 0.09\nCumulative Var        0.16 0.31 0.41 0.50 0.58\nProportion Explained  0.28 0.25 0.17 0.15 0.15\nCumulative Proportion 0.28 0.53 0.70 0.85 1.00\n\nMean item complexity =  1.6\nTest of the hypothesis that 5 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.08 \n with the empirical chi square  227.74  with prob &lt;  0.46 \n\nFit based upon off diagonal values = 0.89\n\n\nThere is a lot to look at here, but here are some impressions.\nMost of our current economics indicators (access to land -&gt; income stability), are coalescing into RC1. However, access to land splits out into RC3, alongside environment indicators like carbon stocks and forest health, as well as production indicators like inputs and total quantity of food products. The access to land indicator does not have an ideal set of metrics under it currently - it is made up of land and building value per farm and acreage per farm as a proxy for access. So it seems reasonable enough that this indicator is pulling away from the rest of the economics indicators.\nThe environment indicators (carbon stocks -&gt; water quantity) mostly make up RC5, but are also split between several other components. The carbon stocks and forest health indicators are currently both derived from the USFS TreeMap dataset, so it makes sense they fall together in RC5. This is a particularly scattered dimension though - perhaps partly from hinky proxy metrics and partly for the diversity of the dimension itself.\nHealth indicators (education -&gt; physical health) fall pretty consistently into RC2. A notable exception is food access, which falls into RC5 along with TreeMap data. I have a hunch that RC5 is measuring the rural/urban divide at this point. The culturally appropriate food indicator also moves over to RC1 alongside economics indicators. Given that this indicator currently consists only of the ability of School Food Authorities to provide culturally appropriate foods, this is probably tracking with wealth.\nProduction indicators (richness -&gt; value added markets) fall mostly into RC2. However, richness (measured by crop diversity) swings out into RC4, which is a bit of a grab bag but does contain some forest indicators. The economics dimension RC1 also loads strongly onto the value added markets indicator.\nFinally, the social dimension is scattered all over the place. I canât make much sense of it as a whole, given that it has a pretty unsatisfactory set of metrics behind it at this point.\nThe next step for this PCA path is to see how things look if we aggregate our indices and dimensions based on these PCA loadings in the style of Nicoletti (2000), rather than working with the arithmetic/geometric means as we do below."
  },
  {
    "objectID": "pages/selection.html",
    "href": "pages/selection.html",
    "title": "Variable Selection and Regression",
    "section": "",
    "text": "On this page we will take our min-max normalized, geometrically averaged scores, which look like the most reliable and approachable so far, and take a deeper dive into variable selection, regression, and PCA. From the dimension meetings, it sounds like we may have some indicators with a couple of metrics, and potentially others with dozens. Because of this, and because of our focus on developing sensible indicators, I think it will be best to do any weighting at the indicator level or above. This also reduces our variable count substantially in relation to our state count of 51, opening more doors for PCA.\nIt is worth emphasizing at the top that the metrics that are making up this secondary data framework are not a great representation of the system. There are some important holes, as well as a heap of metrics that are serving as rather uninspiring proxies. So, extrapolation of these results beyond the confines of the exercise is not recommended. The purpose here is to explore strengths and tradeoffs in methods for aggregating the data. As primary data come in and make up the bulk of the framework and secondary data are used to fill in the gaps, this should start becoming more interpretable.",
    "crumbs": [
      "Secondary Data Rework",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/selection.html#component-extraction",
    "href": "pages/selection.html#component-extraction",
    "title": "Variable Selection and Regression",
    "section": "2.1 Component Extraction",
    "text": "2.1 Component Extraction\n\n\nCode\npacman::p_load(\n  psych\n)\n\n# Filter down to just indicators for PCA\npca_dat &lt;- dat %&gt;% \n  select(starts_with('indic')) %&gt;% \n  setNames(c(str_remove(names(.), 'indic_')))\n# get_str(pca_dat)\n\n# Explore how many factors to extract\nVSS(pca_dat, n = 8, rotate = 'varimax')\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.54  with  5  factors\nVSS complexity 2 achieves a maximimum of 0.74  with  7  factors\n\nThe Velicer MAP achieves a minimum of 0.04  with  5  factors \nBIC achieves a minimum of  -707.01  with  2  factors\nSample Size adjusted BIC achieves a minimum of  43.19  with  8  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq                  prob sqresid  fit RMSEA  BIC SABIC\n1 0.42 0.00 0.046 324   594 0.0000000000000000041    38.4 0.42 0.126 -680   337\n2 0.49 0.59 0.039 298   465 0.0000000020261430581    27.4 0.59 0.103 -707   229\n3 0.51 0.65 0.040 273   407 0.0000002377063332005    21.1 0.68 0.096 -666   191\n4 0.53 0.70 0.039 249   346 0.0000454455701748708    15.2 0.77 0.085 -633   149\n5 0.54 0.73 0.037 226   282 0.0067253171779996842    11.0 0.83 0.067 -607   103\n6 0.52 0.73 0.038 204   238 0.0531261528725437693     8.6 0.87 0.053 -564    76\n7 0.50 0.74 0.042 183   208 0.0996491767252502686     7.0 0.89 0.048 -512    63\n8 0.50 0.74 0.042 163   172 0.2932560548556843449     5.6 0.92 0.027 -469    43\n  complex eChisq  SRMR eCRMS eBIC\n1     1.0    977 0.165 0.172 -297\n2     1.3    609 0.130 0.142 -562\n3     1.5    440 0.111 0.126 -633\n4     1.7    282 0.089 0.105 -697\n5     1.7    171 0.069 0.086 -718\n6     1.8    115 0.057 0.074 -687\n7     2.0     89 0.050 0.069 -631\n8     2.1     60 0.041 0.060 -581\n\n\nCode\nfa.parallel(pca_dat)\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  5 \n\n\nBoth MAP and Parallel Analysis determine that there should be 5 components.\n\n\nCode\npca_out &lt;- pca(pca_dat, nfactors = 5)\nplot(pca_out$values)\nabline(h = 1)\n\n\n\n\n\n\n\n\n\nThe scree plot is not all that convincing, though. One could make an argument for seven components.",
    "crumbs": [
      "Secondary Data Rework",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/selection.html#run-pca",
    "href": "pages/selection.html#run-pca",
    "title": "Variable Selection and Regression",
    "section": "2.2 Run PCA",
    "text": "2.2 Run PCA\nLetâs go ahead with the recommended components from MAP and PA, which are generally more reliable than scree plots.\n\n\nCode\npca_out\n\n\nPrincipal Components Analysis\nCall: principal(r = r, nfactors = nfactors, residuals = residuals, \n    rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, \n    missing = missing, impute = impute, oblique.scores = oblique.scores, \n    method = method, use = use, cor = cor, correct = 0.5, weight = NULL)\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                        RC1   RC2   RC3   RC5   RC4   h2   u2\naccess_to_land                        -0.06 -0.03  0.65 -0.31 -0.24 0.57 0.43\nwealth_income_distribution             0.68  0.05  0.09  0.34 -0.03 0.58 0.42\noperations_diversification             0.85 -0.19 -0.09 -0.08  0.19 0.81 0.19\nincome_stability                       0.69 -0.21 -0.09  0.26 -0.32 0.70 0.30\nstocks                                 0.04 -0.11  0.03  0.75 -0.11 0.59 0.41\nembodied                              -0.47 -0.22  0.56  0.14  0.07 0.61 0.39\ncomplexity                             0.11  0.23 -0.50  0.27  0.60 0.75 0.25\nhealth                                 0.34 -0.36 -0.03  0.68  0.07 0.72 0.28\nquantity                               0.12  0.05 -0.02  0.20  0.76 0.64 0.36\neducational_attainment                 0.37 -0.72  0.03  0.00 -0.09 0.66 0.34\nfood_access                            0.11 -0.06  0.31  0.67  0.11 0.57 0.43\naccess_to_culturally_appropriate_food  0.66 -0.05  0.20  0.07 -0.05 0.48 0.52\ndietary_quality                        0.45 -0.22 -0.24 -0.04 -0.02 0.31 0.69\nfood_affordability                    -0.13  0.89 -0.10 -0.03 -0.02 0.82 0.18\nfood_security_tbd                     -0.07  0.88  0.01 -0.06 -0.11 0.79 0.21\nhousing_supply_and_quality             0.41  0.16 -0.38  0.11  0.08 0.36 0.64\nphysical_health_tbd                   -0.08  0.64 -0.10  0.10  0.12 0.45 0.55\nrichness                              -0.03  0.11 -0.06  0.15 -0.79 0.66 0.34\nproduction_inputs                     -0.11  0.38  0.57  0.11  0.11 0.50 0.50\ntotal_quantity_food_products          -0.46  0.03 -0.55 -0.10 -0.11 0.54 0.46\ntotal_quantity_forest_products        -0.17  0.35 -0.25  0.57 -0.02 0.55 0.45\nvalue_added_market                     0.80 -0.09  0.10 -0.16  0.12 0.70 0.30\nsocial_connectedness                   0.00  0.53  0.00 -0.12  0.01 0.29 0.71\ndiverse_representation                 0.05  0.03  0.69  0.22  0.19 0.56 0.44\ngender_diversity                       0.60 -0.09 -0.18  0.09 -0.04 0.41 0.59\nage_diversity                          0.27  0.11 -0.24  0.15 -0.60 0.52 0.48\nparticipatory_governance              -0.10 -0.70 -0.23  0.10  0.21 0.61 0.39\n                                      com\naccess_to_land                        1.8\nwealth_income_distribution            1.5\noperations_diversification            1.3\nincome_stability                      2.0\nstocks                                1.1\nembodied                              2.5\ncomplexity                            2.8\nhealth                                2.1\nquantity                              1.2\neducational_attainment                1.5\nfood_access                           1.6\naccess_to_culturally_appropriate_food 1.2\ndietary_quality                       2.1\nfood_affordability                    1.1\nfood_security_tbd                     1.1\nhousing_supply_and_quality            2.5\nphysical_health_tbd                   1.2\nrichness                              1.1\nproduction_inputs                     2.0\ntotal_quantity_food_products          2.1\ntotal_quantity_forest_products        2.3\nvalue_added_market                    1.2\nsocial_connectedness                  1.1\ndiverse_representation                1.4\ngender_diversity                      1.3\nage_diversity                         2.0\nparticipatory_governance              1.5\n\n                       RC1  RC2  RC3  RC5  RC4\nSS loadings           4.36 3.97 2.69 2.43 2.31\nProportion Var        0.16 0.15 0.10 0.09 0.09\nCumulative Var        0.16 0.31 0.41 0.50 0.58\nProportion Explained  0.28 0.25 0.17 0.15 0.15\nCumulative Proportion 0.28 0.53 0.70 0.85 1.00\n\nMean item complexity =  1.6\nTest of the hypothesis that 5 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.08 \n with the empirical chi square  227.74  with prob &lt;  0.46 \n\nFit based upon off diagonal values = 0.89\n\n\nThere is a lot to look at here, but here are some impressions.\nMost of our current economics indicators (access to land to income stability), are coalescing into RC1. However, access to land splits out into RC3, alongside environment indicators like carbon stocks and forest health, as well as production indicators like inputs and total quantity of food products. The access to land indicator does not have an ideal set of metrics under it currently - it is made up of land and building value per farm and acreage per farm as a proxy for access. So it seems reasonable enough that this indicator is pulling away from the rest of the economics indicators.\nThe environment indicators (carbon stocks to water quantity) mostly make up RC5, but are also split between several other components. The carbon stocks and forest health indicators are currently both derived from the USFS TreeMap dataset, so it makes sense they fall together in RC5. This is a particularly scattered dimension though - perhaps partly from hinky proxy metrics and partly for the diversity of the dimension itself.\nHealth indicators (education to physical health) fall pretty consistently into RC2. A notable exception is food access, which falls into RC5 along with TreeMap data. I have a hunch that RC5 is measuring the rural/urban divide at this point. The culturally appropriate food indicator also moves over to RC1 alongside economics indicators. Given that this indicator currently consists only of the ability of School Food Authorities to provide culturally appropriate foods, this is probably tracking with wealth.\nProduction indicators (richness to value added markets) fall mostly into RC2. However, richness (measured by crop diversity) swings out into RC4, which is a bit of a grab bag but does contain some forest indicators. The economics dimension RC1 also loads strongly onto the value added markets indicator.\nFinally, the social dimension is scattered all over the place. I canât make much sense of it as a whole, given that it has a pretty unsatisfactory set of metrics behind it at this point.\nThe next step for this PCA path is to see how things look if we aggregate our indices and dimensions based on these PCA loadings in the style of Nicoletti (2000), rather than working with the arithmetic/geometric means as we do below.",
    "crumbs": [
      "Secondary Data Rework",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/selection.html#economics",
    "href": "pages/selection.html#economics",
    "title": "Variable Selection and Regression",
    "section": "3.1 Economics",
    "text": "3.1 Economics\n\n3.1.1 Linear Model\nFirst we can try a plain old linear model to see how economics loads onto its indicators.\n\n\nCode\n# Reduce data down to dimen_economics and all indicators\necon_dat &lt;- select(dat, dimen_economics, starts_with('indic')) %&gt;% \n  setNames(c(names(.) %&gt;% str_remove('indic_|dimen_')))\nget_str(econ_dat)\n\n\nrowws_df [51 Ã 28] (S3: rowwise_df/tbl_df/tbl/data.frame)\n $ economics                            : num [1:51] 0.0933 0.1706 0.1077 0.10..\n $ access_to_land                       : num [1:51] 0.0948 0.0366 0.025 0.155..\n $ wealth_income_distribution           : num [1:51] 0.336 0.36 0.343 0.342 0...\n $ operations_diversification           : num [1:51] 0.01315 0.39392 0.05679 0..\n $ income_stability                     : num [1:51] 0.0496 0.3594 0.3734 0.07..\n $ stocks                               : num [1:51] 0.1621 0.3616 0.0181 0.31..\n $ embodied                             : num [1:51] 0.05821 0.00648 0.14482 0..\n $ complexity                           : num [1:51] 0.5363 0.3491 0.053 0.476..\n $ health                               : num [1:51] 0.2028 0.4019 0.121 0.328..\n $ quantity                             : num [1:51] 0.1788 0.7638 0.1694 0.10..\n $ educational_attainment               : num [1:51] 0.2323 0.4223 0.4479 0.15..\n $ food_access                          : num [1:51] 0.2805 0.3153 0.0677 0.24..\n $ access_to_culturally_appropriate_food: num [1:51] 0.1563 0.3824 0.0818 0.07..\n $ dietary_quality                      : num [1:51] 0.4706 0.6929 0.2826 0.66..\n $ food_affordability                   : num [1:51] 0.798 0.362 0.306 0.768 0..\n $ food_security_tbd                    : num [1:51] 0.861 0.481 0.444 0.926 0..\n $ housing_supply_and_quality           : num [1:51] 0.46 0.596 0.508 0.345 0...\n $ physical_health_tbd                  : num [1:51] 0.368 0.511 0.377 0.327 0..\n $ richness                             : num [1:51] 0.5496 0.4202 0.6537 0.37..\n $ production_inputs                    : num [1:51] 0.2 1 0.381 0.6952 0.4667..\n $ total_quantity_food_products         : num [1:51] 0.9903 0.6528 0.4495 0.72..\n $ total_quantity_forest_products       : num [1:51] 0.635061 0.000087 0.00048..\n $ value_added_market                   : num [1:51] 0.01481 0.30841 0.04466 0..\n $ social_connectedness                 : num [1:51] 0.425 0.545 0.222 0.489 0..\n $ diverse_representation               : num [1:51] 0.498 0.449 0.284 0.648 0..\n $ gender_diversity                     : num [1:51] 0.274 0.9229 1 0.4825 0.4..\n $ age_diversity                        : num [1:51] 0.623 0.245 0.887 0.415 0..\n $ participatory_governance             : num [1:51] 0.3192 0.514 0.4967 0.011..\n\n\nCode\nlm &lt;- lm(economics ~ ., data = econ_dat)\nsummary(lm)\n\n\n\nCall:\nlm(formula = economics ~ ., data = econ_dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.046012 -0.009026  0.000915  0.010218  0.027461 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           -0.043017   0.052920  -0.813 0.424621    \naccess_to_land                         0.271718   0.064231   4.230 0.000317 ***\nwealth_income_distribution             0.194485   0.054301   3.582 0.001580 ** \noperations_diversification             0.070557   0.037680   1.873 0.073903 .  \nincome_stability                       0.077955   0.035707   2.183 0.039484 *  \nstocks                                -0.036121   0.039646  -0.911 0.371699    \nembodied                               0.023358   0.043587   0.536 0.597177    \ncomplexity                            -0.009612   0.030643  -0.314 0.756588    \nhealth                                 0.067347   0.047412   1.420 0.168888    \nquantity                               0.020691   0.026151   0.791 0.436898    \neducational_attainment                -0.045740   0.028220  -1.621 0.118686    \nfood_access                           -0.018855   0.037256  -0.506 0.617607    \naccess_to_culturally_appropriate_food  0.062033   0.029151   2.128 0.044273 *  \ndietary_quality                       -0.001036   0.022305  -0.046 0.963346    \nfood_affordability                    -0.118349   0.068263  -1.734 0.096354 .  \nfood_security_tbd                      0.058557   0.053843   1.088 0.288059    \nhousing_supply_and_quality             0.098798   0.038582   2.561 0.017476 *  \nphysical_health_tbd                    0.040881   0.056823   0.719 0.479113    \nrichness                               0.080891   0.027118   2.983 0.006651 ** \nproduction_inputs                     -0.097837   0.025241  -3.876 0.000765 ***\ntotal_quantity_food_products          -0.026093   0.025395  -1.028 0.314869    \ntotal_quantity_forest_products         0.016282   0.021888   0.744 0.464466    \nvalue_added_market                     0.106278   0.035569   2.988 0.006574 ** \nsocial_connectedness                   0.124042   0.047764   2.597 0.016119 *  \ndiverse_representation                 0.041104   0.026992   1.523 0.141443    \ngender_diversity                      -0.035572   0.027458  -1.296 0.207990    \nage_diversity                         -0.010597   0.020113  -0.527 0.603303    \nparticipatory_governance               0.003441   0.020826   0.165 0.870221    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02191 on 23 degrees of freedom\nMultiple R-squared:  0.9623,    Adjusted R-squared:  0.918 \nF-statistic: 21.73 on 27 and 23 DF,  p-value: 0.00000000007309\n\n\nWe can see that most of the economics indicators (access to land, wealth and income distribution, income stability) are significant predictors, while operations diversification is close. But some surprises are access to culturally appropriate food (school food authorities serving culturally relevant food), housing supply and quality, as well as a few production indicators, like richness (crop diversity), production inputs, and value-added markets. Social connectedness from the social dimension also makes it on the list. The largest coefficients by a wide margin are for access to land and wealth and income distribution.\n\n\n3.1.2 Splitting Data\nHere we split out data into a 60/40 training/test set for cross validation with GLMnet and Random Forest models. Note that we are pushing the limits of our sample size. But this should help protect against overfitting.\n\n\nCode\n# Split data 60/40\nset.seed(42)\nindices &lt;- createDataPartition(econ_dat$economics, p = 0.60, list = FALSE)\ntraining_data &lt;- econ_dat[indices, ]\ntesting_data &lt;- econ_dat[-indices,]\n\nmy_folds &lt;- createFolds(training_data$economics, k = 5, list = TRUE)\n\n# Control\nmy_control &lt;- trainControl(\n  method = 'cv',\n  number = 5,\n  verboseIter = TRUE,\n  index = my_folds\n)\n\n# Check for zero variance or near zero variance indicators\nnearZeroVar(dat, names = TRUE, saveMetrics = TRUE)\n# All clear\n\n\n\n\n3.1.3 GLMnet\nHere we use a GLMnet to find an optimal balance between a ridge regression, which penalizes variables based on the magnitude of coefficients, and lasso regression, which adds a penalty based on the absolute value of coefficients. We use a tuning grid to find optimal values of alpha (0 = ridge, 1 = lasso) and lambda (the penalty parameter). Both this and the random forest model are particularly good at prediction, but also provide a metric for variable importance that can help us interpret our indicators.\n\n\nCode\nset.seed(42)\necon_glmnet &lt;- train(\n  economics ~ .,\n  data = training_data, \n  tuneGrid = expand.grid(\n    alpha = seq(0.1, 1, length = 5),\n    lambda = seq(0.0001, 0.1, length = 100)\n  ),\n  method = \"glmnet\",\n  trControl = my_control,\n  preProcess = c('zv', 'center', 'scale')\n)\n\n\n+ Fold1: alpha=0.100, lambda=0.1 \n- Fold1: alpha=0.100, lambda=0.1 \n+ Fold1: alpha=0.325, lambda=0.1 \n- Fold1: alpha=0.325, lambda=0.1 \n+ Fold1: alpha=0.550, lambda=0.1 \n- Fold1: alpha=0.550, lambda=0.1 \n+ Fold1: alpha=0.775, lambda=0.1 \n- Fold1: alpha=0.775, lambda=0.1 \n+ Fold1: alpha=1.000, lambda=0.1 \n- Fold1: alpha=1.000, lambda=0.1 \n+ Fold2: alpha=0.100, lambda=0.1 \n- Fold2: alpha=0.100, lambda=0.1 \n+ Fold2: alpha=0.325, lambda=0.1 \n- Fold2: alpha=0.325, lambda=0.1 \n+ Fold2: alpha=0.550, lambda=0.1 \n- Fold2: alpha=0.550, lambda=0.1 \n+ Fold2: alpha=0.775, lambda=0.1 \n- Fold2: alpha=0.775, lambda=0.1 \n+ Fold2: alpha=1.000, lambda=0.1 \n- Fold2: alpha=1.000, lambda=0.1 \n+ Fold3: alpha=0.100, lambda=0.1 \n- Fold3: alpha=0.100, lambda=0.1 \n+ Fold3: alpha=0.325, lambda=0.1 \n- Fold3: alpha=0.325, lambda=0.1 \n+ Fold3: alpha=0.550, lambda=0.1 \n- Fold3: alpha=0.550, lambda=0.1 \n+ Fold3: alpha=0.775, lambda=0.1 \n- Fold3: alpha=0.775, lambda=0.1 \n+ Fold3: alpha=1.000, lambda=0.1 \n- Fold3: alpha=1.000, lambda=0.1 \n+ Fold4: alpha=0.100, lambda=0.1 \n- Fold4: alpha=0.100, lambda=0.1 \n+ Fold4: alpha=0.325, lambda=0.1 \n- Fold4: alpha=0.325, lambda=0.1 \n+ Fold4: alpha=0.550, lambda=0.1 \n- Fold4: alpha=0.550, lambda=0.1 \n+ Fold4: alpha=0.775, lambda=0.1 \n- Fold4: alpha=0.775, lambda=0.1 \n+ Fold4: alpha=1.000, lambda=0.1 \n- Fold4: alpha=1.000, lambda=0.1 \n+ Fold5: alpha=0.100, lambda=0.1 \n- Fold5: alpha=0.100, lambda=0.1 \n+ Fold5: alpha=0.325, lambda=0.1 \n- Fold5: alpha=0.325, lambda=0.1 \n+ Fold5: alpha=0.550, lambda=0.1 \n- Fold5: alpha=0.550, lambda=0.1 \n+ Fold5: alpha=0.775, lambda=0.1 \n- Fold5: alpha=0.775, lambda=0.1 \n+ Fold5: alpha=1.000, lambda=0.1 \n- Fold5: alpha=1.000, lambda=0.1 \n\n\nAggregating results\nSelecting tuning parameters\nFitting alpha = 0.1, lambda = 0.00313 on full training set\n\n\nCode\nimportance &lt;- varImp(econ_glmnet, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nCode\n# Predict\n# p &lt;- predict(econ_glmnet, testing_data)\n# postResample(pred = p, obs = testing_data$economics)\n\n\nThe optimal hyperparameters from the tuning grid were alpha = 0.1 (mostly ridge regression) and lambda = 0.00313. The variable importance plot is on a relative scale of 0 (unimportant) to 100 (most important) in terms of predictive power. Curiously, it is showing that the value added market indicator from the production dimension is a better predictor of economics than any economics indicator.\n\n\n3.1.4 Random Forest\nNow we can try a random forest, which is particularly good at handling non-linear relationships. Here we use the RMSE to determine the optimal combination of mtry (the number of variables selected at each node in the decision tree), the split rule, and the minimum node size.\n\n\nCode\nset.seed(42)\necon_rf &lt;- train(\n  economics ~ .,\n  data = training_data, \n  tuneLength = 7,\n  method = \"ranger\",\n  trControl = my_control,\n  importance = 'impurity'\n)\n\n\n+ Fold1: mtry= 2, min.node.size=5, splitrule=variance \n- Fold1: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold1: mtry= 6, min.node.size=5, splitrule=variance \n- Fold1: mtry= 6, min.node.size=5, splitrule=variance \n+ Fold1: mtry=10, min.node.size=5, splitrule=variance \n- Fold1: mtry=10, min.node.size=5, splitrule=variance \n+ Fold1: mtry=14, min.node.size=5, splitrule=variance \n- Fold1: mtry=14, min.node.size=5, splitrule=variance \n+ Fold1: mtry=18, min.node.size=5, splitrule=variance \n- Fold1: mtry=18, min.node.size=5, splitrule=variance \n+ Fold1: mtry=22, min.node.size=5, splitrule=variance \n- Fold1: mtry=22, min.node.size=5, splitrule=variance \n+ Fold1: mtry=27, min.node.size=5, splitrule=variance \n- Fold1: mtry=27, min.node.size=5, splitrule=variance \n+ Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry= 6, min.node.size=5, splitrule=extratrees \n- Fold1: mtry= 6, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=10, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=10, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=18, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=18, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=22, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=22, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=27, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=27, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry= 2, min.node.size=5, splitrule=variance \n- Fold2: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold2: mtry= 6, min.node.size=5, splitrule=variance \n- Fold2: mtry= 6, min.node.size=5, splitrule=variance \n+ Fold2: mtry=10, min.node.size=5, splitrule=variance \n- Fold2: mtry=10, min.node.size=5, splitrule=variance \n+ Fold2: mtry=14, min.node.size=5, splitrule=variance \n- Fold2: mtry=14, min.node.size=5, splitrule=variance \n+ Fold2: mtry=18, min.node.size=5, splitrule=variance \n- Fold2: mtry=18, min.node.size=5, splitrule=variance \n+ Fold2: mtry=22, min.node.size=5, splitrule=variance \n- Fold2: mtry=22, min.node.size=5, splitrule=variance \n+ Fold2: mtry=27, min.node.size=5, splitrule=variance \n- Fold2: mtry=27, min.node.size=5, splitrule=variance \n+ Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry= 6, min.node.size=5, splitrule=extratrees \n- Fold2: mtry= 6, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=10, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=10, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=18, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=18, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=22, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=22, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=27, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=27, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry= 2, min.node.size=5, splitrule=variance \n- Fold3: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold3: mtry= 6, min.node.size=5, splitrule=variance \n- Fold3: mtry= 6, min.node.size=5, splitrule=variance \n+ Fold3: mtry=10, min.node.size=5, splitrule=variance \n- Fold3: mtry=10, min.node.size=5, splitrule=variance \n+ Fold3: mtry=14, min.node.size=5, splitrule=variance \n- Fold3: mtry=14, min.node.size=5, splitrule=variance \n+ Fold3: mtry=18, min.node.size=5, splitrule=variance \n- Fold3: mtry=18, min.node.size=5, splitrule=variance \n+ Fold3: mtry=22, min.node.size=5, splitrule=variance \n- Fold3: mtry=22, min.node.size=5, splitrule=variance \n+ Fold3: mtry=27, min.node.size=5, splitrule=variance \n- Fold3: mtry=27, min.node.size=5, splitrule=variance \n+ Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry= 6, min.node.size=5, splitrule=extratrees \n- Fold3: mtry= 6, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=10, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=10, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=18, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=18, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=22, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=22, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=27, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=27, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry= 2, min.node.size=5, splitrule=variance \n- Fold4: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold4: mtry= 6, min.node.size=5, splitrule=variance \n- Fold4: mtry= 6, min.node.size=5, splitrule=variance \n+ Fold4: mtry=10, min.node.size=5, splitrule=variance \n- Fold4: mtry=10, min.node.size=5, splitrule=variance \n+ Fold4: mtry=14, min.node.size=5, splitrule=variance \n- Fold4: mtry=14, min.node.size=5, splitrule=variance \n+ Fold4: mtry=18, min.node.size=5, splitrule=variance \n- Fold4: mtry=18, min.node.size=5, splitrule=variance \n+ Fold4: mtry=22, min.node.size=5, splitrule=variance \n- Fold4: mtry=22, min.node.size=5, splitrule=variance \n+ Fold4: mtry=27, min.node.size=5, splitrule=variance \n- Fold4: mtry=27, min.node.size=5, splitrule=variance \n+ Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry= 6, min.node.size=5, splitrule=extratrees \n- Fold4: mtry= 6, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=10, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=10, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=18, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=18, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=22, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=22, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=27, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=27, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry= 2, min.node.size=5, splitrule=variance \n- Fold5: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold5: mtry= 6, min.node.size=5, splitrule=variance \n- Fold5: mtry= 6, min.node.size=5, splitrule=variance \n+ Fold5: mtry=10, min.node.size=5, splitrule=variance \n- Fold5: mtry=10, min.node.size=5, splitrule=variance \n+ Fold5: mtry=14, min.node.size=5, splitrule=variance \n- Fold5: mtry=14, min.node.size=5, splitrule=variance \n+ Fold5: mtry=18, min.node.size=5, splitrule=variance \n- Fold5: mtry=18, min.node.size=5, splitrule=variance \n+ Fold5: mtry=22, min.node.size=5, splitrule=variance \n- Fold5: mtry=22, min.node.size=5, splitrule=variance \n+ Fold5: mtry=27, min.node.size=5, splitrule=variance \n- Fold5: mtry=27, min.node.size=5, splitrule=variance \n+ Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry= 6, min.node.size=5, splitrule=extratrees \n- Fold5: mtry= 6, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=10, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=10, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=18, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=18, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=22, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=22, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=27, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=27, min.node.size=5, splitrule=extratrees \n\n\nAggregating results\nSelecting tuning parameters\nFitting mtry = 18, splitrule = extratrees, min.node.size = 5 on full training set\n\n\nCode\n# econ_rf\n# plot(econ_rf)\n\nimportance &lt;- varImp(econ_rf, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nCode\n# Predict\n# p &lt;- predict(model_mf, testing_data)\n# postResample(pred = p, obs = testing_data$rebl_tpm)\n\n\nThe random forest model is also picking out the value-added market indicator as the best predictor of economics dimension scores, followed closely by operations diversification, wealth and income distribution, and income stability.\nVery curious how value-added markets keep sticking out. The two metrics making up this indicator are both from NASS: the percentage of farms reporting value-added sales, and of those farms, the percentage of value-added sales out of total sales.",
    "crumbs": [
      "Secondary Data Rework",
      "Selection and Regression"
    ]
  }
]