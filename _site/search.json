[
  {
    "objectID": "preso/preso.html#introduction",
    "href": "preso/preso.html#introduction",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nGoals for secondary data:\n\nIdentify existing data and gaps\nExplore methods of aggregating data\n\nStructure of presentation:\n\nFramework and methods\nPreliminary results of case study\nUncertainty and sensitivity\n\nGuiding questions:\n\nHow well do the data represesent the system?\nValuation, scaling, weighting, aggregation\nWhere and how do we incorporate qualitative data?\n\n\n\n\n\n\nIntervale Farm, Sally McCay, UVM Photo\n\n\n\n\n\nThe point is to take stock of existing data, swap out and fill in with primary research where necessary.\nSkeleton of a framework."
  },
  {
    "objectID": "preso/preso.html#framework",
    "href": "preso/preso.html#framework",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Framework",
    "text": "Framework\n\nEconomicsEnvironmentHealthProductionSocial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNONE is a placeholder for when I don’t have a metric there\nMany of these are not ideal. All ears for more datasets"
  },
  {
    "objectID": "preso/preso.html#framework-secondary-data",
    "href": "preso/preso.html#framework-secondary-data",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Framework: Secondary Data",
    "text": "Framework: Secondary Data\n\n\n\n\n\nDownload as CSV\n\n\n\n\n\n\n\n\n~650 metrics, available on website\nSet of 130 to match refined framework\n\nSources\n\nUSDA NASS: income, expenses, labor, size, d2c sales\nUSDA ERS Farm Income and Wealth Statistics: imports, exports, margin protection payments, emergency program payments, indemnities, capital expenditures and consumption\nUSDA ERS Food Environment Atlas: WIC, SNAP, grocery store density\nUSDA Farm Service Agency Disaster Assistance: emergency declarations (proxy for EWE)\nUSDA Food and Nutrition Service Farm to School program: SNAP and WIC eligibility rates, coverage rates, school food authority spending, farm to school program, culturally relevant foods\nU Wisconsin Population health institute, county health rankings and roadmaps: all health stuff, social associations, connectedness, voter turnout, health care infrastructure\nUS Census ACS: earnings for FFF and Food service jobs, rental vacancy rates, income inequality\nEPA State GHG Data: emissions form agriculture (N20, CO2, CH4)\nEPA National Aquatic Surveys: water quality, conditions based on phosphorus, nitrogen, habitat complexity,\nTreeMap 2016: carbon stocks, forest health\nNatureServe - biodiversity data, ecosystem data"
  },
  {
    "objectID": "preso/preso.html#methods-desirable-directions",
    "href": "preso/preso.html#methods-desirable-directions",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Methods: Desirable Directions",
    "text": "Methods: Desirable Directions\n\n\n\nHow do we decide if values for an indicator are good or bad?\nSchneider et al. (2023) gave every indicator a positive and negative direction\nJacobi et al. (2020) used official benchmarks where possible, otherwise the largest value set to 100%\nOthers have used the distance to a reference system (Adriaanse 1993)"
  },
  {
    "objectID": "preso/preso.html#methods-transformations",
    "href": "preso/preso.html#methods-transformations",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Methods: Transformations",
    "text": "Methods: Transformations\n\nRawWinsorBox Cox\n\n\n\n\nRaw Values\n\nDo nothing\nStrengths:\n\nTransparent\nUnbiased\n\nWeaknesses:\n\nVulnerable to outliers\nHarder to distinguish between similar systems\n\nHammond et al. (2017) use raw values in RHoMIS survey to compare sustainability across food systems\n\n\n\n\n\nHammond et al. (2017)\n\n\n\n\n\nWinsorization\n\nReduce extreme values to a percentile (95th and 5th)\nStrengths:\n\nDoesn’t reward overperformance\nAllows for more robust analyses without outliers (Mayer 2008)\n\nWeaknesses\n\nLoss of information\nNon-linear transformation of data\n\nUsed in the Environmental Performance Indicator (EPI) before scaling from 0 to 100 (D. Esty 2008)\n\n\n\nBox Cox (Bickel and Doksum 1981)\n\nFinds an optimal value of \\(\\lambda\\) to normalize distribution\n\n\\({\\rm For}\\ \\lambda\\neq0,\\ f\\lambda(x) = (sign(x)|x|^\\lambda-1)/\\lambda\\)\n\\({\\rm For}\\ \\lambda = 0,\\ f_0(x) = log(x)\\)\n\nStrengths:\n\nMore tractable distributions for analysis\n\nWeaknesses:\n\nChallenging to interpret\nChanges relationships between variables\n\nBéné et al. (2019) used Box Cox transformations on skewed indicators (&gt;2) before using min-max scaling."
  },
  {
    "objectID": "preso/preso.html#methods-rescaling",
    "href": "preso/preso.html#methods-rescaling",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Methods: Rescaling",
    "text": "Methods: Rescaling\n\nRank OrderMin MaxZ-ScoresDistance*\n\n\n\n\nRank Order\n\nRank against other states (50 is best, 1 is worst)\nStrengths:\n\nMakes no distributional assumptions\nUseful for comparing other transformation methods\n\nWeaknesses:\n\nLoss of information\n\nUsed by Schneider et al. (2025) for lack of robust data\n\n\n\n\n\nSchneider et al. (2023)\n\n\n\n\n\n\n\nMin Max (OECD 2008)\n\nScales all data from 0 (worst) to 1 (best)\n\n\\(\\Large I^t_qc = \\frac{x^t_qc - min_c(x^{t_0}_q)}{max_c(x^{t_0}_q)-min_c(x^{t_0}_q)}\\)\nWhere \\(x^t_qc\\) is the metric \\(q\\) for state \\(c\\) at time \\(t\\).\n\nStrengths:\n\nIntitive, approachable\nLinear transformation preserves relationships\n\nWeaknesses:\n\nSusceptible to outliers\n\nWidely used in sustainability composite indices Béné et al. (2019)\n\n\n\n\n\nJacobi et al. (2020)\n\n\n\n\n\nZ-Scores (OECD 2008)\n\nScales data to mean of 0 and standard deviation of 1\n\n\\(I^t_{qc} = \\frac{x^t_{qc}-x^t_{qc=\\overline{c}}}{\\sigma^t_{qc=\\overline{c}}}\\)\n\nLarger numbers are better, but no limits\nStrengths:\n\nLinear transformation\nMakes empirical sense\n\nWeaknesses:\n\nHarder to interpret and communicate\nSometimes incompatible with other methods\n\n\n\n\nDistance to Target (OECD 2008)\n\nDimensionless ratio of the indicator to a reference system or reference value\n\n\\(I^t_qc = \\frac{x^t_qc}{x^t_{qc=\\overline{c}}}\\)\n\nUsed with official benchmarks like minimum wage (Jacobi et al. 2020)\nStrengths:\n\nFlexible and intuitive\nMore nuanced and holistic\n\nWeaknesses:\n\nChallenging to set targets\nStakeholder involvement\n\n\n\n\n\n\n\nSchneider: average country rankings per theme, by grouped. Min max distance from group to global mean.\nJacobi: minmax - three food systems in Kenya (56 indicators)"
  },
  {
    "objectID": "preso/preso.html#methods-aggregation-and-weighting",
    "href": "preso/preso.html#methods-aggregation-and-weighting",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Methods: Aggregation and Weighting",
    "text": "Methods: Aggregation and Weighting\n\nArithmeticGeometricBudget*PCA*Others*\n\n\nArithmetic Means\n\nPlain old average\nStrengths:\n\nSimple, interpretable\nKnow how it’s wrong\n\nWeaknesses:\n\nCompensatory method - make up for underperformance in one area with overperformance in another\nImplicit equal weighting\n\nCommonly used in composite index literature (Jacobi et al. 2020)\n\n\n\nGeometric Means\n\n\\(n\\)th root of the product of values\n\n\\(\\Large \\sqrt[n]{x_1 * x_2 * ... * x_n}\\)\n\nStrengths:\n\nPartially compensable\nRewards even performance across areas\n\nWeaknesses:\n\nNot compatible with negative values*\nLimits to variance-based sensitivity analysis\n\nGómez-Limón and Sanchez-Fernandez (2010) compared results of both arithmetic and geometric means\nBéné et al. (2019) used arithmetic means for social and food dimensions, geometric means for environmental and economic dimensions\n\n\n\nBudget Allocation (Nardo et al. 2005)\n\nWeight based on expert or stakeholder opinion\n\nAllocate a budget of 100 ‘points’ to distribute among indicators\nTake means of allocations to get weights\nDiscussion, iteration, convergence\n\nStrengths:\n\nLeverage knowledge and experience\nFlexible and holistic\n\nWeaknesses:\n\nParticipant selection\nLimited generalizability\n\n\n\n\nPrincipal Components Analysis\n\nReduces variables to a smaller set of uncorrelated components\nLoadings show contribution of variables to component\nStrengths:\n\nEmpirical, quantitative results\n\nWeaknesses:\n\nExtraction\nInterpretation\nIncompatible with a priori hypotheses\n\nUse loadings to weight indicators, calculate weighted sum as domain score (Nicoletti 2000)\nD. C. Esty, Granoff, and Ruth (2002) tried PCA to develop composite but found the components were uninterpretable\n\n\n\nAnalytic Hierarchy Process\n\nAnalysis of pairwise preferences\nQuantify tradeoffs between indicators\n\nConjoint Analysis\n\nSurvey-based\nStated preference research\nDiscrete choice analysis\n\nMulticriteria Approach (Nardo et al. 2005)\n\nNon-compensatory, based on pair-wise ratio comparisons between units\n\nData Envelopment Analysis (OECD 2008)\n\nUse an efficiency frontier to define relative performance of countries"
  },
  {
    "objectID": "preso/preso.html#methods-comparisons",
    "href": "preso/preso.html#methods-comparisons",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Methods: Comparisons",
    "text": "Methods: Comparisons\n\nRaw RankWinsor RankBox Cox RankRaw Min MaxWinsor Min MaxBox Cox Min MaxRaw Z-ScoreWinsor Z-ScoreBox Cox Z-Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now have 18 combinations of transformation, scaling, aggregation\nNo one way is objectively better than any other - just exploring options\nNote about spider graph/radar chart: misleading, do not interpret area or order.\nUS state median in red\nTRANSITION INTO CASE STUDY OF MIN MAX GEO"
  },
  {
    "objectID": "preso/preso.html#case-study-dimension-score-maps",
    "href": "preso/preso.html#case-study-dimension-score-maps",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Case Study: Dimension Score Maps",
    "text": "Case Study: Dimension Score Maps\nRaw + min max scaling + geometric aggregation\n\nEconomicsEnvironmentHealthProductionSocial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Why state level? So that we have some benchmarks to compare VT to. Allows us to use empirical methods.\nThis is where I want qualitative data\nEnvironment: NV is low because of TreeMap data - scaled to 0.\nHealth: Mississippi is low - life expectancy, infant mortality, access to grocery\nArkansas: ?"
  },
  {
    "objectID": "preso/preso.html#case-study-indicator-correlations",
    "href": "preso/preso.html#case-study-indicator-correlations",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Case Study: Indicator Correlations",
    "text": "Case Study: Indicator Correlations\nRaw + min max scaling + geometric aggregation\n\nCorrelation MatrixInfluential Indicators\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighly correlating indicators from different dimensions are a problem (double counting)\nNegative correlations within same dimension means measuring different things\nNote: social dimension has strong positive AND negative correlations\nInfluential indicators: Most track with GDP and pop, social connectedness is pretty high (social associations, disconnected youth, single parent households)\nTRANSITION TO VALIDATION"
  },
  {
    "objectID": "preso/preso.html#case-study-validation-by-cronbach",
    "href": "preso/preso.html#case-study-validation-by-cronbach",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Case Study: Validation by Cronbach",
    "text": "Case Study: Validation by Cronbach\nRaw + min max scaling + geometric aggregation\n\n\n\nCronbach’s Alpha is a measure of internal reliability or consistency (Cronbach 1951)\nArbitrary guideline of 0.7 to validate a measurement tool (Taber 2018)\n\n\\[\\alpha=\\frac{n}{n-1}\\left( 1-\\frac{\\sum_{i}^{V_i}}{V_t} \\right)\\]\nWhere \\(n\\) is the number of items, \\(V_t\\) is the variance of item \\(i\\), and \\(V_t\\) is the variance of all items\n\nIt is not supposed to be negative.\n\n\n\n\n\n\n\n\n\n\n\nWarning: Unconventional use of Cronbach, and it is fraught at best\nContext for Cronbach: not necessarily what we want, just one way to show consistency within dimension.\nMight be okay if dimension is scattered\nTau equivalence - assume equal loadings, but can have unequal variance\nSplit half reliability - average of correlations between subscales, all combinations"
  },
  {
    "objectID": "preso/preso.html#case-study-validation-by-regression",
    "href": "preso/preso.html#case-study-validation-by-regression",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Case Study: Validation by Regression",
    "text": "Case Study: Validation by Regression\nRaw + min max scaling + geometric aggregation\n\nFood InsecurityLife ExpectancyFood Environment Index\n\n\nFood Insecurity Index (UW Population Health Institute 2024)\n\n\n\n\n\n\nLife Expectancy (UW Population Health Institute 2024)\n\n\n\n\n\n\nFood Environment Index (UW Population Health Institute 2024)\n\n\nRandom Forest Regression\nTuning Parameters:\n\nMtry = 6\nSplit Rule = extratrees\nMin Node Size = 5\n\nPerformance:\n\nRMSE = 0.822\nR^2 = 0.762\nMAE = 0.686\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFood Environment Index: 0 (worst) 10 (best). Distance to grocery, cost of health diet.\nRF Importance: difference (MSE) between accuracy with and without feature, averaged across all trees, normalized by standard error"
  },
  {
    "objectID": "preso/preso.html#case-study-validation-by-pca",
    "href": "preso/preso.html#case-study-validation-by-pca",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Case Study: Validation by PCA",
    "text": "Case Study: Validation by PCA\n\n\n\nPrincipal Components Analysis is a dimension reduction method\nCharacterize variance of many variables using smaller set of components\nGuideline: loadings &gt; 0.32 are meaningful (Tabachnick, Fidell, and Ullman 2019)\nKey to loadings:\n\nx &lt; 0.2 ~ ’ ’\nx &lt; 0.32 ~ ‘.’\nx &gt;= 0.32 ~ x\n\nJacobi et al. (2020) use PCA to validate dimensions, but still have indicators cross-loading or loading onto no components\n\n\n\n\n\n\n\n\n\n\n\nPA: randomize rows, do PCA. Keep PCs that explain significantly more variance than expected by chance\nMAP: get average squared partial correlations for each PC. Keep PCs that lead to lowest average squared partial correlation\nVSS: compare fit of simplified model to original correlations. VSS = 1-sumsquares(r*)/sumsquares(r). Peaks at optimal (most interpretable) number of factors.\nPromax is best interpretation, but not great\nLoadings: correlation coefficients between variables and components\nEigenvalue (ss loadings): varaince explained by component\nCommunality (h2): proportion of common variance present in variable\nUniqueness (u2): unique variable for each var (1-h2)\nComplexity (com): how well it reflects a construct (1 for 1, loading evenly on 2 is 2)"
  },
  {
    "objectID": "preso/preso.html#uncertainty-dimensions",
    "href": "preso/preso.html#uncertainty-dimensions",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Uncertainty: Dimensions",
    "text": "Uncertainty: Dimensions\n\n\n\nSample from uncertain inputs (OECD 2008)\n\nTransformations (3)\nRescaling methods (3)\nAggregation methods (2)\nLeave out one indicator (39)\n= 702 combinations\n\nHigher ranks are desirable\nSome dimensions are quite unstable (Economics)\nSome are quite stable (Health)\n\n\n\nEconomicsEnvironmentHealthProductionSocial\n\n\nDistribution for Vermont\n\n\n\n\n\n\n\n\n\n\n\nDistribution for Vermont\n\n\n\n\n\n\n\n\n\n\n\nDistribution for Vermont\n\n\n\n\n\n\n\n\n\n\n\nDistribution for Vermont\n\n\n\n\n\n\n\n\n\n\n\nDistribution for Vermont"
  },
  {
    "objectID": "preso/preso.html#sensitivity-indicators",
    "href": "preso/preso.html#sensitivity-indicators",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Sensitivity: Indicators",
    "text": "Sensitivity: Indicators\n\n\n\nSteps:\n\nRun all 18 methods without the indicator\nRun all 18 methods with the indicator\nCalculate average change in VT dimension rank\n\nShows how influential an indicator is on Vermont dimension scores\n\n\n\nEconomicsEnvironmentHealthProductionSocial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is VT centric.\nCould also see how each indicator changes all states on average"
  },
  {
    "objectID": "preso/preso.html#conclusions",
    "href": "preso/preso.html#conclusions",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Conclusions",
    "text": "Conclusions\n\n\n\nFindings:\n\nGaps in secondary data\nFramework complexity and coherence\nStability and sensitivity\nInfluential indicators\n\nLooking ahead:\n\nFind more info on metrics and analysis at the Sustainability Metrics methods website\nShare your thoughts on:\n\nQuality and relevance of secondary data sources\nOther secondary sources to fill in gaps\nFramework methodology and how to represent your findings\n\nSustainability Metrics repository is approved!\n\n\nThank you!\n\n \n\n\n\nGaps:\n\nSoil health, carbon stocks, water quality and quantity\nFood distribution, loss, and waste\nDiversity of farm types, production diversity\nDietary quality and nutrition\nSocial dimension\nOthers: NASS representativeness, updates (TreeMap)\n\nComplexity and coherence:\n\nCronbach: Some are coherent (health, and lesser extent economics and environment)\nIs this important to us? What does this mean for social dimension?\n\nSensitivity:\n\nSome dimensions stable (health)\nOthers ambiguous (economics)\nMetric counts: fewer means risk of poor fit\nDiagnostic for indicator influence (production species diversity)\n\nMetric counts: fewer increases risks?\nInfluential indicators:\n\nCarbon fluxes (10 interactions)\nAccess to care, value added market (6)\nOperations diversification, phyiscal health, production quantities (5)\nMore about GDP? Need to deal with this\nSocial connectedness (5)\nFrom indicator sensitivity: social connectedness and production diversity"
  },
  {
    "objectID": "preso/preso.html#directions-for-discussion",
    "href": "preso/preso.html#directions-for-discussion",
    "title": "Sustainability MetricsSecondary Data",
    "section": "Directions for Discussion",
    "text": "Directions for Discussion\n\nHow well do these data represent the system? What should be changed or added?\nHow should we assign values to metrics and indicators?\nWhat are fair and interpretable methods of transformations and aggregation?\nWhere and how should we incorporate qualitative data?\nWhat does this mean for shaping the next RFP?"
  },
  {
    "objectID": "preso/preso.html#references",
    "href": "preso/preso.html#references",
    "title": "Sustainability MetricsSecondary Data",
    "section": "References",
    "text": "References\n\n\n\n\nAdriaanse, A. 1993. “Environmental Policy Performance Indicators,” May. https://www.osti.gov/etdeweb/biblio/6352065.\n\n\nBéné, Christophe, Steven D. Prager, Harold A. E. Achicanoy, Patricia Alvarez Toro, Lea Lamotte, Camila Bonilla, and Brendan R. Mapes. 2019. “Global Map and Indicators of Food System Sustainability.” Scientific Data 6 (1): 279. https://doi.org/10.1038/s41597-019-0301-5.\n\n\nBickel, Peter J., and Kjell A. Doksum. 1981. “An Analysis of Transformations Revisited.” Journal of the American Statistical Association 76 (374): 296–311. https://doi.org/10.1080/01621459.1981.10477649.\n\n\nCronbach, Lee J. 1951. “Coefficient Alpha and the Internal Structure of Tests.” Psychometrika 16 (3): 297–334. https://doi.org/10.1007/BF02310555.\n\n\nEsty, Daniel. 2008. “Pilot 2006 Environmental Performance Index.” Yale Center for Environmental Law & Policy, January, 45-2621-45-2621. https://doi.org/10.5860/CHOICE.45-2621.\n\n\nEsty, Daniel C, Ilmi M E Granoff, and Barbara Ruth. 2002. “Yale Center for Environmental Law and Policy (YCELP).”\n\n\nGómez-Limón, José A., and Gabriela Sanchez-Fernandez. 2010. “Empirical Evaluation of Agricultural Sustainability Using Composite Indicators.” Ecological Economics 69 (5): 1062–75. https://doi.org/10.1016/j.ecolecon.2009.11.027.\n\n\nHammond, James, Simon Fraval, Jacob van Etten, Jose Gabriel Suchini, Leida Mercado, Tim Pagella, Romain Frelat, et al. 2017. “The Rural Household Multi-Indicator Survey (RHoMIS) for Rapid Characterisation of Households to Inform Climate Smart Agriculture Interventions: Description and Applications in East Africa and Central America.” Agricultural Systems 151 (February): 225–33. https://doi.org/10.1016/j.agsy.2016.05.003.\n\n\nJacobi, Johanna, Stellah Mukhovi, Aymara Llanque, Markus Giger, Adriana Bessa, Christophe Golay, Chinwe Ifejika Speranza, et al. 2020. “A New Understanding and Evaluation of Food Sustainability in Six Different Food Systems in Kenya and Bolivia.” Scientific Reports 10 (1): 19145. https://doi.org/10.1038/s41598-020-76284-y.\n\n\nMayer, Audrey L. 2008. “Strengths and Weaknesses of Common Sustainability Indices for Multidimensional Systems.” Environment International 34 (2): 277–91. https://doi.org/10.1016/j.envint.2007.09.004.\n\n\nNardo, Michela, Michaela Saisana, Andrea Saltelli, and Stefano Tarantola. 2005. “Tools for Composite Indicators Building.”\n\n\nNicoletti, Giuseppe. 2000. “Summary Indicators of Product Market Regulation with an Extension to Employment Protection Legislation.” {{OECD Economics Department Working Papers}} 226. Vol. 226. OECD Economics Department Working Papers. https://doi.org/10.1787/215182844604.\n\n\nOECD. 2008. Handbook on Constructing Composite Indicators: Methodology and User Guide. Paris: Organisation for Economic Co-operation and Development.\n\n\nSchneider, Kate R., Jessica Fanzo, Lawrence Haddad, Mario Herrero, Jose Rosero Moncayo, Anna Herforth, Roseline Remans, et al. 2023. “The State of Food Systems Worldwide in the Countdown to 2030.” Nature Food 4 (12): 1090–110. https://doi.org/10.1038/s43016-023-00885-9.\n\n\nSchneider, Kate R., Roseline Remans, Tesfaye Hailu Bekele, Destan Aytekin, Piero Conforti, Shouro Dasgupta, Fabrice DeClerck, et al. 2025. “Governance and Resilience as Entry Points for Transforming Food Systems in the Countdown to 2030.” Nature Food 6 (1): 105–16. https://doi.org/10.1038/s43016-024-01109-4.\n\n\nTabachnick, Barbara G., Linda S. Fidell, and Jodie B. Ullman. 2019. Using Multivariate Statistics. Seventh edition. NY, NY: Pearson.\n\n\nTaber, Keith S. 2018. “The Use of Cronbach’s Alpha When Developing and Reporting Research Instruments in Science Education.” Research in Science Education 48 (6): 1273–96. https://doi.org/10.1007/s11165-016-9602-2."
  },
  {
    "objectID": "pages/sm-explorer.html",
    "href": "pages/sm-explorer.html",
    "title": "SM-Explorer",
    "section": "",
    "text": "Caution\n\n\n\n\n\nThe SM-Explorer is a work in progress. There are a small heap of bugs I’m already aware of, and about a hundred things I’d still like to add. If/when you find things that aren’t working properly, please feel free to let Chris know!\n\n\n\nThis is a Shiny app that allows for interactive exploration of metrics, mostly at the county level. It includes a map page, a bivariate plot explorer, and a metadata table much like what is included in this Quarto doc. It tends to work best if you open it in its own page using the button below:\n\n\n\n\nGo To SM-Explorer\n\n\n\n\nYou can also just use it here in the window. Note that some functions (like the full screen button) won’t work here.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/selection.html",
    "href": "pages/selection.html",
    "title": "Variable Selection and Regression",
    "section": "",
    "text": "On this page we will take our min-max normalized, geometrically averaged scores, which look like the most reliable and approachable so far, and take a deeper dive into variable selection, regression, and PCA. From the dimension meetings, it sounds like we may have some indicators with a couple of metrics, and potentially others with dozens. Because of this, and because of our focus on developing sensible indicators, I think it will be best to do any weighting at the indicator level or above. This also reduces our variable count substantially in relation to our state count of 51, opening more doors for PCA.\nIt is worth emphasizing at the top that the metrics that are making up this secondary data framework are not a great representation of the system. There are some important holes, as well as a heap of metrics that are serving as rather uninspiring proxies. So, extrapolation of these results beyond the confines of the exercise is not recommended. The purpose here is to explore strengths and tradeoffs in methods for aggregating the data. As primary data come in and make up the bulk of the framework and secondary data are used to fill in the gaps, this should start becoming more interpretable."
  },
  {
    "objectID": "pages/selection.html#component-extraction",
    "href": "pages/selection.html#component-extraction",
    "title": "Variable Selection and Regression",
    "section": "2.1 Component Extraction",
    "text": "2.1 Component Extraction\n\n\nCode\n# Filter down to just indicators for PCA\npca_dat &lt;- dat %&gt;% \n  select(starts_with('indic')) %&gt;% \n  setNames(c(str_remove(names(.), 'indic_'))) %&gt;% \n  as.data.frame()\n# get_str(pca_dat)\n\n# Explore how many factors to extract\nVSS(pca_dat, n = 8, fm = 'pc', rotate = 'Promax')\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.53  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.75  with  3  factors\n\nThe Velicer MAP achieves a minimum of 0.04  with  6  factors \n\n\n\nBIC achieves a minimum of  Inf  with    factors\n\n\nSample Size adjusted BIC achieves a minimum of  Inf  with    factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq prob sqresid  fit RMSEA BIC SABIC complex eChisq\n1 0.42 0.00 0.069   0    NA   NA    85.4 0.42    NA  NA    NA      NA     NA\n2 0.53 0.67 0.059   0    NA   NA    48.6 0.67    NA  NA    NA      NA     NA\n3 0.53 0.75 0.055   0    NA   NA    30.7 0.79    NA  NA    NA      NA     NA\n4 0.53 0.75 0.046   0    NA   NA    19.6 0.87    NA  NA    NA      NA     NA\n5 0.47 0.73 0.039   0    NA   NA    13.2 0.91    NA  NA    NA      NA     NA\n6 0.48 0.73 0.039   0    NA   NA    10.5 0.93    NA  NA    NA      NA     NA\n7 0.46 0.72 0.041   0    NA   NA     8.5 0.94    NA  NA    NA      NA     NA\n8 0.47 0.68 0.040   0    NA   NA     6.5 0.96    NA  NA    NA      NA     NA\n  SRMR eCRMS eBIC\n1   NA    NA   NA\n2   NA    NA   NA\n3   NA    NA   NA\n4   NA    NA   NA\n5   NA    NA   NA\n6   NA    NA   NA\n7   NA    NA   NA\n8   NA    NA   NA\n\n\nCode\nset.seed(42)\nfa.parallel(pca_dat, fm = 'ml')\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  5 \n\n\nMAP suggests 6, VSS 2 or 3, PA suggests 5. Not half bad. I think we are justified to go with 5.\n\n\nCode\n# Oblique rotations: promax, oblimin, simplimax, cluster\nrotations &lt;- c(\n  'Promax',\n  'oblimin',\n  'simplimax',\n  'cluster'\n)\npca_outs &lt;- map(rotations, ~ {\n  pca_dat %&gt;% \n    # scale() %&gt;% \n    pca(nfactors = 5, rotate = .x)\n}) %&gt;% \n  setNames(c(rotations))\n\n# Save a version of promax for preso?\npng(\n  filename = 'preso/plots/scree.png',\n  width = 800,\n  height = 600,\n  units = 'px',\n  res = 150\n)\nplot(\n  pca_outs$simplimax$values,\n  xlab = 'Number of Components',\n  ylab = 'Eigen Values'\n)\nabline(h = 1)\ndev.off()\n\n\npng \n  2 \n\n\nCode\n# Now actually show it \nplot(\n  pca_outs$simplimax$values,\n  xlab = 'Number of Components',\n  ylab = 'Eigen Values'\n)\n\n\n\n\n\n\n\n\n\nThe scree plot makes a reasonably convincing case for 5 components, as the slope falls off substantially after the fifth."
  },
  {
    "objectID": "pages/selection.html#run-pca",
    "href": "pages/selection.html#run-pca",
    "title": "Variable Selection and Regression",
    "section": "2.2 Run PCA",
    "text": "2.2 Run PCA\n\n\nCode\npca_tables &lt;- map(pca_outs, ~ {\n  .x$loadings %&gt;% \n    unclass() %&gt;% \n    as.data.frame() %&gt;% \n    select(order(colnames(.))) %&gt;%\n    mutate(\n      across(everything(), ~ round(.x, 3)),\n      across(everything(), ~ case_when(\n        .x &lt; 0.20 ~ '',\n        .x &gt;= 0.20 & .x &lt; 0.32 ~ '.',\n        .x &gt;= 0.32 ~ as.character(.x),\n        .default = NA\n      ))\n    ) %&gt;% \n    rownames_to_column() %&gt;% \n    rename(indicator = 1) %&gt;% \n    mutate(\n      dimension = framework$dimension[match(indicator, framework$indicator)]\n    ) %&gt;% \n    select(indicator, dimension, everything())\n})\nget_str(pca_tables)  \n\n\nList of 4\n $ Promax   :'data.frame':  38 obs. of  7 variables:\n  ..$ indicator: chr [1:38] \"access to land\" \"wealth/income distribution\" \"in\"..\n  ..$ dimension: chr [1:38] \"economics\" \"economics\" \"economics\" \"economics\" \"\"..\n  ..$ RC1      : chr [1:38] \"\" \"\" \"1.013\" \"\" \"\" \"\" \".\" \"0.731\" \"\" \"\" \"\" \"\" \"\"\"..\n  ..$ RC2      : chr [1:38] \"\" \"0.78\" \".\" \".\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"..\n  ..$ RC3      : chr [1:38] \"\" \"\" \"\" \"0.414\" \"\" \"\" \"0.653\" \"\" \"0.825\" \".\" \"\" \"..\n  ..$ RC4      : chr [1:38] \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"0.482\" \"\" \"0.723\" \"0\"..\n  ..$ RC5      : chr [1:38] \".\" \"\" \".\" \"0.357\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"..\n $ oblimin  :'data.frame':  38 obs. of  7 variables:\n  ..$ indicator: chr [1:38] \"access to land\" \"wealth/income distribution\" \"in\"..\n  ..$ dimension: chr [1:38] \"economics\" \"economics\" \"economics\" \"economics\" \"\"..\n  ..$ TC1      : chr [1:38] \"\" \"\" \"0.927\" \"\" \"\" \"\" \".\" \"0.715\" \"\" \"\" \"\" \"\" \"\"\"..\n  ..$ TC2      : chr [1:38] \"\" \"0.757\" \".\" \".\" \"\" \"0.427\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"..\n  ..$ TC3      : chr [1:38] \".\" \"\" \"\" \"0.452\" \".\" \"\" \"0.605\" \"\" \"0.824\" \"\" \".\"..\n  ..$ TC4      : chr [1:38] \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"0.677\" \"\" \"0.805\" \"0\"..\n  ..$ TC5      : chr [1:38] \".\" \"\" \".\" \"0.365\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\"..\n $ simplimax:'data.frame':  38 obs. of  7 variables:\n  ..$ indicator: chr [1:38] \"access to land\" \"wealth/income distribution\" \"in\"..\n  ..$ dimension: chr [1:38] \"economics\" \"economics\" \"economics\" \"economics\" \"\"..\n  ..$ TC1      : chr [1:38] \"0.448\" \"0.43\" \"\" \"0.78\" \"\" \"0.816\" \"\" \"\" \"0.442\"\"..\n  ..$ TC2      : chr [1:38] \"\" \"0.621\" \".\" \"\" \"\" \"\" \".\" \"0.508\" \"0.336\" \"\" \"\"\"..\n  ..$ TC3      : chr [1:38] \"\" \"\" \"\" \".\" \"\" \"\" \"0.539\" \"\" \"0.482\" \"0.464\" \"\" \"..\n  ..$ TC4      : chr [1:38] \"\" \"\" \"\" \"\" \"0.486\" \"\" \".\" \"\" \"0.456\" \".\" \"0.598\"\"..\n  ..$ TC5      : chr [1:38] \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"0.511\" \"\" \"0.589\" \"0\"..\n $ cluster  :'data.frame':  38 obs. of  7 variables:\n  ..$ indicator: chr [1:38] \"access to land\" \"wealth/income distribution\" \"in\"..\n  ..$ dimension: chr [1:38] \"economics\" \"economics\" \"economics\" \"economics\" \"\"..\n  ..$ RC1      : chr [1:38] \".\" \"\" \"\" \"0.44\" \"\" \"0.791\" \"\" \"\" \"\" \"\" \"\" \".\" \".\"..\n  ..$ RC2      : chr [1:38] \"\" \"0.751\" \".\" \".\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\"..\n  ..$ RC3      : chr [1:38] \"\" \"\" \"\" \"0.389\" \"\" \"\" \"0.648\" \"\" \"0.824\" \".\" \"\" \"..\n  ..$ RC4      : chr [1:38] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"0.427\" \"\" \"0.638\" \"0.\"..\n  ..$ RC5      : chr [1:38] \"\" \"\" \"0.33\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\"..\n\n\nCode\n# Save it for preso\nsaveRDS(pca_tables, 'preso/data/pca_tables.rds')\n\n\n\n\nCode\nget_reactable(pca_tables$simplimax)\n\n\n\n\n\n\nAdd interpretation here []"
  },
  {
    "objectID": "pages/selection.html#economics",
    "href": "pages/selection.html#economics",
    "title": "Variable Selection and Regression",
    "section": "3.1 Economics",
    "text": "3.1 Economics\n\n3.1.1 Linear Model\nFirst we can try a plain old linear model to see how economics loads onto its indicators.\n\n\nCode\n# Reduce data down to dimen_economics and all indicators\necon_dat &lt;- select(dat, dimen_economics, starts_with('indic')) %&gt;% \n  setNames(c(names(.) %&gt;% str_remove('indic_|dimen_')))\nget_str(econ_dat)\n\n\nrowws_df [50 × 39] (S3: rowwise_df/tbl_df/tbl/data.frame)\n $ economics                            : num [1:50] 0.186 0.299 0.183 0.207 0..\n $ access to land                       : num [1:50] 0.363 0.219 0.198 0.435 0..\n $ wealth/income distribution           : num [1:50] 0.275 0.684 0.373 0.328 0..\n $ income stability                     : num [1:50] 0.08724 0.06626 0.20328 0..\n $ operations diversification           : num [1:50] 0.013147 0.39392 0.05679 ..\n $ use of ag/farm/crop insurance        : num [1:50] 0.2314 0.21962 0 0.29855 ..\n $ carbon fluxes                        : num [1:50] 0.346 0.751 0.879 0.436 0..\n $ carbon stocks                        : num [1:50] 0.1654 0.3674 0.0185 0.32..\n $ embodied carbon                      : num [1:50] 0.05821 0.00615 0.06465 0..\n $ forest health                        : num [1:50] 0.25 0.4275 0.1041 0.3586..\n $ biodiversity                         : num [1:50] 0.572 0.298 0.574 0.489 0..\n $ land use diversity                   : num [1:50] 0.954307 0.587913 0.29319..\n $ sensitive or rare habitats           : num [1:50] 0.642 0.7319 0.6918 0.419..\n $ water quality                        : num [1:50] 0.581 0.585 0.449 0.454 0..\n $ water quantity                       : num [1:50] 0.712 0.915 0.42 0.791 0...\n $ educational attainment               : num [1:50] 0.2323 0.4223 0.4479 0.15..\n $ access to culturally appropriate food: num [1:50] 0.1563 0.3824 0.0818 0.07..\n $ dietary quality                      : num [1:50] 0.36 0.352 0.352 0.768 0...\n $ food access                          : num [1:50] 0.1684 0.2286 0.1061 0.12..\n $ food affordability                   : num [1:50] 0.498 0.526 0.35 0.172 0...\n $ mental health tbd                    : num [1:50] 0.352 0.34 0.312 0.339 0...\n $ access to care                       : num [1:50] 0.198 0.435 0.313 0.247 0..\n $ housing supply and quality           : num [1:50] 0.621 0.616 0.633 0.536 0..\n $ physical health tbd                  : num [1:50] 0.274 0.585 0.507 0.377 0..\n $ total quantity exported              : num [1:50] 0.068031 0 0.068105 0.174..\n $ production species diversity         : num [1:50] 0.5496 0.2917 0.6537 0.37..\n $ production inputs                    : num [1:50] 0.8 1 0.619 0.305 0.533 0..\n $ total quantity food products         : num [1:50] 0.107721 0.000613 0.07753..\n $ total quantity forest products       : num [1:50] 0.635061 0.000087 0.00048..\n $ total quantity non-food ag products  : num [1:50] 0.19143 0.00143 0.02043 0..\n $ value added market                   : num [1:50] 0.01481 0.30841 0.04466 0..\n $ crop failure                         : num [1:50] 0.95 1 0.967 0.391 0.636 ..\n $ social connectedness                 : num [1:50] 0.417 0.406 0.314 0.388 0..\n $ community safety                     : num [1:50] 0.749 0.384 0.896 0.677 0..\n $ diverse representation               : num [1:50] 0.45 0.53 0.638 0.34 0.41..\n $ age diversity                        : num [1:50] 0.3774 0.7547 0.1132 0.58..\n $ gender diversity                     : num [1:50] 0.274 0.9229 1 0.4825 0.4..\n $ racial diversity                     : num [1:50] 0.28454 0.34292 0.82326 0..\n $ participatory governance             : num [1:50] 0.3192 0.514 0.4967 0.011..\n\n\nCode\nlm &lt;- lm(economics ~ ., data = econ_dat)\nsummary(lm)\n\n\n\nCall:\nlm(formula = economics ~ ., data = econ_dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.028597 -0.008780 -0.001355  0.009435  0.033763 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             -0.129261   0.177775  -0.727  0.48235\n`access to land`                         0.407912   0.159224   2.562  0.02643\n`wealth/income distribution`             0.264800   0.100376   2.638  0.02307\n`income stability`                       0.007812   0.108346   0.072  0.94381\n`operations diversification`            -0.080939   0.071452  -1.133  0.28140\n`use of ag/farm/crop insurance`          0.104228   0.027921   3.733  0.00331\n`carbon fluxes`                          0.024741   0.071354   0.347  0.73533\n`carbon stocks`                          0.031166   0.117489   0.265  0.79571\n`embodied carbon`                        0.090569   0.125130   0.724  0.48430\n`forest health`                          0.007066   0.162326   0.044  0.96606\nbiodiversity                             0.019697   0.181185   0.109  0.91539\n`land use diversity`                     0.030419   0.042042   0.724  0.48445\n`sensitive or rare habitats`            -0.060387   0.089124  -0.678  0.51205\n`water quality`                          0.040110   0.076031   0.528  0.60829\n`water quantity`                        -0.006883   0.076736  -0.090  0.93014\n`educational attainment`                 0.053691   0.096731   0.555  0.58997\n`access to culturally appropriate food` -0.071258   0.123987  -0.575  0.57704\n`dietary quality`                       -0.029778   0.067903  -0.439  0.66949\n`food access`                            0.082575   0.128307   0.644  0.53304\n`food affordability`                    -0.085322   0.095028  -0.898  0.38849\n`mental health tbd`                     -0.216905   0.162632  -1.334  0.20926\n`access to care`                         0.059387   0.180315   0.329  0.74808\n`housing supply and quality`             0.087502   0.125383   0.698  0.49974\n`physical health tbd`                   -0.161052   0.165392  -0.974  0.35111\n`total quantity exported`                0.343059   0.163633   2.097  0.05996\n`production species diversity`           0.063850   0.066954   0.954  0.36075\n`production inputs`                      0.055077   0.053327   1.033  0.32386\n`total quantity food products`          -0.315811   0.207882  -1.519  0.15692\n`total quantity forest products`         0.039438   0.057882   0.681  0.50974\n`total quantity non-food ag products`   -0.005035   0.081416  -0.062  0.95180\n`value added market`                     0.319651   0.115996   2.756  0.01870\n`crop failure`                          -0.011586   0.090693  -0.128  0.90065\n`social connectedness`                  -0.049821   0.083092  -0.600  0.56093\n`community safety`                      -0.030003   0.039398  -0.762  0.46235\n`diverse representation`                 0.011552   0.040475   0.285  0.78063\n`age diversity`                          0.035801   0.059580   0.601  0.56009\n`gender diversity`                       0.061339   0.092878   0.660  0.52257\n`racial diversity`                       0.073204   0.099326   0.737  0.47655\n`participatory governance`               0.064864   0.066379   0.977  0.34948\n                                          \n(Intercept)                               \n`access to land`                        * \n`wealth/income distribution`            * \n`income stability`                        \n`operations diversification`              \n`use of ag/farm/crop insurance`         **\n`carbon fluxes`                           \n`carbon stocks`                           \n`embodied carbon`                         \n`forest health`                           \nbiodiversity                              \n`land use diversity`                      \n`sensitive or rare habitats`              \n`water quality`                           \n`water quantity`                          \n`educational attainment`                  \n`access to culturally appropriate food`   \n`dietary quality`                         \n`food access`                             \n`food affordability`                      \n`mental health tbd`                       \n`access to care`                          \n`housing supply and quality`              \n`physical health tbd`                     \n`total quantity exported`               . \n`production species diversity`            \n`production inputs`                       \n`total quantity food products`            \n`total quantity forest products`          \n`total quantity non-food ag products`     \n`value added market`                    * \n`crop failure`                            \n`social connectedness`                    \n`community safety`                        \n`diverse representation`                  \n`age diversity`                           \n`gender diversity`                        \n`racial diversity`                        \n`participatory governance`                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02723 on 11 degrees of freedom\nMultiple R-squared:  0.9703,    Adjusted R-squared:  0.8677 \nF-statistic: 9.461 on 38 and 11 DF,  p-value: 0.0001723\n\n\nWe can see that most of the economics indicators (access to land, wealth and income distribution, income stability) are significant predictors, while operations diversification is close. But some surprises are access to culturally appropriate food (school food authorities serving culturally relevant food), housing supply and quality, as well as a few production indicators, like richness (crop diversity), production inputs, and value-added markets. Social connectedness from the social dimension also makes it on the list. The largest coefficients by a wide margin are for access to land and wealth and income distribution.\n\n\n3.1.2 Splitting Data\nHere we split out data into a 60/40 training/test set for cross validation with GLMnet and Random Forest models. Note that we are pushing the limits of our sample size. But this should help protect against overfitting.\n\n\nCode\npacman::p_load(\n  caret,\n  ranger,\n  glmnet\n)\n\n# Split data 60/40\nset.seed(42)\nindices &lt;- createDataPartition(econ_dat$economics, p = 0.60, list = FALSE)\ntraining_data &lt;- econ_dat[indices, ]\ntesting_data &lt;- econ_dat[-indices,]\n\nmy_folds &lt;- createFolds(training_data$economics, k = 5, list = TRUE)\n\n# Control\nmy_control &lt;- trainControl(\n  method = 'cv',\n  number = 5,\n  verboseIter = TRUE,\n  index = my_folds\n)\n\n# Check for zero variance or near zero variance indicators\nnearZeroVar(dat, names = TRUE, saveMetrics = TRUE)\n# All clear\n\n\n\n\n3.1.3 GLMnet\nHere we use a GLMnet to find an optimal balance between a ridge regression, which penalizes variables based on the magnitude of coefficients, and lasso regression, which adds a penalty based on the absolute value of coefficients. We use a tuning grid to find optimal values of alpha (0 = ridge, 1 = lasso) and lambda (the penalty parameter). Both this and the random forest model are particularly good at prediction, but also provide a metric for variable importance that can help us interpret our indicators.\n\n\nCode\nset.seed(42)\necon_glmnet &lt;- train(\n  economics ~ .,\n  data = training_data, \n  tuneGrid = expand.grid(\n    alpha = seq(0.1, 1, length = 5),\n    lambda = seq(0.0001, 0.1, length = 100)\n  ),\n  method = \"glmnet\",\n  trControl = my_control,\n  preProcess = c('zv', 'center', 'scale')\n)\n\n\n+ Fold1: alpha=0.100, lambda=0.1 \n- Fold1: alpha=0.100, lambda=0.1 \n+ Fold1: alpha=0.325, lambda=0.1 \n- Fold1: alpha=0.325, lambda=0.1 \n+ Fold1: alpha=0.550, lambda=0.1 \n- Fold1: alpha=0.550, lambda=0.1 \n+ Fold1: alpha=0.775, lambda=0.1 \n- Fold1: alpha=0.775, lambda=0.1 \n+ Fold1: alpha=1.000, lambda=0.1 \n- Fold1: alpha=1.000, lambda=0.1 \n+ Fold2: alpha=0.100, lambda=0.1 \n- Fold2: alpha=0.100, lambda=0.1 \n+ Fold2: alpha=0.325, lambda=0.1 \n- Fold2: alpha=0.325, lambda=0.1 \n+ Fold2: alpha=0.550, lambda=0.1 \n- Fold2: alpha=0.550, lambda=0.1 \n+ Fold2: alpha=0.775, lambda=0.1 \n- Fold2: alpha=0.775, lambda=0.1 \n+ Fold2: alpha=1.000, lambda=0.1 \n- Fold2: alpha=1.000, lambda=0.1 \n+ Fold3: alpha=0.100, lambda=0.1 \n- Fold3: alpha=0.100, lambda=0.1 \n+ Fold3: alpha=0.325, lambda=0.1 \n- Fold3: alpha=0.325, lambda=0.1 \n+ Fold3: alpha=0.550, lambda=0.1 \n- Fold3: alpha=0.550, lambda=0.1 \n+ Fold3: alpha=0.775, lambda=0.1 \n- Fold3: alpha=0.775, lambda=0.1 \n+ Fold3: alpha=1.000, lambda=0.1 \n- Fold3: alpha=1.000, lambda=0.1 \n+ Fold4: alpha=0.100, lambda=0.1 \n- Fold4: alpha=0.100, lambda=0.1 \n+ Fold4: alpha=0.325, lambda=0.1 \n- Fold4: alpha=0.325, lambda=0.1 \n+ Fold4: alpha=0.550, lambda=0.1 \n- Fold4: alpha=0.550, lambda=0.1 \n+ Fold4: alpha=0.775, lambda=0.1 \n- Fold4: alpha=0.775, lambda=0.1 \n+ Fold4: alpha=1.000, lambda=0.1 \n- Fold4: alpha=1.000, lambda=0.1 \n+ Fold5: alpha=0.100, lambda=0.1 \n- Fold5: alpha=0.100, lambda=0.1 \n+ Fold5: alpha=0.325, lambda=0.1 \n- Fold5: alpha=0.325, lambda=0.1 \n+ Fold5: alpha=0.550, lambda=0.1 \n- Fold5: alpha=0.550, lambda=0.1 \n+ Fold5: alpha=0.775, lambda=0.1 \n- Fold5: alpha=0.775, lambda=0.1 \n+ Fold5: alpha=1.000, lambda=0.1 \n- Fold5: alpha=1.000, lambda=0.1 \n\n\nAggregating results\nSelecting tuning parameters\nFitting alpha = 1, lambda = 0.0758 on full training set\n\n\nCode\nimportance &lt;- varImp(econ_glmnet, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nCode\n# Predict\n# p &lt;- predict(econ_glmnet, testing_data)\n# postResample(pred = p, obs = testing_data$economics)\n\n\nThe optimal hyperparameters from the tuning grid were alpha = 0.1 (mostly ridge regression) and lambda = 0.00313. The variable importance plot is on a relative scale of 0 (unimportant) to 100 (most important) in terms of predictive power. Curiously, it is showing that the value added market indicator from the production dimension is a better predictor of economics than any economics indicator.\n\n\n3.1.4 Random Forest\nNow we can try a random forest, which is particularly good at handling non-linear relationships. Here we use the RMSE to determine the optimal combination of mtry (the number of variables selected at each node in the decision tree), the split rule, and the minimum node size.\n\n\nCode\nset.seed(42)\necon_rf &lt;- train(\n  economics ~ .,\n  data = training_data, \n  tuneLength = 7,\n  method = \"ranger\",\n  trControl = my_control,\n  importance = 'impurity'\n)\n\n\n+ Fold1: mtry= 2, min.node.size=5, splitrule=variance \n- Fold1: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold1: mtry= 8, min.node.size=5, splitrule=variance \n- Fold1: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold1: mtry=14, min.node.size=5, splitrule=variance \n- Fold1: mtry=14, min.node.size=5, splitrule=variance \n+ Fold1: mtry=20, min.node.size=5, splitrule=variance \n- Fold1: mtry=20, min.node.size=5, splitrule=variance \n+ Fold1: mtry=26, min.node.size=5, splitrule=variance \n- Fold1: mtry=26, min.node.size=5, splitrule=variance \n+ Fold1: mtry=32, min.node.size=5, splitrule=variance \n- Fold1: mtry=32, min.node.size=5, splitrule=variance \n+ Fold1: mtry=38, min.node.size=5, splitrule=variance \n- Fold1: mtry=38, min.node.size=5, splitrule=variance \n+ Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold1: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=38, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=38, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry= 2, min.node.size=5, splitrule=variance \n- Fold2: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold2: mtry= 8, min.node.size=5, splitrule=variance \n- Fold2: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold2: mtry=14, min.node.size=5, splitrule=variance \n- Fold2: mtry=14, min.node.size=5, splitrule=variance \n+ Fold2: mtry=20, min.node.size=5, splitrule=variance \n- Fold2: mtry=20, min.node.size=5, splitrule=variance \n+ Fold2: mtry=26, min.node.size=5, splitrule=variance \n- Fold2: mtry=26, min.node.size=5, splitrule=variance \n+ Fold2: mtry=32, min.node.size=5, splitrule=variance \n- Fold2: mtry=32, min.node.size=5, splitrule=variance \n+ Fold2: mtry=38, min.node.size=5, splitrule=variance \n- Fold2: mtry=38, min.node.size=5, splitrule=variance \n+ Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold2: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=38, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=38, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry= 2, min.node.size=5, splitrule=variance \n- Fold3: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold3: mtry= 8, min.node.size=5, splitrule=variance \n- Fold3: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold3: mtry=14, min.node.size=5, splitrule=variance \n- Fold3: mtry=14, min.node.size=5, splitrule=variance \n+ Fold3: mtry=20, min.node.size=5, splitrule=variance \n- Fold3: mtry=20, min.node.size=5, splitrule=variance \n+ Fold3: mtry=26, min.node.size=5, splitrule=variance \n- Fold3: mtry=26, min.node.size=5, splitrule=variance \n+ Fold3: mtry=32, min.node.size=5, splitrule=variance \n- Fold3: mtry=32, min.node.size=5, splitrule=variance \n+ Fold3: mtry=38, min.node.size=5, splitrule=variance \n- Fold3: mtry=38, min.node.size=5, splitrule=variance \n+ Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold3: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=38, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=38, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry= 2, min.node.size=5, splitrule=variance \n- Fold4: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold4: mtry= 8, min.node.size=5, splitrule=variance \n- Fold4: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold4: mtry=14, min.node.size=5, splitrule=variance \n- Fold4: mtry=14, min.node.size=5, splitrule=variance \n+ Fold4: mtry=20, min.node.size=5, splitrule=variance \n- Fold4: mtry=20, min.node.size=5, splitrule=variance \n+ Fold4: mtry=26, min.node.size=5, splitrule=variance \n- Fold4: mtry=26, min.node.size=5, splitrule=variance \n+ Fold4: mtry=32, min.node.size=5, splitrule=variance \n- Fold4: mtry=32, min.node.size=5, splitrule=variance \n+ Fold4: mtry=38, min.node.size=5, splitrule=variance \n- Fold4: mtry=38, min.node.size=5, splitrule=variance \n+ Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold4: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=38, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=38, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry= 2, min.node.size=5, splitrule=variance \n- Fold5: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold5: mtry= 8, min.node.size=5, splitrule=variance \n- Fold5: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold5: mtry=14, min.node.size=5, splitrule=variance \n- Fold5: mtry=14, min.node.size=5, splitrule=variance \n+ Fold5: mtry=20, min.node.size=5, splitrule=variance \n- Fold5: mtry=20, min.node.size=5, splitrule=variance \n+ Fold5: mtry=26, min.node.size=5, splitrule=variance \n- Fold5: mtry=26, min.node.size=5, splitrule=variance \n+ Fold5: mtry=32, min.node.size=5, splitrule=variance \n- Fold5: mtry=32, min.node.size=5, splitrule=variance \n+ Fold5: mtry=38, min.node.size=5, splitrule=variance \n- Fold5: mtry=38, min.node.size=5, splitrule=variance \n+ Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold5: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=38, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=38, min.node.size=5, splitrule=extratrees \n\n\nAggregating results\nSelecting tuning parameters\nFitting mtry = 2, splitrule = extratrees, min.node.size = 5 on full training set\n\n\nCode\n# econ_rf\n# plot(econ_rf)\n\nimportance &lt;- varImp(econ_rf, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nCode\n# Predict\n# p &lt;- predict(model_mf, testing_data)\n# postResample(pred = p, obs = testing_data$rebl_tpm)\n\n\nThe random forest model is also picking out the value-added market indicator as the best predictor of economics dimension scores, followed closely by operations diversification, wealth and income distribution, and income stability.\nVery curious how value-added markets keep sticking out. The two metrics making up this indicator are both from NASS: the percentage of farms reporting value-added sales, and of those farms, the percentage of value-added sales out of total sales."
  },
  {
    "objectID": "pages/refine_environment.html",
    "href": "pages/refine_environment.html",
    "title": "Environment Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the environment dimension. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting. A final set of indicators to incorporate into the next RFP is still in the works!",
    "crumbs": [
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/refine_environment.html#indicator-progression",
    "href": "pages/refine_environment.html#indicator-progression",
    "title": "Environment Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework for the dimension as described in the Wiltshire et al. paper.\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/wiltshire_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/matrix_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Refinement Meeting\nFinally, the tentative set of indicators following the indicator refinement meeting on November 22nd, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/env_meeting_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)",
    "crumbs": [
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/refine_environment.html#survey",
    "href": "pages/refine_environment.html#survey",
    "title": "Environment Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the economic indicator refinement meeting on November 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/env_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    ends_with('GROUP'),\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not'\n  )) %&gt;% \n  .[-c(1:2), ]\n\nto_df &lt;- function(x) {\n  x %&gt;%\n    str_replace_all('PFAS, PFOS', 'PFAS/PFOS') %&gt;% \n    str_replace_all('soil loss/', 'Soil loss/') %&gt;% \n    str_split(',') %&gt;% \n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n    arrange(desc(freq))\n}\n\nindi_out &lt;- map(dat[1:4], to_df)\nidx_out &lt;- map(dat[5:8], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\n\n\nCode\nggplot(graph_table, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so “Must Include” is worth 3 points, “Probably Include” is worth 2 points, “Probably Not Include” is worth 1 point, and “Must Not Include” is worth 0 points. Note that the last column is the sum of proportions of “Must Include” and “Probably Include”. You can sort, search, expand, or page through the table below.\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nidx_tables &lt;- map2(idx_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    sort_key = ifelse(str_detect(index, 'Carbon'), 5e6, sort_key),\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\",\n      'must_not'\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\",\n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nThe indices are going through the same treatment as indicators above - scored from 3 to 0. Note here that the “Carbon ($ GHGs/nutrients)” index seems to be missing a vote. So, it only has 12 points, but the proportion of votes for “Must Include” is 1.",
    "crumbs": [
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/refinement_process.html",
    "href": "pages/refinement_process.html",
    "title": "Indicator Refinement Process",
    "section": "",
    "text": "To Add:\n\nDescribe dimension meetings\nlink Wiltshire (Wiltshire et al. 2024)\nLink Bene et al 2024 (Béné et al. 2024), describe their process\nWhat is more settled, what is not (social and human)\n\n\n\n\nWhiteboard from economics dimension refinement meeting, November 15th, 2024"
  },
  {
    "objectID": "pages/refinement_process.html#introduction",
    "href": "pages/refinement_process.html#introduction",
    "title": "Indicator Refinement Process",
    "section": "",
    "text": "To Add:\n\nDescribe dimension meetings\nlink Wiltshire (Wiltshire et al. 2024)\nLink Bene et al 2024 (Béné et al. 2024), describe their process\nWhat is more settled, what is not (social and human)\n\n\n\n\nWhiteboard from economics dimension refinement meeting, November 15th, 2024"
  },
  {
    "objectID": "pages/overview.html",
    "href": "pages/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Dr. Josh Taylor (left) and Dr. David Conner (right) at the FSRC Sustainability Metrics workshop in 2024. Photo by Colleen Goodhue, FSRC.\n\n\nThe original framework of dimensions, indices, and indicators representing food system sustainability was developed through a transdisciplinary team science process described in detail by Wiltshire et al. (2024). The figure below shows the structure of this collaborative process.\nAs the project progressed, a collection of new indicators were proposed across all dimensions, yielding a total of 135 indicators. This full set of indicators is shown in Section 2. Starting in July of 2024, the FSRC has been using a collaborative and transparent process to reduce the number of indicators to manageable amount that can comprehensively represent the food system while being tractable enough to be interpretable and actionable.\nMore information about this refinement process can be found in the Indicator Refinement pages, including results from surveys on indicator and index importance. Subsequent analyses in the Refined Framework Analysis use this reduce set of indicators. The selection of metrics to represent those indicators is tentative. For now, we are using a larger set of metrics than might be otherwise ideal to give us flexibility to explore which best represent the system and what the consequences are given different numbers and configurations of metrics.\n\n\n\nTeam science diagram from Wiltshire et al., 2024.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "pages/overview.html#introduction",
    "href": "pages/overview.html#introduction",
    "title": "Overview",
    "section": "",
    "text": "Dr. Josh Taylor (left) and Dr. David Conner (right) at the FSRC Sustainability Metrics workshop in 2024. Photo by Colleen Goodhue, FSRC.\n\n\nThe original framework of dimensions, indices, and indicators representing food system sustainability was developed through a transdisciplinary team science process described in detail by Wiltshire et al. (2024). The figure below shows the structure of this collaborative process.\nAs the project progressed, a collection of new indicators were proposed across all dimensions, yielding a total of 135 indicators. This full set of indicators is shown in Section 2. Starting in July of 2024, the FSRC has been using a collaborative and transparent process to reduce the number of indicators to manageable amount that can comprehensively represent the food system while being tractable enough to be interpretable and actionable.\nMore information about this refinement process can be found in the Indicator Refinement pages, including results from surveys on indicator and index importance. Subsequent analyses in the Refined Framework Analysis use this reduce set of indicators. The selection of metrics to represent those indicators is tentative. For now, we are using a larger set of metrics than might be otherwise ideal to give us flexibility to explore which best represent the system and what the consequences are given different numbers and configurations of metrics.\n\n\n\nTeam science diagram from Wiltshire et al., 2024.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "pages/overview.html#sec-framework_overview",
    "href": "pages/overview.html#sec-framework_overview",
    "title": "Overview",
    "section": "2 Framework Overview",
    "text": "2 Framework Overview\nBelow is a diagram of all 135 indicators in the framework as of July, 2024. Colors represent dimensions, and splits occur at the dimension and index level. See the table in Section 3 for a more detailed look at indicators.\n\n\nCode\n## Load packages\npacman::p_load(\n  ggraph,\n  igraph,\n  dplyr,\n  RColorBrewer,\n  viridisLite\n)\n\n\n## Load data and add an origin level\ndat &lt;- readRDS('data/trees/tree_dat.rds') %&gt;% \n  mutate(Framework = 'Sustainability') %&gt;% \n  select(Framework, Dimension:Indicator)\n\n\n## Make edges\n# include groupings by dimension, then combine them\nedges &lt;- list()\nedges$sm_dim &lt;- dat %&gt;% \n  select(Framework, Dimension) %&gt;% \n  unique() %&gt;% \n  rename(from = Framework, to = Dimension) %&gt;% \n  mutate(group = to)\nedges$dim_ind &lt;- dat %&gt;% \n  select(Dimension, Index) %&gt;% \n  unique() %&gt;% \n  rename(from = Dimension, to = Index) %&gt;% \n  mutate(group = from)\nedges$ind_ind &lt;- dat %&gt;% \n  select(Index, Indicator) %&gt;% \n  unique() %&gt;% \n  rename(from = Index, to = Indicator) %&gt;% \n  mutate(group = edges$dim_ind$from[match(.$from, edges$dim_ind$to)])\nedges &lt;- bind_rows(edges)\n\n\n## Make vertices\n# Each line is a single vertex (dimension, index, or indicator)\n# We are just giving them random values to control point size for now\nvertices = data.frame(\n  name = unique(c(as.character(edges$from), as.character(edges$to))) , \n  value = runif(nrow(edges) + 1)\n) \n\n# Add the dimension groupings to the vertices as well\nvertices$group = edges$group[match(vertices$name, edges$to)]\n\n# Calculate the angles to arrange indicator labels\nvertices$id = NA\nmyleaves = which(is.na(match(vertices$name, edges$from)))\nnleaves = length(myleaves)\nvertices$id[myleaves] = seq(1:nleaves)\nvertices$angle = 90 - 360 * vertices$id / nleaves\n\n# Calculate alignment of indicator labels\nvertices$hjust &lt;- ifelse(vertices$angle &lt; -90, 1, 0)\n\n# Flip label angles around 180 degrees if they are facing the wrong way\nvertices$angle &lt;- ifelse(vertices$angle &lt; -90, vertices$angle + 180, vertices$angle)\n\n\n## Create graph\n# Make ggraph object from edges and vertices\ngraph &lt;- graph_from_data_frame(edges, vertices = vertices)\n\n# Plot the graph\nggraph(graph, layout = 'dendrogram', circular = TRUE) +\n  \n  # Color edges by dimension\n  geom_edge_diagonal(aes(color = group), width = 0.5) +\n  \n  # Create text for indicators using angles, hjust, and dimension groupings\n  geom_node_text(\n    aes(\n      x = x * 1.04,\n      y = y * 1.04,\n      filter = leaf,\n      label = name,\n      angle = angle,\n      hjust = hjust,\n      colour = group\n    ),\n    size = 2.7,\n    alpha = 1\n  ) +\n  \n  # Make the points for indicators based on dimension groupings\n  # geom_node_point(aes(\n  #   filter = leaf,\n  #   x = x * 1.07,\n  #   y = y * 1.07,\n  #   colour = group,\n  #   size = value,\n  #   alpha = 0.2\n  # )) +\n  \n  # Label the dimensions within the graph\n  geom_node_label(\n    aes(label = ifelse(name == group, name, NA)),\n    label.padding = unit(0.2, \"lines\"),\n    label.r = unit(0.3, \"lines\"),\n    label.size = 0.1,\n    size = 3\n  ) +\n  \n  # Various formatting options\n  scale_colour_manual(values = brewer.pal(5, 'Set1')) +\n  scale_edge_color_manual(values = brewer.pal(5, 'Set1')) +\n  scale_size_continuous(range = c(0.1, 7)) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.margin = unit(c(0, 0, 0, 0), \"cm\")\n  ) +\n  expand_limits(x = c(-2, 2), y = c(-2, 2))\n\n\n\n\n\nRadial dendrogram of Sustainability Metrics framework",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "pages/overview.html#sec-framework_table",
    "href": "pages/overview.html#sec-framework_table",
    "title": "Overview",
    "section": "3 Full Indicator Table",
    "text": "3 Full Indicator Table\nBelow is an interactive table with the full set of 135 indicators from July of 2024. You can search, filter, and page through the table, and download the filtered set of data as a .csv file using the download button.\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools\n)\n\n# Load framework data as a tree\ntree &lt;- readRDS('data/trees/tree_dat.rds')\n\n# Load custom reactable table function\nsource('dev/get_reactable.R')\n\n# Pick out variables to display\ndat &lt;- tree %&gt;% \n  select(-c(tooltip, count_))\n\n# Make reactable table\nhtmltools::browsable(\n  tagList(\n    tags$div(\n      style = \"display: flex; margin-bottom: 20px; justify-content: center;\",\n      tags$button(\n          class = \"btn btn-primary\",\n          style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n          tagList(fontawesome::fa(\"download\"), \"Download as CSV\"),\n          onclick = \"Reactable.downloadDataCSV('indicator_table', 'indicator_framework.csv')\"\n      )\n    ),\n    get_reactable(\n      dat,\n      elementId = \"indicator_table\",\n      columns = list(\n        Dimension = colDef(minWidth = 75),\n        Index = colDef(minWidth = 100),\n        Indicator = colDef(minWidth = 200)\n      )\n    )\n  )\n)\n\n\n\n\n\nDownload as CSV",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "pages/metric_counts.html",
    "href": "pages/metric_counts.html",
    "title": "Metric Counts",
    "section": "",
    "text": "Here we will see how much the dimension scores for Vermont change when we reduce certain indicators with many metrics to a single metric. We do this for an indicator in every dimension except social, which really doesn’t have any metrics to spare currently.\nNote that this should be redone more systematically and across all metrics to compare them at some point, rather than a picking just a few. On to-do list.\n\n\nCode\n# Load data for aggregations\nstate_key &lt;- readRDS('data/sm_data.rds')[['state_key']]\nmetrics_df &lt;- readRDS('data/metrics_df.rds')\nnormed_data &lt;- readRDS('data/normalized_metrics_df.rds')\nframework &lt;- readRDS('data/filtered_frame.rds')\n\n# Test out process\noriginal_scores &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df\n)\n\n\n\n1 Reduce Physical Health Metrics\nFirst let’s take the physical health tbd indicator and reduce it to only lifeExpectancy.\n\n\nCode\n# Remove everything from physical health tbd except life expectancy\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'physical health tbd', \n    variable_name != 'lifeExpectancy'\n  ) %&gt;% \n  dplyr::pull(variable_name)\n\n# Get dimension scores\nto_life &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\nNow the same for drugOverdoseDeaths\n\n\nCode\n# Remove everything from physical health tbd except life expectancy\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'physical health tbd', \n    variable_name != 'drugOverdoseDeaths'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_overdose &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\nFinally, we can compare the health dimension score and rank for the original framework against the physical health tbd indicator as represented by only life expectancy or only drug overdoses.\n\n\nCode\ndf &lt;- map_dfr(list(original_scores, to_life, to_overdose), ~ {\n  .x$minmax_geometric$dimension_scores %&gt;% \n    as.data.frame() %&gt;%\n    dplyr::filter(str_length(state) == 2) %&gt;% \n    mutate(\n      rank = dense_rank(health),\n    ) %&gt;% \n    dplyr::filter(state == 'VT') %&gt;% \n    dplyr::select(health, rank)\n}) %&gt;% \n  mutate(\n    health = round(health, 3),\n    iteration = c('Original', 'Life Exp Only', 'Overdoses Only')\n  )\n\n\n\n\n\n\n\n2 Reduce Crop Failure Metrics\nFor production, we will reduce the crop failure indicator to only the value of dairy margin protection payments and then income from insurance indemnities.\n\n\nCode\nget_str(normed_data)\nget_str(framework)\n\n## Reduce crop failure indicator to totalValueDairyMarginProtPayments\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'crop failure', \n    variable_name != 'totalValueDairyMarginProtPayments'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_dairy &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Reduce crop failure indicator to totalIncomeInsuranceIndemnities\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'crop failure', \n    variable_name != 'totalIncomeInsuranceIndemnities'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_insurance &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Put them together\nprod_df &lt;- map_dfr(list(original_scores, to_dairy, to_insurance), ~ {\n  .x$minmax_geometric$dimension_scores %&gt;% \n    as.data.frame() %&gt;%\n    dplyr::filter(str_length(state) == 2) %&gt;% \n    mutate(\n      rank = dense_rank(production),\n    ) %&gt;% \n    dplyr::filter(state == 'VT') %&gt;% \n    dplyr::select(production, rank)\n}) %&gt;% \n  mutate(\n    production = round(production, 3),\n    iteration = c('Original', 'Dairy Only', 'Insurance Only')\n  )\n\n\n\n\n\n\n\n3 Reduce Biodiversity Metrics\nNow for the environment dimension, we will reduce the biodiversity indicator from its current set of 8 species down to the percentage of animal species at risk and the percentage of plant species at risk.\n\n\nCode\nget_str(normed_data)\nget_str(framework)\n\n## Reduce crop failure indicator to pctAtRiskAnimalSpp\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'biodiversity', \n    variable_name != 'pctAtRiskAnimalSpp'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_animal &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Reduce biodiversity indicator to pctAtRiskPlantSpp\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'crop failure', \n    variable_name != 'pctAtRiskPlantSpp'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_plant &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Put them together\nenv_df &lt;- map_dfr(list(original_scores, to_animal, to_plant), ~ {\n  .x$minmax_geometric$dimension_scores %&gt;% \n    as.data.frame() %&gt;%\n    dplyr::filter(str_length(state) == 2) %&gt;% \n    mutate(\n      rank = dense_rank(environment),\n    ) %&gt;% \n    dplyr::filter(state == 'VT') %&gt;% \n    dplyr::select(environment, rank)\n}) %&gt;% \n  mutate(\n    environment = round(environment, 3),\n    iteration = c('Original', 'Animal Spp Only', 'Plant Spp Only')\n  )\n\n\n\n\n\n\n\n4 Reduce Wealth/Income Metrics\nFor the economics dimension, we will reduce the wealth/income distribution indicator down to unemployment rate and then gini index.\n\n\nCode\nget_str(normed_data)\nget_str(framework)\n\n## Reduce wealth/income indicator to unemploymentRate\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'wealth/income distribution', \n    variable_name != 'unemploymentRate'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_unemployment &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Reduce wealth/income indicator to gini index\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'wealth/income distribution', \n    variable_name != 'gini'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_gini &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Put them together\necon_df &lt;- map_dfr(list(original_scores, to_unemployment, to_gini), ~ {\n  .x$minmax_geometric$dimension_scores %&gt;% \n    as.data.frame() %&gt;%\n    dplyr::filter(str_length(state) == 2) %&gt;% \n    mutate(\n      rank = dense_rank(economics),\n    ) %&gt;% \n    dplyr::filter(state == 'VT') %&gt;% \n    dplyr::select(economics, rank)\n}) %&gt;% \n  mutate(\n    economics = round(economics, 3),\n    iteration = c('Original', 'Unemployment', 'Gini')\n  )\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/metadata_table.html",
    "href": "pages/metadata_table.html",
    "title": "Metadata Table",
    "section": "",
    "text": "This page contains a metadata table for exploring the sources of secondary data used throughout this project.\nUsing the table:\n\nClick column headers to sort\nGlobal search in the top right, or column search in each header\nChange page length and page through results at the bottom\nUse the download button to download a .csv file of the filtered table\nClick the arrow on the left of each row for details, including a URL to the data source.\n\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools\n)\n\n# Load full metadata table\nmetadata_all &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\n# Pick out variables to display\nmetadata &lt;- metadata_all %&gt;% \n  select(\n    metric,\n    'Variable Name' = variable_name,\n    definition,\n    dimension,\n    index,\n    indicator,\n    units,\n    'Year' = latest_year, # Renaming latest year as year, not including og year\n    source,\n    scope,\n    resolution,\n    url\n) %&gt;% \n  setNames(c(str_to_title(names(.))))\n\n###\nhtmltools::browsable(\n  tagList(\n    \n    tags$div(\n      style = \"display: flex; gap: 16px; margin-bottom: 20px; justify-content: center;\",\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Show/hide more columns\"),\n        onclick = \"Reactable.setHiddenColumns('metadata_table', prevColumns =&gt; {\n          return prevColumns.length === 0 ? ['Definition', 'Scope', 'Resolution', 'Url'] : []\n        })\"\n      ),\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Download as CSV\"),\n        onclick = \"Reactable.downloadDataCSV('metadata_table', 'sustainability_metadata.csv')\"\n      )\n    ),\n    \n    reactable(\n      metadata,\n      sortable = TRUE,\n      resizable = TRUE,\n      filterable = TRUE,\n      searchable = TRUE,\n      pagination = TRUE,\n      bordered = TRUE,\n      wrap = TRUE,\n      rownames = FALSE,\n      onClick = 'select',\n      striped = TRUE,\n      pageSizeOptions = c(5, 10, 25, 50, 100),\n      defaultPageSize = 5,\n      showPageSizeOptions = TRUE,\n      highlight = TRUE,\n      style = list(fontSize = \"14px\"),\n      compact = TRUE,\n      fullWidth = TRUE,\n      columns = list(\n        Metric = colDef(\n          minWidth = 200,\n          sticky = 'left'\n        ),\n        'Variable Name' = colDef(\n          minWidth = 150\n        ),\n        Definition = colDef(\n          minWidth = 250\n        ),\n        'Latest Year' = colDef(minWidth = 75),\n        Source = colDef(minWidth = 250),\n        Scope = colDef(show = FALSE),\n        Resolution = colDef(show = FALSE),\n        Url = colDef(\n          minWidth = 300,\n          show = FALSE\n        )\n      ),\n      defaultColDef = colDef(minWidth = 100),\n      elementId = \"metadata_table\",\n      details = function(index) {\n        div(\n          style = \"padding: 15px; border: 1px solid #ddd; margin: 10px 0;\n             background-color: #E0EEEE; border-radius: 10px; border-color: black;\n             box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\",\n          \n          tags$h4(\n            strong(\"Details\"), \n          ),\n          tags$p(\n            strong('Metric Name: '), \n            as.character(metadata_all[index, 'metric']),\n          ),\n          tags$p(\n            strong('Variable Name: '), \n            as.character(metadata_all[index, 'variable_name']),\n          ),\n          tags$p(\n            strong('Definition: '), \n            as.character(metadata_all[index, 'definition']),\n          ),\n          tags$p(\n            strong('Source: '), \n            as.character(metadata_all[index, 'source'])\n          ),\n          tags$p(\n            strong('Latest Year: '), \n            as.character(metadata_all[index, 'latest_year'])\n          ),\n          tags$p(\n            strong('All Years (cleaned, wrangled, and included): '), \n            as.character(metadata_all[index, 'year'])\n          ),\n          tags$p(\n            strong('Updates: '), \n            str_to_title(as.character(metadata_all[index, 'updates']))\n          ),\n          tags$p(\n            strong('URL: '), \n            tags$a(\n              href = as.character(metadata_all[index, 'url']),\n              target = '_blank',\n              as.character(metadata_all[index, 'url'])\n            )\n          )\n        )\n      }\n    )\n  )\n)\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Secondary Data",
      "Metadata"
    ]
  },
  {
    "objectID": "pages/indicator_correlations.html",
    "href": "pages/indicator_correlations.html",
    "title": "Indicator Correlations",
    "section": "",
    "text": "Code\npacman::p_load(\n  dplyr,\n  conflicted,\n  tidyr,\n  tibble,\n  stringr,\n  purrr,\n  tidyr,\n  ggplot2,\n  plotly,\n  reshape,\n  Hmisc,\n  viridisLite,\n  reactable\n)\nsource('dev/get_reactable.r')\nconflicts_prefer(\n  dplyr::select(),\n  dplyr::filter(),\n  dplyr::summarize(),\n  .quiet = TRUE\n)\n\n\nThis page will explore correlations between variables at the indicator level.\n\n1 Correlation Matrix\n\n\nCode\n# Load indicator data.\nfinal_scores &lt;- readRDS('data/state_score_iterations.rds')\nget_str(final_scores)\n\n# Get filtered frame subset to be able to color indicators by dimension later\nfiltered_frame &lt;- readRDS('data/filtered_frame.rds')\ninds_and_dims &lt;- filtered_frame %&gt;% \n  select(indicator, dimension) %&gt;%\n  unique() %&gt;% \n  mutate(color = case_when(\n    dimension == 'economics' ~ 'royalblue',\n    dimension == 'environment' ~ 'darkgreen',\n    dimension == 'health' ~ 'orange',\n    dimension == 'production' ~ 'darkred',\n    dimension == 'social' ~ 'black'\n  ))\ncolor_map &lt;- setNames(inds_and_dims$color, inds_and_dims$indicator)\n\n# Pull out minmax geo indicators only. Also use states only, no aggregates\nminmax_geo_indicators &lt;- final_scores$raw_minmax_geometric$indicator_scores %&gt;% \n  filter(! state %in% c('US_mean', 'US_median', 'NE_median', 'NE_mean'))\nget_str(minmax_geo_indicators)\n\n# Make a correlation matrix using all the selected variables\nmat &lt;- minmax_geo_indicators %&gt;% \n  select(-state) %&gt;% \n  as.matrix()\n\n# Get correlations\ncor &lt;- rcorr(mat, type = 'pearson')\n\n# Melt correlation values and rename columns\ncor_r &lt;- reshape::melt(cor$r) %&gt;% \n  setNames(c('var_1', 'var_2', 'value'))\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Make heatmap with custom text aesthetic for tooltip\nplot_out &lt;- cor_r %&gt;% \n  ggplot(aes(var_1, var_2, fill = value, text = paste0(\n    'Var 1: ', var_1, '\\n',\n    'Var 2: ', var_2, '\\n',\n    'Correlation: ', format(round(value, 3), nsmall = 3), '\\n',\n    'P-Value: ', format(round(p.value, 3), nsmall = 3)\n  ))) + \n  geom_tile() + \n  scale_fill_gradient2(\n    low = \"#762a83\", \n    mid = \"white\", \n    high = \"#1b7837\", \n    midpoint = 0\n  ) +\n  geom_hline(yintercept = 5.5) +\n  geom_hline(yintercept = 14.5) +\n  geom_hline(yintercept = 23.5) +\n  geom_hline(yintercept = 31.5) +\n  geom_vline(xintercept = 5.5) +\n  geom_vline(xintercept = 14.5) +\n  geom_vline(xintercept = 23.5) +\n  geom_vline(xintercept = 31.5) +\n  theme(\n    axis.text.x = element_text(\n      hjust = 1, \n      angle = 45\n    )\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = NULL\n  )\nplot_out\n\n\nCode\n# Save this for preso\n# preso_matrix &lt;- list(mat, color_map)\nsaveRDS(mat, 'preso/data/correlation_data.rds')\n\n\n\n\nCode\n# Fair warning - this is some pretty jenky code. Plotly apparently doesn't want us to know what happens when we make our x and y axes different colors.\n\n# Set options for font and size\nfont_family = 'Arial'\nfont_size = 11\n\n# Convert to interactive plotly figure with text tooltip\nplot &lt;- ggplotly(\n  plot_out, \n  tooltip = 'text',\n  width = 850,\n  height = 650\n)\n\nplot &lt;- plot %&gt;% add_trace(xaxis = 'x2', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(xaxis = 'x3', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(xaxis = 'x4', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(xaxis = 'x5', showscale = FALSE)\n\nplot &lt;- plot %&gt;%\n  plotly::layout(\n    xaxis = list(\n      range = list(0.5, 38.5),\n      tickvals = list(1, 2, 3, 4, 5),\n      tickfont = list(\n        color = '#104E8B',\n        family = font_family,\n        size = font_size\n      )\n    ),\n    xaxis2 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'x',\n      tickangle = -45,\n      ticktext = list(\n        'carbon fluxes',\n        'carbon stocks',\n        'embodied carbon',\n        'forest health',\n        'biodiversity',\n        'land use diversity',\n        'sensitive or rare habitats',\n        'water quality',\n        'water quantity'\n      ),\n      tickvals = list(6, 7, 8, 9, 10, 11, 12, 13, 14),\n      tickfont = list(\n        color = 'darkgreen',\n        family = font_family,\n        size = font_size\n      )\n    ),\n    xaxis3 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'x',\n      tickangle = -45,\n      ticktext = list(\n        'educational attainment',\n        'access to culturally appropriate food',\n        'dietary quality',\n        'food access',\n        'food affordability',\n        'mental health tbd',\n        'access to care',\n        'housing supply and quality',\n        'physical health tbd'\n      ),\n      tickvals = list(15, 16, 17, 18, 19, 20, 21, 22, 23),\n      tickfont = list(\n        color = 'darkred',\n        family = font_family,\n        size = font_size\n      )\n    ),\n    xaxis4 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'x',\n      tickangle = -45,\n      ticktext = list(\n        'total quantity exported',\n        'production species diversity',\n        'production inputs',\n        'total quantity food products',\n        'total quantity forest products',\n        'total quantity non-food ag products',\n        'value added market',\n        'crop failure'\n      ),\n      tickvals = list(24, 25, 26, 27, 28, 29, 30, 31),\n      tickfont = list(\n        color = 'darkorange',\n        family = font_family,\n        size = font_size\n      )\n    ),\n    xaxis5 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'x',\n      tickangle = -45,\n      ticktext = list(\n        'social connectedness',\n        'community safety',\n        'diverse representation',\n        'age diversity',\n        'gender diversity',\n        'racial diversity',\n        'participatory governance'\n      ),\n      tickvals = list(32, 33, 34, 35, 36, 37, 38),\n      tickfont = list(\n        color = 'black',\n        family = font_family,\n        size = font_size\n      )\n    )\n  )\n\nplot &lt;- plot %&gt;% add_trace(yaxis = 'y2', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(yaxis = 'y3', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(yaxis = 'y4', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(yaxis = 'y5', showscale = FALSE)\n\nplot &lt;- plot %&gt;%\n  plotly::layout(\n    yaxis = list(\n      range = list(0.5, 38.5),\n      tickvals = list(1, 2, 3, 4, 5),\n      tickfont = list(\n        color = '#104E8B',\n        family = font_family,\n        size = font_size\n      )\n    ),\n    yaxis2 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'y',\n      tickangle = 0,\n      ticktext = list(\n        'carbon fluxes',\n        'carbon stocks',\n        'embodied carbon',\n        'forest health',\n        'biodiversity',\n        'land use diversity',\n        'sensitive or rare habitats',\n        'water quality',\n        'water quantity'\n      ),\n      tickvals = list(6, 7, 8, 9, 10, 11, 12, 13, 14),\n      tickfont = list(\n        color = 'darkgreen',\n        family = font_family,\n        size = font_size\n      )\n    ),\n    yaxis3 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'y',\n      tickangle = 0,\n      ticktext = list(\n        'educational attainment',\n        'access to culturally appropriate food',\n        'dietary quality',\n        'food access',\n        'food affordability',\n        'mental health tbd',\n        'access to care',\n        'housing supply and quality',\n        'physical health tbd'\n      ),\n      tickvals = list(15, 16, 17, 18, 19, 20, 21, 22, 23),\n      tickfont = list(\n        color = 'darkred',\n        family = font_family,\n        size = font_size\n      )\n    ),\n    yaxis4 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'y',\n      tickangle = 0,\n      ticktext = list(\n        'total quantity exported',\n        'production species diversity',\n        'production inputs',\n        'total quantity food products',\n        'total quantity forest products',\n        'total quantity non-food ag products',\n        'value added market',\n        'crop failure'\n      ),\n      tickvals = list(24, 25, 26, 27, 28, 29, 30, 31),\n      tickfont = list(\n        color = 'darkorange',\n        family = font_family,\n        size = font_size\n      )\n    ),\n    yaxis5 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'y',\n      tickangle = 0,\n      ticktext = list(\n        'social connectedness',\n        'community safety',\n        'diverse representation',\n        'age diversity',\n        'gender diversity',\n        'racial diversity',\n        'participatory governance'\n      ),\n      tickvals = list(32, 33, 34, 35, 36, 37, 38),\n      tickfont = list(\n        color = 'black',\n        family = font_family,\n        size = font_size\n      )\n    )\n  )\n\n# Save this for preso\nhtmlwidgets::saveWidget(plot, 'preso/plots/correlation_plotly.html')\n\n# Show it \nplot\n\n\n\n\nInteractive Correlation Plot\n\n\n\n\n2 Strong Correlations\nWe have many significant correlations between indicators, but we probably don’t care too much about weak correlations. Let’s isolate the correlations that are significant and &gt; 0.5. These are the ones that might suggest we are double-counting certain aspects of the food system.\n\n\nCode\n# Isolate all significant correlations\nget_str(cor_r)\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Add p values to dataframe with correlations\ncor_r$p &lt;- cor_p$value\nget_str(cor_r)\n\n# filter for correlations over 0.5\nsig &lt;- cor_r %&gt;% \n  rowwise() %&gt;%\n  mutate(pair = paste(sort(c(var_1, var_2)), collapse = \"_\")) %&gt;%\n  ungroup() %&gt;%\n  distinct(pair, .keep_all = TRUE) %&gt;%\n  select(-pair) %&gt;% \n  filter(!is.na(p), abs(value) &gt; 0.5)\n\n# Clean up columns for table\nsig &lt;- sig %&gt;% \n  mutate(\n    value = abs(value),\n    across(where(is.numeric), ~ format(round(.x, 3), nsmall = 3))\n  ) %&gt;% \n  setNames(c('Indicator 1', 'Indicator 2', 'Correlation', 'P Value'))\nget_str(sig)\n\n# Get reactable table for next cell\ntable_out &lt;- get_reactable(sig)\n\n# But while we're at it, count how many times each indicator appears\ncor_counts &lt;- c(sig[[1]], sig[[2]]) %&gt;% \n  table() %&gt;%\n  sort(decreasing = TRUE) %&gt;% \n  as.data.frame() %&gt;% \n  setNames(c('indicator', 'correlations'))\nget_str(cor_counts)\n\n# Clean it up for preso\nframework &lt;- readRDS('data/filtered_frame.rds') %&gt;% \n  select(indicator, index, dimension)\n\ncor_table &lt;- cor_counts %&gt;% \n  left_join(framework) %&gt;% \n  select(indicator, index, dimension, correlations) %&gt;% \n  unique() %&gt;%\n  filter(correlations &gt; 0) %&gt;% \n  setNames(c(str_to_title(names(.))))\nget_str(cor_table)\n\n# Save this for preso  \nsaveRDS(cor_table, 'preso/data/correlation_counts.rds')\n\n\n\n\n\n\n\n\nThe wealth/income distribution indicator (economics) is correlating strongly with several indicators, some from the economics dimension and some from health. Note that there are several metrics in that indicator related to median earnings, which might be a proxy for gdp per capita. Now that I look at this, it might be worth including gdp per capita at least as a control variable to see how much fo the variation it accounts for.\nIt looks like all the indicators from the carbon index (embodied, fluxes, stocks) correlate with one another, which makes enough sense. I imagine that one shouldn’t be too much of a problem if they are being aggregated at the index level anyway.\nForest health and carbon stocks are currently quite highly correlated, but this is because the metrics for carbon stocks are not ideal. The metrics for carbon stocks and forest health all come from the same TreeMap dataset. I suspect that if we include a better set of metrics for carbon stocks, this won’t be a such a problem.\nValue-added markets and operations diversification are all using a very similar set of metrics as well. They mostly come from NASS, and it would be worth digging into the NASS docs to see how whether value-added sales might overlap with agritourism, direct to consumer sales, or local marketing channel sales.\nFood affordability and food security also unsurprisingly correlate strongly. The current framework here is a work in progress and a bit haphazard. It will need some reworking. Curiously, these indicators also strongly correlate with participatory governance. That’s quite an interesting finding.\nAs for what to do about highly correlating indicators in general:\n\nThey could be reworked to use metrics that don’t lead to indicator correlations. This sounds rather difficult to me, and maybe impossible. It seems likely to be the reality that aspects of the economics and health dimensions are indeed related, for example.\nThey could be weighted in their respective dimensions to account for the correlations. This might be done with PCA loadings or by expert opinion.\nWe could also leave them as is. This would mean potentially double-counting certain aspects, but may be a reasonable approximation of reality.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Analysis",
      "Indicator Correlations"
    ]
  },
  {
    "objectID": "pages/comparison.html",
    "href": "pages/comparison.html",
    "title": "Comparison of Aggregation Methods",
    "section": "",
    "text": "In the last page we created 18 sets of scores by state based on combinations of three transformatino methods (raw, winsor, box cox), three rescaling methods (z-scores, min max, rank) and two aggregation methods (arithmetic, geometric). Here, we will explore differences between them in terms of state distributions and rankings.\nNote that each set of spider plots are scaled to the minimum and maximum of any single state in that dimension, given the rescaling and aggregation methods. This means in the case of min-max scaling, for example, raw metrics are scaled from 0 to 1, arithmetic and geometric means consolidate values to dimension scores, and these sets of dimension scores are scaled on the plot from the lowest to the highest value of any state. A “perfect” score here means that it is the best of any state. Plots show dimension values for Vermont in green. The dotted purple polygon behind it is the median of US states. Arithmetic means are on the left, and geometric on the right.\nBe aware that spider/radar charts can be hard to interpret, and sometimes misleading The Radar Chart and its Caveats. The order of variables makes a big impact on the area of chart, and area is not a terribly reliable way to show differences, as it increases quadratically as variables increase linearly. Will explore some other ways to show this information, but using these for now as they are quite popular in the literature for sustainability metrics.\n\n1 Min Max\nWe are starting with min max normalization, which is probably the most intuitive transformation. Every metric is scaled from 0 to 1, so the lowest value from any state becomes 0, and the highest value from any state becomes 1. Indicators, indices, and dimensions are then aggregated using arithmetic or geometric means.\nThe transformation is easy to understand, but has some drawbacks, including being quite sensitive to outliers. Strong outliers in either direction could condense the rest of the distribution, making it seem like most states are very low or very high.\n\n\nCode\n# Load state score data\ndat &lt;- readRDS('data/state_score_iterations.rds')\n# get_str(dat)\n\n# Custom function to create spider plots\nget_vt_spiders(dat, 'raw_minmax')\n\n\n\n\n\n\n\n\n\n\n\nCode\nget_vt_spiders(dat, 'winsor_minmax')\n\n\n\n\n\n\n\n\n\n\n\nCode\nget_vt_spiders(dat, 'boxcox_minmax')\n\n\n\n\n\n\n\n\n\n\n\n2 Z-Scores\nZ-scores are standardized values. We subtract the mean from every metric to center it at 0, then divide by the standard deviation to get units in terms of standard deviations. This should be more robust to outliers than the min-max method; extreme values have no effect on values. However, rescaling Z-scores to fit the spider plots might have a similar effect.\n\n\nCode\nget_vt_spiders(dat, 'raw_zscore')\n\n\n\n\n\n\n\n\n\n\n\nCode\nget_vt_spiders(dat, 'winsor_zscore')\n\n\n\n\n\n\n\n\n\n\n\nCode\nget_vt_spiders(dat, 'boxcox_zscore')\n\n\n\n\n\n\n\n\n\n\n\n3 Rank\n\n\nCode\nget_vt_spiders(dat, 'raw_rank')\n\n\n\n\n\n\n\n\n\n\n\nCode\nget_vt_spiders(dat, 'winsor_rank')\n\n\n\n\n\n\n\n\n\n\n\nCode\nget_vt_spiders(dat, 'boxcox_rank')\n\n\n\n\n\n\n\n\n\n\n\n4 References\n\n\n\n\n Back to top",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/aggregation.html",
    "href": "pages/aggregation.html",
    "title": "Metric Aggregation",
    "section": "",
    "text": "1 Introduction\nExploring methods of transforming, rescaling, and aggregating data into indicator, index, and dimension scores. Using the OECD handbook on composite indices as a guide (OECD 2008).\n\nSome examples to pull from:\n\nSchneider et al. (2023)\n\nrank order comparisons only\ncompare to global weighted means by groups based on GDP\nmin max scaling to show distance from global groups\n\nBéné et al. (2019)\n\nBox cox for most skewed indicators (skew &gt; 2) then min max\ngeometric means for enviro and economic dimensions\narithmetic means for social and food dimensions\ngeometric mean for combining all four dimensions into one\n\nNicoletti (2000)\n\nUsed normalized square loadings (indicator weights) to weight each indicator\n\nGómez-Limón and Sanchez-Fernandez (2010)\n\nmin max normalization\ncompared several different methods of aggregation\n\nmostly correlated, no big differences\nweighted sum of indicators\nproduct of weighted indicators\nmuilticriteria function based on distance to ideal point\n\nWeighting - did both PCA and analytic hierarchy process\nValidation (identifying important factors) - double censored tobit - index as dependent, indicators as independent\n\nAdamu Demelash and Abate Alemu (2024)\n\nNormalization with distance to reference\n\nreference determined by quartile analysis\nnot affected by outliers, extreme values\n\nEqual weighting\nLinear aggregation\nAdditive method for indicators within dimensions\nGeometric means for aggregate scores across four dimensions\n\n\n\n\n\n2 Imputation\nFirst, check how much missing data there are. If it is within reason, use missForest algorithm to impute missing data (Stekhoven and Bühlmann 2012). This is particularly good at handling MAR data, and does a decent job at handling MNAR data and non-linear relationships as well. If less than 5% of data are missing, just about any method for handling it is reasonable, even listwise deletion (Beaujean 2013).\n\n\nCode\nmetrics_df &lt;- readRDS('data/metrics_df.rds')\nget_str(metrics_df)\n\n# Check for missing data\nskimr::skim(metrics_df)\nmis_dat &lt;- sum(is.na(metrics_df))/(nrow(metrics_df)*(ncol(metrics_df) - 1)) * 100 \nmis_dat &lt;- round(mis_dat, 3)\n\n# Change fips from column to rowname so we can impute without losing it\nmetrics_df &lt;- metrics_df %&gt;% \n  column_to_rownames('fips')\nget_str(metrics_df)\n\n# Impute missing variables\nset.seed(42)\nmf_out &lt;- metrics_df %&gt;%\n  missForest(\n    ntree = 200,\n    mtry = 10,\n    verbose = FALSE,\n    variablewise = FALSE\n  )\nget_str(mf_out)\n\n# Extract OOB error\n(oob &lt;- mf_out$OOBerror)\n\n# Check missing again\nskimr::skim(mf_out$ximp)\n# Looks good\n\n# Save just imputed data\nimp_dat &lt;- mf_out$ximp\n\n\nWe had 0.785% missing data, which is rather little, and gives us flexibility in handling it. The Out-of-Bag (OOB) error, quantified by the normalized residual mean squared error (NRMSE) the missForest imputation algorithm was 0.\n\n\n3 Transformations\nTransforming distributions and dealing with outliers. Three options.\nRaw Values\nJust as it sounds - do nothing. More interpretable, but vulnerable to outliers that throw off the distribution of scores.\nWinsorization\nShift extreme values to the 5th and 95th percentiles. Does not reward overperformance in any one area, but is rather heavy-handed in reshaping the distribution and leads to information loss.\nBox Cox (Bickel and Doksum 1981)\nBox-Cox transformations are non-linear transformations that use an optimal value of lambda to make the distribution as normal as possible. This has some strengths in that the data are easier to work with in further analyses. It also effectively pulls outliers inward toward the center of the distribution. However, it also changes relationships between the variables, so it will distort any bivariate correlations.\n\\[\\begin{equation}\n{\\rm For}\\ \\lambda\\neq0,\\ f\\lambda(x) = (sign(x)|x|^\\lambda-1)/\\lambda\n\\end{equation}\\]\n\\[\\begin{equation}\n{\\rm For}\\ \\lambda = 0,\\ f_0(x) = log(x)\n\\end{equation}\\]\n\n\nCode\n# List of results of transformations\ntransformed &lt;- list()\nget_str(imp_dat)\n\n# Raw - just leave it as is\ntransformed$raw &lt;- imp_dat\n\n# Box Cox. If there are negatives, shifting to remove them\ntransformed$boxcox &lt;- imp_dat %&gt;% \n  mutate(across(everything(), ~ {\n    if (any(.x &lt;= 0)) {\n      shift &lt;- abs(min(.x)) + 1\n      vals &lt;- .x + shift\n    } else {\n      vals &lt;- .x\n    }\n    optimal_lambda &lt;- forecast::BoxCox.lambda(vals, method = 'loglik')\n    print(paste0('Optimal lambda: ', optimal_lambda))\n    out &lt;- forecast::BoxCox(vals, lambda = optimal_lambda)\n    return(out)\n  }))\nget_str(transformed$boxcox)    \n\n# Winsorization\ntransformed$winsor &lt;- imp_dat %&gt;% \n  mutate(across(everything(), DescTools::Winsorize))\nget_str(transformed$winsor)\n\n# Check\nmap(transformed, get_str)\n\n\n\n\n4 Rescaling\nWe are rescaling our data using five methods: rank order, min-max, and Z-scores.\nRank Order\nYields nice clean distributions, but means lots of lost information. used by Schneider et al. (2023) in the Food Systems Countdown to 2030.\nMin Max (OECD 2008)\nMin-maxing scales all the data from 0 to 1 by subtracting the minimum value of each variable from all cases and dividing by the range of all cases in the variable. It is rather intuitive, as 1 is the best score, and 0 is the worst. This is a linear transformation, so the relationships between the values should not change.\n\\[\\begin{equation}\nI^t_qc = \\frac{x^t_qc - min_c(x^{t_0}_q)}{max_c(x^{t_0}_q)-min_c(x^{t_0}_q)}\n\\end{equation}\\]\nWhere \\(x^t_qc\\) is the metric \\(q\\) for state \\(c\\) at time \\(t\\).\nZ-Scores (OECD 2008)\nZ-scores are stardized to have a mean of 0 and a standard deviation of 1. Larger numbers are better, but there are no caps on the highest or lowest values. A value of 2 would mean that it is 2 standard deviations greater than the mean. Again, this is a linear transformation, so relationships between variables should not change.\n\\[\\begin{equation}\nI^t_{qc} = \\frac{x^t_{qc}-x^t_{qc=\\overline{c}}}{\\sigma^t_{qc=\\overline{c}}}\n\\end{equation}\\]\nWhere \\(x^t_qc\\) is the metric \\(q\\) for state \\(c\\) at time \\(t\\).\n\n\nCode\nget_str(imp_dat)\n\n# List of results\nscaled &lt;- list()\n\n# Z-scores, one for each transformation\nscaled$zscore &lt;- map(transformed, \\(trans) {\n  trans %&gt;% \n    mutate(across(\n      everything(),\n      ~ as.numeric(scale(.x, scale = TRUE, center = TRUE))\n    ))\n})\nget_str(scaled$zscore)\n\n# Min Max\nmin_max &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\nscaled$minmax &lt;- map(transformed, \\(trans) {\n  trans %&gt;% \n    mutate(across(everything(), min_max))\n})\nget_str(scaled$minmax)  \n\n# Rank order from lowest to highest value for each var. We are coding this such\n# that higher ranks are better. So 51 should have the highest/best value and\n# rank 1 should have the worst.\nscaled$rank &lt;- map(transformed, \\(trans) {\n  map(names(trans), \\(col_name) {\n    trans %&gt;% \n      rownames_to_column('fips') %&gt;% \n      dplyr::select(fips, col_name) %&gt;% \n      mutate(!!sym(col_name) := dense_rank(.data[[col_name]]))\n  }) %&gt;% \n    reduce(full_join) %&gt;% \n    column_to_rownames('fips')\n})\nget_str(scaled$rank)\n\n# Check\nmap(scaled, get_str)\n\n# Unlist these so instead of 3x3 it is 1x9\nscaled &lt;- purrr::list_flatten(scaled, name_spec = \"{inner}_{outer}\")\nnames(scaled)\nget_str(scaled)\n\n\n\n\n5 Directional Values\nHere, we are assuming that each metric has a direction that is more sustainable than the opposite. Either more of it is better, or less of it is better. This is rather problematic in that just about any metric becomes negative with too much or too little of it. What might make more sense in the long run would be to consult the expertise of our teams and develop targets or acceptable ranges for some metrics once they are settled. Still, just about every sustainability indicator framework does some variation of this one-way value system (Schneider et al. 2023; Béné et al. 2019; Nicoletti 2000; Jacobi et al. 2020; Gómez-Limón and Sanchez-Fernandez 2010).\nAlas, for now we will invert variables in each of the transformed datasets as necessary so that larger numbers are more sustainable, and smaller numbers are less sustainable. The table below shows this assignment in the desirable column. For a handful of variables (vacancy rate, mean farm size, etc.) I was not comfortable assigning one direction as better than the other, so I have removed them from the refined framework.\n\n\nCode\n# Check variable names\n(vars &lt;- names(scaled[[1]]))\n\n# Higher numbers should be better. Reverse metrics that are the opposite, \n# where lower numbers are better. Only listing reverse here - metrics are \n# implicitly better with larger numbers otherwise.\nreverse &lt;- c(\n  'unemploymentRate',\n  'gini',\n  'lowBirthweight',\n  'teenBirths',\n  'uninsured',\n  'incomeInequality',\n  'childrenInSingleParentHouseholds',\n  'injuryDeaths',\n  'airPollutionParticulateMatter',\n  'drinkingWaterViolations',\n  'severeHousingProblems',\n  'prematureAgeAdjustedMortality',\n  'infantMortality',\n  'frequentPhysicalDistress',\n  'frequentMentalDistress',\n  'diabetesPrevalence',\n  'hivPrevalence',\n  'limitedAccessToHealthyFoods',\n  'drugOverdoseDeaths',\n  'disconnectedYouth',\n  'residentialSegregationBlackWhite',\n  'suicides',\n  'motorVehicleCrashDeaths',\n  'severeHousingCostBurden',\n  'schoolSegregation',\n  'childCareCostBurden',\n  'wicPercEligible', # Iffy on this one\n  'droughtMeanPercArea',\n  'pctAtRiskAnimalSpp',\n  'pctAtRiskPlantSpp',\n  'pctAtRiskBeeSpp',\n  'pctAtRiskOrchidSpp',\n  'pctAtRiskEcosystems',\n  'expChemicalPct',\n  'ageProducers', # Could be better to use age diversity?\n  'waterIrrSrcOffFarmExp',\n  'waterIrrSrcOffFarmExpPerAcreFt',\n  'CH4FromAg',\n  'N2OFromAg',\n  'CO2FromAg',\n  'propAreaFsaSecDisasters',\n  'totalCapConsNoDwellings',\n  'totalIntExpRealEstateNoDwellings',\n  'totalIncomeInsuranceIndemnities',\n  'totalIncomeInsuranceIndemnitiesFederal',\n  'totalValueEmergPayments',\n  'totalValueOtherAdHocEmergPayments',\n  'totalValueDairyMarginProtPayments',\n  'totalValueAllLossCoveragePayments',\n  'totalValueAgRiskCoveragePayments',\n  'totalCapExpBldgsLandNoDwellings',\n  'alcoholImpairedDrivingDeaths' \n)\n\n# Iffy: landValPF, landValPerAcre - in good column for now, but unclear\n# indemnities and emergency payments - in bad column for now, but more access\n# coud be good?\n\n# Some are unclear - without clear direction, better to remove:\nremove &lt;- c(\n  'vacancyRate',\n  'expHiredLaborPercOpExp',\n  'acresPF',\n  'medianAcresPF',\n  'importsTopFive'\n)\n\n## Remove the unclear ones from all three datasets\n# Then for each transformation, flip values in a way that makes sense\nvalued &lt;- imap(scaled, \\(df, method) {\n  df %&gt;% \n    select(-all_of(remove)) %&gt;% \n    mutate(\n      across(all_of(reverse), ~ case_when(\n        str_detect(method, 'rank') ~ max(.x) - .x + 1,\n        str_detect(method, 'zscore') ~ .x * -1,\n        str_detect(method, 'minmax') ~ 1 - .x,\n        .default = NA\n      )),\n      across(everything(), as.numeric)\n    )\n})\nmap(valued, get_str)\n\n# Compare\nchecklist &lt;- list(scaled, valued)\nmap(checklist, ~ .x$raw_rank[[1]])\nmap(checklist, ~ .x$winsor_zscore[[1]])\nmap(checklist, ~ .x$raw_minmax[[1]])\n\n# Save this as our 'normalized data' that we use for building scores\nsaveRDS(valued, 'data/valued_rescaled_metrics.rds')\n\n\n\n\nCode\n## Show table of which metrics were set in which direction\nsm_data &lt;- readRDS('data/sm_data.rds')\nmeta &lt;- sm_data$metadata\n\n# Reactable table showing var, metric, source, and direction\ntable &lt;- meta %&gt;% \n  dplyr::filter(variable_name %in% names(imp_dat)) %&gt;% \n  mutate(desirable = case_when(\n    variable_name %in% reverse ~ 'Lower',\n    variable_name %in% remove ~ 'Removed',\n    .default = 'Higher'\n  )) %&gt;% \n  select(\n    metric, \n    variable_name, \n    dimension,\n    index,\n    indicator,\n    desirable, \n    definition, \n    source\n  )\nget_str(table)\n\n# Save this desirable direction table for preso\nsaveRDS(table, 'preso/data/desirable_directions_table.rds')\n\n\n\n\nCode\n# Make reactable table\ntable %&gt;% \n  get_reactable(\n    defaultPageSize = 5,\n    columns = list(\n      'definition' = colDef(\n        minWidth = 150\n      ),\n      'source' = colDef(\n        minWidth = 150\n      )\n    )\n  )\n\n\n\n\n\n\n\n\n6 Aggregation\nHere we are combining values in each indicator, index, and dimension using both arithmetic and geometric means (OECD 2008). Arithmetic means are fully compensable, in that a strong score in one area can make up for a weak score in another. Geometric means are only somewhat compensable - it effectively applies a penalty for unbalanced scores. We might also consider PCA here, but it does not do terribly well with strong a prior hypotheses like we have.\nNote that we are using some functions to automate this process. They can be found in dev/get_aggregations.R.\nIndicator aggregation:\n\n\nCode\n# We need to attach these back to framework from metadata\n# Filter frame from earlier down to our current metrics\n# We are also removing the 'remove' metrics without clear directional values\nframe &lt;- readRDS('data/frame.rds')\nfiltered_frame &lt;- frame %&gt;% \n  dplyr::filter(variable_name %in% names(valued[[1]])) %&gt;% \n  dplyr::select(variable_name, indicator, index, dimension)\nget_str(filtered_frame)\n\n# Save this for later - use in regression and variable selection \nsaveRDS(filtered_frame, 'data/filtered_frame.rds')\n\n# Using modular functions here to do each step. \n# See dev/get_aggregations.R for details\nindicator_scores &lt;- get_agg_indicators(\n  valued, \n  filtered_frame, \n  aggregation = 'both'\n)\nget_str(indicator_scores)\n\n\nIndex aggregation:\n\n\nCode\n# For each set of indicator scores, calculate index scores\n# Using custom modular function\nindex_scores &lt;- get_agg_indices(indicator_scores, filtered_frame)\nget_str(index_scores)\n\n\nDimension aggregation:\n\n\nCode\n# Custom function\ndimension_scores &lt;- get_agg_dimensions(index_scores, filtered_frame)\nget_str(dimension_scores)\n\n\n\n\n7 Wrangle\nHere, we organize arithmetic and geometric means for each level of the framework (indicator, index, dimension) in a way that is easier to work with. We also add means and medians for all US states as well as New England states that we can use as points of comparison.\n\n\nCode\nstate_key &lt;- readRDS('data/state_key.rds')\n\nget_str(indicator_scores, 4)\nget_str(index_scores, 4)\nget_str(dimension_scores, 4)\n\n# Want to end up with 18 versions: 3 transforms, 3 rescalings, 2 aggregations\n# Put them all together in one list to work with\n\n# Actually switching to modular function here too:\n# Run whole process with function\nfinal_scores &lt;- get_all_aggregations(\n  normed_data = valued,\n  framework = filtered_frame,\n  state_key = state_key,\n  aggregation = 'both'\n)\nnames(final_scores)\nget_str(final_scores)\n# Now we have a list of 18 iterations\n\n# Save this for use elsewhere\nsaveRDS(final_scores, 'data/state_score_iterations.rds')\n\n\nThis gives us a list of 18 elements, one for each combination of transformation method, rescaling method, and aggregation method.\n\n\n\n\n\n\n\n\n Back to top8 References\n\nAdamu Demelash, Sewareg, and Esubalew Abate Alemu. 2024. “Measuring Food System Sustainability in Ethiopia: Towards a Multi-Dimensional Perspective.” Ecological Indicators 161 (April): 111991. https://doi.org/10.1016/j.ecolind.2024.111991.\n\n\nBeaujean, A. Alexander. 2013. “Factor Analysis Using R.” https://doi.org/10.7275/Z8WR-4J42.\n\n\nBéné, Christophe, Steven D. Prager, Harold A. E. Achicanoy, Patricia Alvarez Toro, Lea Lamotte, Camila Bonilla, and Brendan R. Mapes. 2019. “Global Map and Indicators of Food System Sustainability.” Scientific Data 6 (1): 279. https://doi.org/10.1038/s41597-019-0301-5.\n\n\nBickel, Peter J., and Kjell A. Doksum. 1981. “An Analysis of Transformations Revisited.” Journal of the American Statistical Association 76 (374): 296–311. https://doi.org/10.1080/01621459.1981.10477649.\n\n\nGómez-Limón, José A., and Gabriela Sanchez-Fernandez. 2010. “Empirical Evaluation of Agricultural Sustainability Using Composite Indicators.” Ecological Economics 69 (5): 1062–75. https://doi.org/10.1016/j.ecolecon.2009.11.027.\n\n\nJacobi, Johanna, Stellah Mukhovi, Aymara Llanque, Markus Giger, Adriana Bessa, Christophe Golay, Chinwe Ifejika Speranza, et al. 2020. “A New Understanding and Evaluation of Food Sustainability in Six Different Food Systems in Kenya and Bolivia.” Scientific Reports 10 (1): 19145. https://doi.org/10.1038/s41598-020-76284-y.\n\n\nNicoletti, Giuseppe. 2000. “Summary Indicators of Product Market Regulation with an Extension to Employment Protection Legislation.” {{OECD Economics Department Working Papers}} 226. Vol. 226. OECD Economics Department Working Papers. https://doi.org/10.1787/215182844604.\n\n\nOECD. 2008. Handbook on Constructing Composite Indicators: Methodology and User Guide. Paris: Organisation for Economic Co-operation and Development.\n\n\nSchneider, Kate R., Jessica Fanzo, Lawrence Haddad, Mario Herrero, Jose Rosero Moncayo, Anna Herforth, Roseline Remans, et al. 2023. “The State of Food Systems Worldwide in the Countdown to 2030.” Nature Food 4 (12): 1090–110. https://doi.org/10.1038/s43016-023-00885-9.\n\n\nStekhoven, Daniel J., and Peter Bühlmann. 2012. “MissForest—Non-Parametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics 28 (1): 112–18. https://doi.org/10.1093/bioinformatics/btr597.",
    "crumbs": [
      "Analysis",
      "Aggregation"
    ]
  },
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "Sustainability Metrics",
    "section": "",
    "text": "Caution\n\n\n\n\n\nThe Sustainability Metrics project, as well as this site itself, are works in progress. All data and analyses shown here are preliminary. If you have any questions, comments, or suggestions about this site or the accompanying Shiny app, feel free to reach out to Chris at christopher.donovan@uvm.edu.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/index.html#sec-intro",
    "href": "pages/index.html#sec-intro",
    "title": "Sustainability Metrics",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n\n\nIntervale Center, Burlington, Vermont. Copyright: Sally McCay, UVM Photo.\n\n\nResilient food systems are increasingly recognized as essential, not only in meeting human needs, but in doing so within planetary bounds (Conijn et al. 2018). Approximately 42% of world’s population depend on agriculture for employment, which is a challenging endeavor in the face of farm consolidation, changing consumption patterns, and climate change (Giller et al. 2021; Aznar-Sánchez et al. 2019). Food systems themselves are responsible for one-third of greenhouse gas emissions, while anthropogenic climate change has reduced agricultural output by 21% in the last 60 years (Crippa et al. 2021; Ortiz-Bobea et al. 2021).\nTools for diagnosing and monitoring the sustainability of food systems are thus vital (Fanzo et al. 2021). However, there is little consensus on how to define, let alone measure food system sustainability (Allen and Prosperi 2016; Béné et al. 2019). And while there is an abundance of research at the global level (Bathaei and Štreimikienė 2023; Chaudhary, Gustafson, and Mathys 2018), there exist gaps in understanding at the local, regional, and landscape levels (Dale et al. 2012).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/index.html#sustainability-metrics",
    "href": "pages/index.html#sustainability-metrics",
    "title": "Sustainability Metrics",
    "section": "2 Sustainability Metrics",
    "text": "2 Sustainability Metrics\n\n\n\nSpread from the Climate Kitchen harvest dinner. Photo credit: Colleen Goodhue, FSRC.\n\n\nThe Sustainability Metrics project is an effort to develop both the conceptual and methodological frameworks to define and measure regional food system sustainability in New England. The framework could be used to monitor sustainability over time and inform interventions at the policy and farm levels, creating a healthier and more resilient food system for both social and ecological ends.\nThe project is led by the Food Systems Research Center at the University of Vermont in partnership with, and funded by, the USDA ARS Food Systems Research Unit in Burlington, Vermont. Five teams of researchers and numerous community partners are currently conducting primary research on the development and measurement of indicators for food system sustainability. You can find more information about this work at the UVM FSRC Sustainability Metrics website. For now, what you will find here is a growing collection of secondary data, visualizations, and exploratory analyses to help support the project.\nMetadata and citations will be provided throughout the document, but it is worth appreciating the work of the folks at USDA AMS Food and Agriculture Mapper and Explorer in particular, as many of the data shown here were cleaned and compiled in their data warehouse. Considerable inspiration was also taken from the Food Systems Dashboard, developed by the Global Alliance for Improved Nutrition.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/index.html#about-fsrc",
    "href": "pages/index.html#about-fsrc",
    "title": "Sustainability Metrics",
    "section": "3 About FSRC",
    "text": "3 About FSRC\nThe Food Systems Research Center at the University of Vermont is transforming the research landscape by funding collaborative projects that put people and the planet first, break down traditional academic silos and are integrated with and responsive to the needs of the communities we serve, including decision-makers, farmers, and food systems actors.\nRooted in the belief that no one group can find the answers alone, FSRC empowers researchers to work together across disciplines to address critical issues like soil health, food security, and climate resilience. Instead of funding research that leads to short-term fixes, our commitment is to give researchers the freedom, resources, and time they need to do relevant research that will inform policies, practices, and programs that will long outlast their work.\nFSRC considers the relationship of food systems across scales from local to global and is a partnership between UVM and the U.S. Department of Agriculture (USDA) Agricultural Research Service (ARS). FSRC’s transdisciplinary approach prioritizes research that studies food systems as a whole, including the networks of people, institutions, physical infrastructure, and natural resources through which food is grown, processed, distributed, sold, prepared, and eaten.\nLearn more about us at the Food Systems Research Center website.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/index.html#license",
    "href": "pages/index.html#license",
    "title": "Sustainability Metrics",
    "section": "4 License",
    "text": "4 License\n\n    This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. \n\n\n    The code is licensed under the GNU General Public License v3.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html",
    "href": "pages/indicator_distributions.html",
    "title": "Indicator Distributions",
    "section": "",
    "text": "Now that our metrics have been aggregated into indicators, we can observe the univariate distributions of the indicators themselves. We will do this for all six sets of transformations.",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#rank-arithmetic",
    "href": "pages/indicator_distributions.html#rank-arithmetic",
    "title": "Indicator Distributions",
    "section": "2.1 Rank Arithmetic",
    "text": "2.1 Rank Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'raw_rank_arithmetic')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#rank-geometric",
    "href": "pages/indicator_distributions.html#rank-geometric",
    "title": "Indicator Distributions",
    "section": "2.2 Rank Geometric",
    "text": "2.2 Rank Geometric\n\n\nCode\nget_indicator_distributions(scores, 'raw_rank_geometric')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#min-max-arithmetic",
    "href": "pages/indicator_distributions.html#min-max-arithmetic",
    "title": "Indicator Distributions",
    "section": "2.3 Min Max Arithmetic",
    "text": "2.3 Min Max Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'raw_minmax_arithmetic')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#min-max-geometric",
    "href": "pages/indicator_distributions.html#min-max-geometric",
    "title": "Indicator Distributions",
    "section": "2.4 Min Max Geometric",
    "text": "2.4 Min Max Geometric\n\n\nCode\nget_indicator_distributions(scores, 'raw_minmax_geometric')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#z-score-arithmetic",
    "href": "pages/indicator_distributions.html#z-score-arithmetic",
    "title": "Indicator Distributions",
    "section": "2.5 Z-Score Arithmetic",
    "text": "2.5 Z-Score Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'raw_zscore_arithmetic')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#z-score-geometric",
    "href": "pages/indicator_distributions.html#z-score-geometric",
    "title": "Indicator Distributions",
    "section": "2.6 Z-Score Geometric",
    "text": "2.6 Z-Score Geometric\n\n\nCode\nget_indicator_distributions(scores, 'raw_zscore_geometric')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#rank-arithmetic-1",
    "href": "pages/indicator_distributions.html#rank-arithmetic-1",
    "title": "Indicator Distributions",
    "section": "3.1 Rank Arithmetic",
    "text": "3.1 Rank Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'winsor_rank_arithmetic')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#rank-geometric-1",
    "href": "pages/indicator_distributions.html#rank-geometric-1",
    "title": "Indicator Distributions",
    "section": "3.2 Rank Geometric",
    "text": "3.2 Rank Geometric\n\n\nCode\nget_indicator_distributions(scores, 'winsor_rank_geometric')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#min-max-arithmetic-1",
    "href": "pages/indicator_distributions.html#min-max-arithmetic-1",
    "title": "Indicator Distributions",
    "section": "3.3 Min Max Arithmetic",
    "text": "3.3 Min Max Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'winsor_minmax_arithmetic')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#min-max-geometric-1",
    "href": "pages/indicator_distributions.html#min-max-geometric-1",
    "title": "Indicator Distributions",
    "section": "3.4 Min Max Geometric",
    "text": "3.4 Min Max Geometric\n\n\nCode\nget_indicator_distributions(scores, 'winsor_minmax_geometric')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#z-score-arithmetic-1",
    "href": "pages/indicator_distributions.html#z-score-arithmetic-1",
    "title": "Indicator Distributions",
    "section": "3.5 Z-Score Arithmetic",
    "text": "3.5 Z-Score Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'winsor_zscore_arithmetic')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#z-score-geometric-1",
    "href": "pages/indicator_distributions.html#z-score-geometric-1",
    "title": "Indicator Distributions",
    "section": "3.6 Z-Score Geometric",
    "text": "3.6 Z-Score Geometric\n\n\nCode\nget_indicator_distributions(scores, 'winsor_zscore_geometric')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#rank-arithmetic-2",
    "href": "pages/indicator_distributions.html#rank-arithmetic-2",
    "title": "Indicator Distributions",
    "section": "4.1 Rank Arithmetic",
    "text": "4.1 Rank Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'boxcox_rank_arithmetic')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#rank-geometric-2",
    "href": "pages/indicator_distributions.html#rank-geometric-2",
    "title": "Indicator Distributions",
    "section": "4.2 Rank Geometric",
    "text": "4.2 Rank Geometric\n\n\nCode\nget_indicator_distributions(scores, 'boxcox_rank_geometric')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#min-max-arithmetic-2",
    "href": "pages/indicator_distributions.html#min-max-arithmetic-2",
    "title": "Indicator Distributions",
    "section": "4.3 Min Max Arithmetic",
    "text": "4.3 Min Max Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'boxcox_minmax_arithmetic')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#min-max-geometric-2",
    "href": "pages/indicator_distributions.html#min-max-geometric-2",
    "title": "Indicator Distributions",
    "section": "4.4 Min Max Geometric",
    "text": "4.4 Min Max Geometric\n\n\nCode\nget_indicator_distributions(scores, 'boxcox_minmax_geometric')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#z-score-arithmetic-2",
    "href": "pages/indicator_distributions.html#z-score-arithmetic-2",
    "title": "Indicator Distributions",
    "section": "4.5 Z-Score Arithmetic",
    "text": "4.5 Z-Score Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'boxcox_zscore_arithmetic')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html#z-score-geometric-2",
    "href": "pages/indicator_distributions.html#z-score-geometric-2",
    "title": "Indicator Distributions",
    "section": "4.6 Z-Score Geometric",
    "text": "4.6 Z-Score Geometric\n\n\nCode\nget_indicator_distributions(scores, 'boxcox_zscore_geometric')",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/metrics_table.html",
    "href": "pages/metrics_table.html",
    "title": "Metrics Table",
    "section": "",
    "text": "On this page you can download a bulk .csv file for all the secondary data metrics collected so far in the project, which is around 650 variables. The file is ~ 40MB. Use the Download Bulk CSV button below to download it. Note that the metadata table from the last page can be used to identify and define the the variable names. To download a key to match FIPS codes to state and county names, use the Download FIPS Key button.\nSoon to come on this page is an interactive table of metrics. The file size is large enough that manipulating it in the Quarto page is unwieldy, so it will have to link to a separate database. Coming soon.\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools,\n  readr\n)\n\nmetrics_file_name = paste0(Sys.Date(), \"_bulk_metrics.csv\")\nfips_file_name = paste0(Sys.Date(), \"_fips_key.csv\")\n\ntagList(\n  tags$div(\n    style = \"display: flex; gap: 100px; margin-bottom: 20px; justify-content: center;\",\n    tags$a(\n      class = \"btn btn-primary\",\n      style = \"display: flex; width: 200px; justify-content: center; align-items: center;\",\n      href = '../data/bulk_metrics.csv',\n      download = metrics_file_name,\n      tagList(fontawesome::fa(\"download\"), \"Download Bulk .CSV\")\n    ),\n   tags$a(\n      class = \"btn btn-primary\",\n      style = \"display: flex; width: 200px; justify-content: center; align-items: center;\",\n      href = '../data/all_fips_key.csv',\n      download = fips_file_name,\n      tagList(fontawesome::fa(\"download\"), \"Download FIPS Key\")\n    )\n  )\n)\n\n\n\n\n\nDownload Bulk .CSV\n\n\n\nDownload FIPS Key\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Secondary Data",
      "Metrics Data"
    ]
  },
  {
    "objectID": "pages/metric_distributions.html",
    "href": "pages/metric_distributions.html",
    "title": "Metric Distributions",
    "section": "",
    "text": "This page explores metric distributions before any transformations. Use this to inform how we might want to deal with outliers or normalize data at the metric level.\n\n1 Distributions\nHighly skewed distributions might be good candidates for Box-Cox transformations or Winsorization. The figure below shows metrics with a skew &gt; 2 in red, while those with a skew &lt; 2 are in blue.\n\n\nCode\npacman::p_load(\n  ggplot2,\n  purrr,\n  ggpubr,\n  psych,\n  tibble\n)\n\n# Load metrics_df (created in refined_framework page)\nmetrics_df &lt;- readRDS('data/metrics_df.rds')\n\n# Get skews of variables\nskewed &lt;- psych::describe(metrics_df[, -1]) %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column('variable_name') %&gt;% \n  dplyr::select(variable_name, skew) %&gt;% \n  dplyr::filter(abs(skew) &gt; 2) %&gt;% \n  pull(variable_name)\n\nplots &lt;- map(names(metrics_df)[-1], \\(var){\n  # color based on skewness\n  if (var %in% skewed) {\n    fill &lt;- 'red'\n    color &lt;- 'darkred'\n  } else {\n    fill &lt;- 'lightblue'\n    color &lt;- 'royalblue'\n  }\n  \n  # Make plot for variable\n  metrics_df %&gt;% \n    ggplot(aes(x = !!sym(var))) + \n    geom_density(\n      fill = fill,\n      color = color,\n      alpha = 0.5\n    ) +\n    theme_classic() +\n    theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n}) \n\n# Arrange them in 4 columns\nggarrange(\n  plotlist = plots,\n  ncol = 4,\n  nrow = 33\n)\n\n\n\n\n\nDistributions of metrics at the state level.\n\n\n\n\nIt seems most of our metrics fall along respectable somewhat-normal distributions. 29 metrics are skewed out of 130 total. They include several variables related to local farm economies (agrotourism sales as a percentage of total sales, direct to consumer sales as a percentage of total sales, and value added sales as a percentage of total sales), as well as a couple of the TreeMap 2016 variables (dead standing carbon and live trees) and GHG emissions from agriculture (CH4 and CO2, with an honorable mention for N2O). Just about the whole collection of ERS metrics are skewed, including imports and exports, indemnities, and real estate expenses.\nThere might be a case for transformations here, or it might make more sense to do it at the indicator level. Another option is to weight our metrics to take into account population, number of farms, acres of farmland, GDP, or some other appropriate variable for each metric. Béné et al. (2019) used Box Cox transformations for highly skewed indicators before normalizing all indicators with Min Max transformations.\n\n\n\n\n\n Back to topReferences\n\nBéné, Christophe, Steven D. Prager, Harold A. E. Achicanoy, Patricia Alvarez Toro, Lea Lamotte, Camila Bonilla, and Brendan R. Mapes. 2019. “Global Map and Indicators of Food System Sustainability.” Scientific Data 6 (1): 279. https://doi.org/10.1038/s41597-019-0301-5.",
    "crumbs": [
      "Analysis",
      "Metric Distributions"
    ]
  },
  {
    "objectID": "pages/refined_framework.html",
    "href": "pages/refined_framework.html",
    "title": "Refined Secondary Data Framework",
    "section": "",
    "text": "This page shows the partially refined framework as it stands after three dimension meetings: economics, environment, and production. It also includes a selection of preliminary secondary data metrics to match those indicators. Effectively, we have around 650 meaningful metrics. Here, we are using a selection of 130 of them to make a skeleton framework for preliminary analyses.\nNote that where I have no metrics to represent an indicator, I have added placeholders of the format NONE_#. This does not mean that secondary data do not exist, just that I either haven’t found it or haven’t cleaned and wrangled it yet. If you know of any secondary data to fill in the gaps or improve on data we already have, please do reach out to let Chris know about it.\nAt the bottom of this page is a metadata table with sources and definitions for all the metrics.",
    "crumbs": [
      "Analysis",
      "Refined Framework"
    ]
  },
  {
    "objectID": "pages/refined_framework.html#partially-refined-framework",
    "href": "pages/refined_framework.html#partially-refined-framework",
    "title": "Refined Secondary Data Framework",
    "section": "1 Partially Refined Framework",
    "text": "1 Partially Refined Framework\nHere is the framework with a selection of secondary metrics, split into each dimension for ease of reading.\n\n\nCode\npacman::p_load(\n  conflicted,\n  dplyr,\n  purrr,\n  stringr,\n  readr\n)\nsource('dev/get_dimension_ggraph.R')\n\n# Load refined framework\nsm_data &lt;- readRDS('data/sm_data.rds')\nraw_frame &lt;- sm_data[['refined_tree']]\n\n# Clean up the framework df \nframe &lt;- raw_frame %&gt;% \n  select(dimension:variable_name, use) %&gt;% \n  filter(use == 'x') %&gt;% \n  select(-use) %&gt;% \n  mutate(\n    metric = ifelse(\n      str_length(metric) &gt; 45,\n      paste0(str_sub(metric, end = 45), '...'),\n      metric\n    )\n  )\nget_str(frame)\n\n# Save frame to rds for use in subsequent scripts\nsaveRDS(frame, 'data/frame.rds')\n\n# Start a list to save outputs for preso\nplots &lt;- list()\n\n\n\n1.1 Environment\nWe have reasonable representation of the environment dimension, although some metrics are proxies that are stretched a bit too far. Some weak points are the carbon stocks indicator - so far, this is all from the TreeMap 2016 dataset, which has only been updated once since 2008. I would love to include other stocks of carbon if anyone has leads on datasets. The metrics for embodied carbon are also pretty big stretches.\nOne gap I’ve noticed since this dimension was reworked in the dimension meeting is that there is no direct treatment of soil health included anymore. We might add soil metrics for carbon stocks or forest health, but there is no clear home for it. And that being said, I have had no luck finding any reliable soil health datasets, so I’m all ears here too.\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nplots$environment &lt;- get_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'environment',\n  include_metrics = TRUE,\n  y_limits = c(-2, 3.25),\n  palette = \"ggthemes::stata_s2color\"\n)\nplots$environment\n\n\n\n\n\n\n\n\n\n\n\n1.2 Economics\nIt has definitely been easier to find economics data than other dimensions. Worth noting here is that the access to land indicator is not ideal. I’m using value and farm size as a proxy for access. Use of crop insurance is also a proxy, since I could not find direct insurance claim data from FSA. So for now, we are just using the ag secretary declarations of disasters that allow for insurance claims as a proxy. The failure rate of food businesses should be available in BLS or ERS I believe, but I haven’t gotten around to wrangling and including it.\n\n\nCode\nplots$economics &lt;- get_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'economics',\n  include_metrics = TRUE,\n  y_limits = c(-1.5, 3.1),\n  palette = \"ggthemes::stata_s2color\"\n)\nplots$economics\n\n\n\n\n\n\n\n\n\n\n\n1.3 Production\nAgricultural exports are a pretty robust dataset at the state level from ERS, although the import data only includes the values of the top five imports for each state - not ideal. Crop diversity is based on the Cropland Data Layer, a USDA NASS spatial model estimating of crop types, which I used to calculate Shannon diversity at the county and state level. It turns out that this one does a particularly poor job of representing Vermont, as it focuses on commodity crops.\nThe rest of the metrics come from NASS. Production is an area in which I feel better about using NASS data than some other dimensions, but there is still some risk of these data not representing the kind of diversified systems found in Vermont very well.\n\n\nCode\nplots$production &lt;- get_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'production',\n  include_metrics = TRUE,\n  y_limits = c(-1.75, 3),\n  palette = \"ggthemes::stata_s2color\"\n)\nplots$production\n\n\n\n\n\n\n\n\n\n\n\n1.4 Health\nThe Food Environment Atlas has lots of data on access and nutrition, which accounts for much of the food security data, along with NASS. I threw in a slew of metrics for physical health under the temporary indicator name ‘physical health tbd’ just to differentiate it from the index. I also have a handful of established composite indices for health, including the UW County Health Rankings metrics for health factors (behavior, clinical care, social and economic factors, physical environment) and health outcomes (length of life, quality of life), as well as some established food security indices that are not included in this framework. I will instead use them to compare to dimensions scores as external validation in the Validation section.\n\n\nCode\nplots$health &lt;- get_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'health',\n  include_metrics = TRUE,\n  y_limits = c(-1.7, 3),\n  palette = \"ggthemes::stata_s2color\"\n)\nplots$health\n\n\n\n\n\n\n\n\n\n\n\n1.5 Social\nThe social dimension is admittedly slim, but it could have been worse. The County Health Rankings dataset brings a few useful metrics here, like social associations and disconnected youth. Voter turnout is a proxy for participatory governance in food systems - I can’t imagine finding something much more specific than that at this point. I also need to replace mean producer age with a diversity index for producer age groups from NASS.\n\n\nCode\nplots$social &lt;- get_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'social',\n  include_metrics = TRUE,\n  y_limits = c(-1.7, 3),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n# Save list of plots for preso\nsaveRDS(plots, 'preso/plots/frameworks.rds')\n\nplots$social",
    "crumbs": [
      "Analysis",
      "Refined Framework"
    ]
  },
  {
    "objectID": "pages/refined_framework.html#metadata",
    "href": "pages/refined_framework.html#metadata",
    "title": "Refined Secondary Data Framework",
    "section": "2 Metadata",
    "text": "2 Metadata\nHere we pull out the set of 130 metrics from the larger collection and arrange them into a more functional, tidy dataframe:\n\n\nCode\n# Get latest year function\nsource('dev/data_pipeline_functions.R')\n\n# Load metrics data\nsm_data &lt;- readRDS('data/sm_data.rds')\nmetrics &lt;- sm_data$metrics\n\n# Load refined framework\nraw_tree &lt;- sm_data[['refined_tree']]\n\n# Load refined framework\nframe &lt;- readRDS('data/frame.rds')\n\n# Pull it from the actual metrics data\nmetrics &lt;- sm_data$metrics %&gt;% \n  dplyr::filter(\n    variable_name %in% frame$variable_name,\n    fips %in% sm_data$state_key$state_code\n  )\nget_str(metrics)\nlength(unique(metrics$variable_name))\n\n# Filter to latest year for each metric, and pivot wider\n# Also removing census participation - don't really have data at state level\n# Note to aggregate counties for this at some point\nmetrics_df &lt;- metrics %&gt;%\n  mutate(\n    value = ifelse(value == 'NaN', NA, value),\n    value = str_remove_all(value, ','),\n    value = as.numeric(value)\n  ) %&gt;%\n  get_latest_year() %&gt;% \n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;% \n  unnest(cols = !fips) %&gt;% \n  unique()\nget_str(metrics_df)\n\n# Let's get rid of the years so they are easier to work with\nnames(metrics_df) &lt;- str_split_i(names(metrics_df), '_', 1)\nget_str(metrics_df)\n\n# Also get rid of DC - too many missing values\nmetrics_df &lt;- metrics_df %&gt;% \n  dplyr::filter(fips != '11')\n\n# Save this for use in subsequent pages\nsaveRDS(metrics_df, 'data/metrics_df.rds')\n\n\nBelow, the metrics are displayed in a table that lets you browse and explore them.\n\n\nCode\n# Pull var names from metrics_df out of full metadata\nvars &lt;- unique(frame$variable_name) %&gt;% \n  str_subset('NONE', negate = TRUE)\n\n\n## Load metadata table, but keep framework from frame []\nmetadata &lt;- sm_data$metadata %&gt;% \n  select(-c(dimension, index, indicator)) %&gt;% \n  dplyr::filter(variable_name %in% vars)\n\n# Grab the framework variables from the frame to combine with metadata\nupdated_framework &lt;- frame %&gt;% \n  dplyr::select(variable_name, dimension, index, indicator)\n\n# Combine them\nmetadata &lt;- inner_join(metadata, updated_framework, by = 'variable_name')\n# get_str(metadata)\n\n\n## Pick out variables to display\nmetadata &lt;- metadata %&gt;% \n  select(\n    metric,\n    'Variable Name' = variable_name,\n    definition,\n    dimension,\n    index,\n    indicator,\n    units,\n    years = year,\n    'Year' = latest_year, # Renaming latest year as year, not including og year\n    source,\n    scope,\n    updates,\n    resolution,\n    url\n) %&gt;% \n  setNames(c(str_to_title(names(.))))\n\n# Save this for preso\nsaveRDS(metadata, 'preso/data/meta_for_table.rds')\n\n###\nhtmltools::browsable(\n  tagList(\n    \n    tags$div(\n      style = \"display: flex; gap: 16px; margin-bottom: 20px; justify-content: center;\",\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Show/hide more columns\"),\n        onclick = \"Reactable.setHiddenColumns('metadata_table', prevColumns =&gt; {\n          return prevColumns.length === 0 ? ['Definition', 'Scope', 'Resolution', 'Url'] : []\n        })\"\n      ),\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Download as CSV\"),\n        onclick = \"Reactable.downloadDataCSV('metadata_table', 'sustainability_metadata.csv')\"\n      )\n    ),\n    \n    reactable(\n      metadata[, which(names(metadata) != 'Years')],\n      sortable = TRUE,\n      resizable = TRUE,\n      filterable = TRUE,\n      searchable = TRUE,\n      pagination = TRUE,\n      bordered = TRUE,\n      wrap = TRUE,\n      rownames = FALSE,\n      onClick = 'select',\n      striped = TRUE,\n      pageSizeOptions = c(5, 10, 25, 50, 100),\n      defaultPageSize = 5,\n      showPageSizeOptions = TRUE,\n      highlight = TRUE,\n      style = list(fontSize = \"14px\"),\n      compact = TRUE,\n      fullWidth = TRUE,\n      columns = list(\n        Metric = colDef(\n          minWidth = 200,\n          sticky = 'left'\n        ),\n        'Variable Name' = colDef(\n          minWidth = 150\n        ),\n        Definition = colDef(\n          minWidth = 250\n        ),\n        'Latest Year' = colDef(minWidth = 75),\n        Source = colDef(minWidth = 250),\n        Scope = colDef(show = FALSE),\n        Resolution = colDef(show = FALSE),\n        Url = colDef(\n          minWidth = 300,\n          show = FALSE\n        )\n      ),\n      defaultColDef = colDef(minWidth = 100),\n      elementId = \"metadata_table\",\n      details = function(index) {\n        div(\n          style = \"padding: 15px; border: 1px solid #ddd; margin: 10px 0;\n             background-color: #ecf4ed; border-radius: 10px; border-color: black;\n             box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\",\n          \n          tags$h4(\n            strong(\"Details\"), \n          ),\n          tags$p(\n            strong('Metric Name: '), \n            as.character(metadata[index, 'Metric']),\n          ),\n          tags$p(\n            strong('Variable Name: '), \n            as.character(metadata[index, 'Variable Name']),\n          ),\n          tags$p(\n            strong('Definition: '), \n            as.character(metadata[index, 'Definition']),\n          ),\n          tags$p(\n            strong('Source: '), \n            as.character(metadata[index, 'Source'])\n          ),\n          tags$p(\n            strong('Latest Year: '), \n            as.character(metadata[index, 'Year'])\n          ),\n          tags$p(\n            strong('All Years (cleaned, wrangled, and included): '), \n            as.character(metadata[index, 'Years'])\n          ),\n          tags$p(\n            strong('Updates: '), \n            str_to_title(as.character(metadata[index, 'Updates']))\n          ),\n          tags$p(\n            strong('URL: '), \n            tags$a(\n              href = as.character(metadata[index, 'Url']),\n              target = '_blank',\n              as.character(metadata[index, 'Url'])\n            )\n          )\n        )\n      }\n    )\n  )\n)\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV",
    "crumbs": [
      "Analysis",
      "Refined Framework"
    ]
  },
  {
    "objectID": "pages/refine_economics.html",
    "href": "pages/refine_economics.html",
    "title": "Economic Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the economics dimensions. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting. A final set of indicators to incorporate into the next RFP is still in the works!",
    "crumbs": [
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refine_economics.html#indicator-progression",
    "href": "pages/refine_economics.html#indicator-progression",
    "title": "Economic Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework as described in the Wiltshire et al. paper.\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_wiltshire_tree.csv',\n  dimension_in = 'economics',\n  include_metrics = FALSE,\n  y_limits = c(-1.5, 2.1)\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_tree.csv',\n  dimension_in = 'economics',\n  y_limits = c(-1.5, 2.1)\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Refinement Meeting\nFinally, the tentative set of indicators following the indicator refinement meeting on November 15th, 2024\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_meeting_tree.csv',\n  dimension_in = 'economics',\n  y_limits = c(-1.5, 2.1)\n)",
    "crumbs": [
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refine_economics.html#survey",
    "href": "pages/refine_economics.html#survey",
    "title": "Economic Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the economic indicator refinement meeting on November 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/econ_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    starts_with('Q'),\n    -ends_with('RANK')\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    paste0('add_indi_', 1:3),\n    'notes',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not',\n    paste0('add_idx_', 1:3),\n    'idx_notes',\n    'final_notes'\n  )) %&gt;% \n  .[-c(1:2), ]\n\ngroups &lt;- select(dat, indi_must:indi_must_not, idx_must:idx_probably_not)\n\nto_df &lt;- function(x) {\n  x %&gt;% \n    str_split(',') %&gt;% \n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n    arrange(desc(freq))\n}\n\nindi_out &lt;- map(groups[1:4], to_df)\nidx_out &lt;- map(groups[5:7], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\n\n\nCode\nggplot(graph_table, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so “Must Include” is worth 3 points, “Probably Include” is worth 2 points, “Probably Not Include” is worth 1 point, and “Must Not Include” is worth 0 points. Note that the last column is the sum of proportions of “Must Include” and “Probably Include”. You can sort, search, expand, or page through the table below.\n\n\nCode\n# Add category to tables\nprops &lt;- ind_tables %&gt;% \n  imap(~ .x %&gt;% mutate(cat = .y)) %&gt;% \n  bind_rows() %&gt;% \n  select(-score)\n \n# Get proportion of probably include OR must include\nprop_prob_or_must_include &lt;- props %&gt;% \n  filter(cat %in% c('indi_must', 'indi_probably')) %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(prop_include = sum(freq) / 6) %&gt;% \n  arrange(desc(prop_include))\n\n# Get proportion of must include\nprop_must_include &lt;- props %&gt;% \n  filter(cat == 'indi_must') %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(prop_must = sum(freq) / 6) %&gt;% \n  arrange(desc(prop_must))\n\n# Add up weighted scores\nind_scores &lt;- ind_tables %&gt;% \n  bind_rows() %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(score = sum(score, na.rm = TRUE)) %&gt;% \n  arrange(desc(score))\n\n# Join everything together\nscores_table &lt;- ind_scores %&gt;% \n  full_join(prop_must_include) %&gt;% \n  full_join(prop_prob_or_must_include) %&gt;% \n  arrange(desc(score)) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    across(c(3:4), ~ format(round(.x, 2), nsmall = 2))\n  ) %&gt;% \n  setNames(c('Indicator', 'Score', 'Proportion Must Include', 'Proportion Must OR Probably Include'))\n\n\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\nidx_out &lt;- map(groups[5:7], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:1)\nidx_tables &lt;- map2(idx_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:probably_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')[2:4]\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 16),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\"\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nThe indices are going through the same treatment as indicators above - scored from 3 to 0. Note that there were no indices rated as “Must Not Include”.",
    "crumbs": [
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refine_production.html",
    "href": "pages/refine_production.html",
    "title": "Production Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the production dimension. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting.",
    "crumbs": [
      "Indicator Refinement",
      "Production"
    ]
  },
  {
    "objectID": "pages/refine_production.html#indicator-progression",
    "href": "pages/refine_production.html#indicator-progression",
    "title": "Production Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework for the dimension as described in the Wiltshire et al. paper.\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/wiltshire_tree.csv',\n  dimension_in = 'Production',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/matrix_tree.csv',\n  dimension_in = 'Production',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)",
    "crumbs": [
      "Indicator Refinement",
      "Production"
    ]
  },
  {
    "objectID": "pages/refine_production.html#survey",
    "href": "pages/refine_production.html#survey",
    "title": "Production Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the production indicator refinement meeting on January 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/prod_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    ends_with('GROUP'),\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not'\n  )) %&gt;% \n  .[-c(1:2), ]\n\nto_df &lt;- function(x) {\n  if (all(is.na(x))) {\n    return(NULL)\n  } else {\n   x %&gt;%\n    str_remove(' \\\\(joint indicator with Marketability\\\\)') %&gt;%\n    str_remove('\\\\*.*') %&gt;%\n    str_remove(' \\\\(see notes with questions') %&gt;%\n    str_split(',(?!\\\\s)') %&gt;% # Split on comma not followed by a space\n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n     arrange(desc(freq))\n  }\n}\n\nindi_out &lt;- map(dat[1:4], to_df)\nidx_out &lt;- map(dat[5:8], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n})\n\ngraph_table &lt;- graph_table %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n# Note some missing data throws off the graph table. Fix it here\ngraph_table_clean &lt;- graph_table %&gt;% \n  mutate(\n    sort_key = case_when(\n      str_detect(indicator, 'Production Species Diversity') ~ 3e6,\n      str_detect(indicator, 'Not livestock specific') ~ 1010002,\n      .default = sort_key\n    )\n  )\n\n\n\n\nCode\nggplot(graph_table_clean, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so “Must Include” is worth 3 points, “Probably Include” is worth 2 points, “Probably Not Include” is worth 1 point, and “Must Not Include” is worth 0 points. Note that the last column is the sum of proportions of “Must Include” and “Probably Include”. You can sort, search, expand, or page through the table below.\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\n# Add scores by multipliers\nmultipliers &lt;- c(3:1)\nidx_tables &lt;- map2(idx_out[1:3], multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    sort_key = ifelse(str_detect(index, 'Carbon'), 5e6, sort_key),\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:probably_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      # \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\"\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\"\n    )\n  )",
    "crumbs": [
      "Indicator Refinement",
      "Production"
    ]
  },
  {
    "objectID": "pages/sensitivity.html",
    "href": "pages/sensitivity.html",
    "title": "Sensitivity and Uncertainty",
    "section": "",
    "text": "Steps",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#sensitivity",
    "href": "pages/sensitivity.html#sensitivity",
    "title": "Sensitivity and Uncertainty",
    "section": "1.1 Sensitivity",
    "text": "1.1 Sensitivity\n\n\nCode\n## Load data for aggregations\nstate_key &lt;- readRDS('data/sm_data.rds')[['state_key']]\n# metrics_df &lt;- readRDS('data/metrics_df.rds')\nvalued &lt;- readRDS('data/valued_rescaled_metrics.rds')\nframework &lt;- readRDS('data/filtered_frame.rds')\n\n# Set up options for uncertain input factors\ntransformations &lt;- c('raw', 'boxcox', 'winsor')\nscalings &lt;- c('rank', 'minmax', 'zscore')\naggregations &lt;- c('geometric', 'arithmetic')\n\n# Get unique values of metrics and indicators. We will leave each one out once,\n# plus the 'none' value means we use them all (don't remove any)\nmetrics &lt;- c('none', framework$variable_name)\nlength(metrics) # 125 metrics (plus none is 126)\nindicators &lt;- c('none', unique(framework$indicator))\nlength(indicators) # 38 indicators (plus none is 39)\n\n# Create grid of combinations\ngrid &lt;- expand.grid(\n  transformations,\n  scalings,\n  aggregations,\n  indicators\n) %&gt;% \n  setNames(c('transformation', 'scaling', 'aggregation', 'indicator'))\nget_str(grid)\n\n\n\n\nCode\n# For each set of input factors, get dimension scores for VT\n# tic()\n# plan(multisession, workers = parallelly::availableCores(omit = 1))\n# \n# out &lt;- future_map(1:nrow(grid), \\(i) {\n# \n#   # Print outputs for debugging\n#   print(grid[i, ])\n#   get_time(c('\\n~~~~~ Starting row ', i, ' at:'))\n#   \n#   # Get iteration specs to pull df from valued data\n#   iter_specs &lt;- paste0(\n#     as.character(grid[i, 1]),\n#     '_',\n#     as.character(grid[i, 2])\n#   )\n#    \n#   # Run model\n#   model_out &lt;- get_all_aggregations(\n#     normed_data = valued[iter_specs],\n#     framework = framework,\n#     state_key = state_key,\n#     aggregation = grid$aggregation[i],\n#     remove_indicators = grid$indicator[i]\n#   )\n#   return(model_out)\n# }, .progress = TRUE)\n# \n# plan(sequential)\n# toc()\n# # 168s in parallel for 702 iterations\n# \n# get_str(out)\n# get_str(out, 4)\n# \n# # Save this so we don't have to run it again\n# saveRDS(out, 'data/objects/sensitivity_out.RDS')\n\n\n\n\nCode\n# Load up saved sensitivity out\nsens_out &lt;- readRDS('data/objects/sensitivity_out.RDS')\n\n## Get names of sampled inputs to identify later\nsampled_inputs &lt;- map_chr(1:nrow(grid), ~ {\n  paste0(\n    c(\n      as.character(grid[.x, 1]), \n      as.character(grid[.x, 2]), \n      str_sub(grid[.x, 3], end = 3),\n      snakecase::to_snake_case(as.character(grid[.x, 4]))\n    ), \n    collapse = '_'\n  )\n})\n\n# Pull out just VT dimension scores\ndimension_df &lt;- map_dfr(sens_out, ~ {\n  df &lt;- .x[[1]]$dimension_scores %&gt;% \n    as.data.frame() %&gt;% \n    filter(str_length(state) == 2) %&gt;% \n    mutate(\n      across(\n        !state, ~ as.numeric(dense_rank(.x)), \n        .names = \"{.col}_rank\"\n      )) %&gt;% \n    filter(state == 'VT') %&gt;% \n    select(matches(regex('_')))\n}) %&gt;% \n  mutate(inputs = sampled_inputs)\nget_str(dimension_df, 3)\n# Each line is result from one sample.\n\n# Get vector of dimension rank names\ndimension_ranks &lt;- str_subset(names(dimension_df),  '_') %&gt;% \n  str_remove('_rank') %&gt;% \n  snakecase::to_title_case()\n\n\n\n\nCode\nplots &lt;- map2(str_subset(names(dimension_df), '_'), dimension_ranks, ~ {\n  dimension_df %&gt;% \n    ggplot(aes(x = !!sym(.x))) + \n    geom_density(\n      fill = 'lightblue',\n      color = 'royalblue',\n      adjust = 3\n    ) +\n    theme_classic() +\n    labs(\n      x = paste(.y, 'Rank'),\n      y = 'Density',\n      title = .y\n    ) +\n    xlim(0, 50) +\n    ylim(0, 0.75)\n}) %&gt;% \n  setNames(c(stringr::str_to_lower(dimension_ranks)))\n\n# Save these plots for presentation\nsaveRDS(plots, 'preso/plots/dimension_sensitivity_plots.rds')\n\n# Arrange into a single diagram\nplot &lt;- ggarrange(\n  plotlist = plots,\n  nrow = 5,\n  ncol = 1\n)\nannotate_figure(\n  plot,\n  top = text_grob(\n    'Distributions of VT Dimension Ranks (Higher is Better)',\n    size = 14,\n    hjust = 0.5\n  )\n)",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#cronbach",
    "href": "pages/sensitivity.html#cronbach",
    "title": "Sensitivity and Uncertainty",
    "section": "1.2 Cronbach",
    "text": "1.2 Cronbach\nTake valued scaled (but not aggregated) data and see what Cronbach looks like at index level, and possibly dimension level.\n\n\nCode\nvalued &lt;- readRDS('data/valued_rescaled_metrics.rds')\nframework &lt;- readRDS('data/filtered_frame.rds')\n\n# Do this with filtered framework of 125 metrics, using minmax\nminmax_dat &lt;- valued$raw_minmax %&gt;%\n  select(all_of(unique(framework$variable_name)))\nget_str(minmax_dat)\n\n# For each dimension, get cronbach for all metrics within it\ndimensions &lt;- unique(framework$dimension)\ncronbachs &lt;- map(dimensions, ~ {\n  dim_metrics &lt;- framework %&gt;% \n    dplyr::filter(dimension == .x) %&gt;% \n    pull(variable_name) %&gt;% \n    unique()\n  cronbach &lt;- minmax_dat %&gt;% \n    select(all_of(dim_metrics)) %&gt;% \n    psych::alpha(check.keys = FALSE)\n  return(cronbach)\n}) %&gt;% \n  setNames(c(dimensions))\ncronbachs\n(cronbach_alphas &lt;- map(cronbachs, ~ .x$total$raw_alpha))\n\n# Save this for presentation\n# First make it a nice table\nout &lt;- cronbach_alphas %&gt;% \n  as.data.frame() %&gt;% \n  t() %&gt;% \n  as.data.frame() %&gt;% \n  tibble::rownames_to_column() %&gt;% \n  setNames(c('Dimension', 'Alpha')) %&gt;% \n  mutate(Alpha = round(Alpha, 3))\nout\nsaveRDS(out, 'preso/data/cronbach_dimensions.rds')\n\n\n\n\nCode\nget_reactable(\n  out,\n  fullWidth = FALSE,\n  searchable = FALSE,\n  filterable = FALSE\n)",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#economics",
    "href": "pages/sensitivity.html#economics",
    "title": "Sensitivity and Uncertainty",
    "section": "2.1 Economics",
    "text": "2.1 Economics",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#environment",
    "href": "pages/sensitivity.html#environment",
    "title": "Sensitivity and Uncertainty",
    "section": "2.2 Environment",
    "text": "2.2 Environment",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#health",
    "href": "pages/sensitivity.html#health",
    "title": "Sensitivity and Uncertainty",
    "section": "2.3 Health",
    "text": "2.3 Health",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#production",
    "href": "pages/sensitivity.html#production",
    "title": "Sensitivity and Uncertainty",
    "section": "2.4 Production",
    "text": "2.4 Production",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#social",
    "href": "pages/sensitivity.html#social",
    "title": "Sensitivity and Uncertainty",
    "section": "2.5 Social",
    "text": "2.5 Social",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/validation.html",
    "href": "pages/validation.html",
    "title": "Validation",
    "section": "",
    "text": "Here we will use the raw + min max + geometric aggregation scores and see how they hold up to validation by external variables and by PCA.\nExternal variables:\n\nFood Security Index (Feeding America, Map the Meal Gap)\nLife expectancy, or premature age-adjusted mortality (UW County Health Rankings)\nFood Environment Index (UW County Health Rankings)\nHappiness Score (WalletHub - if anyone knows of a better metric for this, I’m all ears)\n\n\n\nCode\n# Load sm_data\nsm_data &lt;- readRDS('data/sm_data.rds')\n\n# Load state fips key to join other datasets\nstate_key &lt;- sm_data[['state_key']] %&gt;% \n  select(state, state_code)\n\n# Load cleaned aggregated data for all levels of regresion\nraw_minmax_geo &lt;- readRDS('data/raw_minmax_geo.rds')\nget_str(raw_minmax_geo)\n\n# Reduce to just dimension scores, and remove prefix\ndimension_scores &lt;- raw_minmax_geo %&gt;% \n  select(state, starts_with('dimen')) %&gt;% \n  setNames(c(str_remove(names(.), 'dimen_')))\nget_str(dimension_scores)\n\n# Pull validation variables out of sm_data, wrangle them to match metrics_df\n# Also including covariates, gdp and population\nvalidation_vars &lt;- sm_data$metadata %&gt;% \n  select(variable_name, metric, definition, source) %&gt;% \n  filter(variable_name %in% c(\n    'foodInsecurity',\n    'communityEnvRank',\n    'happinessScore',\n    'wellbeingRank',\n    'workEnvRank',\n    'foodEnvironmentIndex',\n    'lifeExpectancy',\n    'population',\n    'gdpCurrent'\n  )) %&gt;% \n  pull(variable_name)\nvalidation_vars  \n \n# Get subset of metrics for our validation variables, get latest year only\nvalidation_metrics &lt;- sm_data$metrics %&gt;% \n  filter(\n    variable_name %in% validation_vars, \n    !is.na(value), \n    str_length(fips) == 2\n  ) %&gt;% \n  get_latest_year()\nget_str(validation_metrics)\n# All are available in 2024\n\n# Pivot wider, also get rid of trailing year\nvalidation_metrics &lt;- validation_metrics %&gt;% \n  pivot_wider(\n    id_cols = fips,\n    names_from = variable_name,\n    values_from = value\n  ) %&gt;% \n  setNames(c(str_remove(names(.), '_[0-9]{4}'))) %&gt;% \n  mutate(across(!fips, as.numeric))\nget_str(validation_metrics)\n# 00 US is missing a lot obviously\n# 11 DC is the other one with missing data\n# We will just filter down to 50 states to match metrics_df\n\n# Combine validation variables with our dimension scores using state key as the \n# bridge. Also remove DC (don't have validation metrics there)\nkey &lt;- sm_data$state_key %&gt;% \n  select(state, fips = state_code)\ndat &lt;- dimension_scores %&gt;% \n  left_join(key) %&gt;% \n  left_join(validation_metrics) %&gt;% \n  as.data.frame() %&gt;% \n  filter(state != 'DC') %&gt;% \n  select(-fips)\n\n# Make a GDP per capita variable from GDP real and population\n# It was already in millions to begin with\ndat &lt;- dat %&gt;% \n  mutate(gdp_per_cap = ((gdpCurrent / population) * 1e6) / 1000)\nget_str(dat)\n\n# Check it out\nget_str(dat)\nskimr::skim(dat)\n# Looks good\n\n# Save this for other pages\nsaveRDS(dat, 'data/metrics_df_with_vals_and_covars.rds')",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/validation.html#food-insecurity",
    "href": "pages/validation.html#food-insecurity",
    "title": "Validation",
    "section": "2.1 Food Insecurity",
    "text": "2.1 Food Insecurity\n\n\nCode\nlm1 &lt;- lm(\n  foodInsecurity ~ economics + environment + health + production + social,\n  data = dat\n)\n\n\n\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com % Date and time: Tue, Mar 18, 2025 - 6:43:21 PM\n\n\n\nCode\ncheck_model(lm1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlmtest::bptest(lm1)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm1\nBP = 12.068, df = 5, p-value = 0.03387",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/validation.html#life-expectancy",
    "href": "pages/validation.html#life-expectancy",
    "title": "Validation",
    "section": "2.2 Life Expectancy",
    "text": "2.2 Life Expectancy\n\n\nCode\nlm2 &lt;- lm(\n  lifeExpectancy ~ economics + environment + health + production + social,\n  data = dat\n)\n\n\n\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com % Date and time: Tue, Mar 18, 2025 - 6:43:23 PM\n\n\n\nCode\ncheck_model(lm2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlmtest::bptest(lm2)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm2\nBP = 4.9045, df = 5, p-value = 0.4276\n\n\nCode\nlife_exp_vcov &lt;- vcovHC(lm2, type = 'HC3')\n\n\n\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com % Date and time: Tue, Mar 18, 2025 - 6:43:25 PM",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/validation.html#food-environment-index",
    "href": "pages/validation.html#food-environment-index",
    "title": "Validation",
    "section": "2.3 Food Environment Index",
    "text": "2.3 Food Environment Index\n\n\nCode\nlm3 &lt;- lm(\n  foodEnvironmentIndex ~ economics + environment + health + production + social,\n  data = dat\n)\n\n\n\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com % Date and time: Tue, Mar 18, 2025 - 6:43:26 PM\n\n\n\nCode\nbptest(lm3)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm3\nBP = 13.607, df = 5, p-value = 0.01831\n\n\nCode\ncheck_model(lm3)\n\n\n\n\n\n\n\n\n\nThe Food Environment Index Regression does not show heteroskedasticity, but may well have some non-linear relationships given the residual plots. Health and economics are significant predictors, with a pretty healthy \\(R^2\\).\nLet’s try this one again with a random forest instead of linear model:\n\n\nCode\n# Get a version of dat without irrelevant variables\ndat_ml &lt;- dat %&gt;% \n  select(\n    economics, \n    environment, \n    health, \n    production, \n    social, \n    foodEnvironmentIndex,\n    gdp_per_cap\n  )\n\n# Split data 60/40\nset.seed(42)\nindices &lt;- createDataPartition(dat_ml$foodEnvironmentIndex, p = 0.60, list = FALSE)\ntraining_data &lt;- dat_ml[indices, ]\ntesting_data &lt;- dat_ml[-indices,]\n\nmy_folds &lt;- createFolds(training_data$foodEnvironmentIndex, k = 5, list = TRUE)\n\n# Control\nmy_control &lt;- trainControl(\n  method = 'cv',\n  number = 5,\n  verboseIter = TRUE,\n  index = my_folds\n)\n\n# Check for zero variance or near zero variance indicators\nnearZeroVar(dat, names = TRUE, saveMetrics = TRUE)\n# All clear\n\n# Also let's start a list with other results for preso\n# hyperparameters, etc\nml_out &lt;- list()\n\n\n\n2.3.1 GLMnet\n\n\nCode\nset.seed(42)\nfood_env_glmnet &lt;- train(\n  foodEnvironmentIndex ~ economics + environment + health + production + social + gdp_per_cap,\n  data = training_data, \n  tuneGrid = expand.grid(\n    alpha = seq(0.1, 1, length = 5),\n    lambda = seq(0.0001, 0.1, length = 100)\n  ),\n  method = \"glmnet\",\n  trControl = my_control,\n  preProcess = c('zv', 'center', 'scale')\n)\nget_str(food_env_glmnet)\n\n# Pull out best tune\nml_out$glmnet_best_tune &lt;- food_env_glmnet$bestTune\n\n\n\n\nCode\nimportance &lt;- varImp(food_env_glmnet, scale = TRUE)\n\n# Save for preso \nsaveRDS(importance, 'preso/plots/val3_food_env_glmnet_importance.rds')\n\n#\npred &lt;- predict(food_env_glmnet, testing_data)\nml_out$glmnet_performance &lt;- postResample(\n  pred = pred, \n  obs = testing_data$foodEnvironmentIndex\n) %&gt;% \n  round(3)\n# ml_out$glmnet_performance\n\nml_out$glmnet_imp_plot &lt;- importance %&gt;% \n  ggplot(aes(x = Overall, y = rownames(.))) +\n  geom_col(\n    color = 'royalblue',\n    fill = 'lightblue'\n  ) +\n  theme_classic() \n\nplot(importance)\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Random Forest\n\n\nCode\nset.seed(42)\nfood_env_rf &lt;- train(\n  foodEnvironmentIndex ~ production + social + health + economics + environment + gdp_per_cap,\n  data = training_data, \n  tuneLength = 7,\n  method = \"ranger\",\n  trControl = my_control,\n  importance = 'impurity'\n)\n\nget_str(food_env_rf)\n\n# Pull out best tune\nml_out$rf_best_tune &lt;- food_env_rf$bestTune\n\n\nOOB prediction error (MSE): 0.6360507\n\n\nCode\nimportance &lt;- varImp(food_env_rf, scale = TRUE)\n\n# Save for preso\nsaveRDS(importance, 'preso/plots/val3_rf_importance.rds')\n\n# Get RMSEA and stuff\npred &lt;- predict(food_env_rf, testing_data)\nml_out$rf_performance &lt;- postResample(\n  pred = pred, \n  obs = testing_data$foodEnvironmentIndex\n) %&gt;% \n  round(3)\n# ml_out$rf_performance\n\nimp &lt;- importance$importance %&gt;% \n  as.data.frame() %&gt;% \n  tibble::rownames_to_column() %&gt;% \n  setNames(c('Feature', 'Importance')) %&gt;% \n  mutate(Importance = round(Importance, 2)) %&gt;% \n  arrange(desc(Importance)) %&gt;% \n  mutate(\n    Feature = Feature %&gt;% \n      str_to_title() %&gt;% \n      str_replace('Gdp_per_cap', 'GDP per capita')\n  )\n  \n\nml_out$rf_imp_plot &lt;- imp %&gt;% \n  ggplot(aes(\n    x = Importance, \n    y = reorder(Feature, Importance),\n    text = paste0(\n      '&lt;b&gt;Variable:&lt;/b&gt; ', Feature, '\\n',\n      '&lt;b&gt;Importance:&lt;/b&gt; ', Importance\n    )\n  )) +\n  geom_col(\n    color = 'royalblue',\n    fill = 'lightblue',\n  ) +\n  theme_classic() +\n  labs(\n    x = 'Importance',\n    y = 'Feature'\n  )\n\n# Save all results for preso\nsaveRDS(ml_out, 'preso/data/ml_out.rds')\n\n# Show plot\nml_out$rf_imp_plot",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/validation.html#happiness-score",
    "href": "pages/validation.html#happiness-score",
    "title": "Validation",
    "section": "2.4 Happiness Score",
    "text": "2.4 Happiness Score\n\n\nCode\nlm4 &lt;- lm(\n  happinessScore ~ economics + environment + health + production + social,\n  data = dat\n)\n\n\n\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com % Date and time: Tue, Mar 18, 2025 - 6:43:35 PM\n\n\n\nCode\ncheck_model(lm4)",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/validation.html#component-extraction",
    "href": "pages/validation.html#component-extraction",
    "title": "Validation",
    "section": "3.1 Component Extraction",
    "text": "3.1 Component Extraction\n\n\nCode\nraw_minmax_geo &lt;- readRDS('data/raw_minmax_geo.rds')\nframework &lt;- readRDS('data/filtered_frame.rds')\n\n# Filter down to just indicators for PCA\npca_dat &lt;- raw_minmax_geo %&gt;% \n  select(starts_with('indic')) %&gt;% \n  setNames(c(str_remove(names(.), 'indic_'))) %&gt;% \n  as.data.frame()\n# get_str(pca_dat)\n\n# Explore how many factors to extract\nVSS(pca_dat, n = 8, fm = 'pc', rotate = 'Promax')\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.53  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.75  with  3  factors\n\nThe Velicer MAP achieves a minimum of 0.04  with  6  factors \n\n\n\nBIC achieves a minimum of  Inf  with    factors\n\n\nSample Size adjusted BIC achieves a minimum of  Inf  with    factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq prob sqresid  fit RMSEA BIC SABIC complex eChisq\n1 0.42 0.00 0.069   0    NA   NA    85.4 0.42    NA  NA    NA      NA     NA\n2 0.53 0.67 0.059   0    NA   NA    48.6 0.67    NA  NA    NA      NA     NA\n3 0.53 0.75 0.055   0    NA   NA    30.7 0.79    NA  NA    NA      NA     NA\n4 0.53 0.75 0.046   0    NA   NA    19.6 0.87    NA  NA    NA      NA     NA\n5 0.47 0.73 0.039   0    NA   NA    13.2 0.91    NA  NA    NA      NA     NA\n6 0.48 0.73 0.039   0    NA   NA    10.5 0.93    NA  NA    NA      NA     NA\n7 0.46 0.72 0.041   0    NA   NA     8.5 0.94    NA  NA    NA      NA     NA\n8 0.47 0.68 0.040   0    NA   NA     6.5 0.96    NA  NA    NA      NA     NA\n  SRMR eCRMS eBIC\n1   NA    NA   NA\n2   NA    NA   NA\n3   NA    NA   NA\n4   NA    NA   NA\n5   NA    NA   NA\n6   NA    NA   NA\n7   NA    NA   NA\n8   NA    NA   NA\n\n\nCode\nset.seed(42)\nfa.parallel(pca_dat, fm = 'ml')\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  5 \n\n\nMAP suggests 6, VSS 2 or 3, PA suggests 5. Not half bad. I think we are justified to go with 5.\n\n\nCode\n# Oblique rotations: promax, oblimin, simplimax, cluster\nrotations &lt;- c(\n  'Promax',\n  'oblimin',\n  'simplimax',\n  'cluster'\n)\npca_outs &lt;- map(rotations, ~ {\n  pca_dat %&gt;% \n    # scale() %&gt;% \n    pca(nfactors = 5, rotate = .x)\n}) %&gt;% \n  setNames(c(rotations))\n\n# Save a version of promax for preso?\npng(\n  filename = 'preso/plots/scree.png',\n  width = 800,\n  height = 600,\n  units = 'px',\n  res = 150\n)\nplot(\n  pca_outs$simplimax$values,\n  xlab = 'Number of Components',\n  ylab = 'Eigen Values'\n)\nabline(h = 1)\ndev.off()\n\n\npng \n  2 \n\n\nCode\n# Now actually show it \nplot(\n  pca_outs$simplimax$values,\n  xlab = 'Number of Components',\n  ylab = 'Eigen Values'\n)\nabline(h = 1)\n\n\n\n\n\n\n\n\n\nThe scree plot makes a reasonably convincing case for 6 components, as the slope falls off substantially after the fifth.",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/validation.html#run-pca",
    "href": "pages/validation.html#run-pca",
    "title": "Validation",
    "section": "3.2 Run PCA",
    "text": "3.2 Run PCA\n\n\nCode\npca_tables &lt;- map(pca_outs, ~ {\n  .x$loadings %&gt;% \n    unclass() %&gt;% \n    as.data.frame() %&gt;% \n    select(order(colnames(.))) %&gt;%\n    mutate(\n      across(everything(), ~ round(.x, 3)),\n      across(everything(), ~ case_when(\n        .x &lt; 0.20 ~ '',\n        .x &gt;= 0.20 & .x &lt; 0.32 ~ '.',\n        .x &gt;= 0.32 ~ as.character(.x),\n        .default = NA\n      ))\n    ) %&gt;% \n    rownames_to_column() %&gt;% \n    rename(indicator = 1) %&gt;% \n    mutate(\n      dimension = framework$dimension[match(indicator, framework$indicator)]\n    ) %&gt;% \n    select(indicator, dimension, everything())\n})\nget_str(pca_tables)\n\n# Save it for preso\nsaveRDS(pca_tables, 'preso/data/pca_tables.rds')\n\n\n\n\nCode\nget_reactable(pca_tables$simplimax)",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  }
]