[
  {
    "objectID": "pages/validation.html",
    "href": "pages/validation.html",
    "title": "Validation",
    "section": "",
    "text": "The goal here is to use our five tentative dimension scores as predictors to compare against other established metrics:\n\nFood security index, overall and/or child (Feeding America, Map the Meal Gap)\nHealth outcomes (UW county health rankings)\nLife expectancy, or premature age-adjusted mortality (UW rankings)\nOther ideas: a food affordability index, happiness index, happy planet index?\n\nTo Add:\n\ncite Schneider 2023 (Schneider et al. 2023)\n\nWLS regression to get deviations of region and income group weighted means from global weighted mean\n\n\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  stringr,\n  tidyr\n)\n\n# Load sm_data\nsm_data &lt;- readRDS('data/sm_data.rds')\n\n# Load state fips key to join other datasets\nstate_key &lt;- sm_data[['state_key']] %&gt;% \n  select(state, state_code)\n\n# Load cleaned aggregated data for all levels of regresion\nminmax_geo &lt;- readRDS('data/minmax_geo_all_levels.rds')\nget_str(minmax_geo)\n\n# Reduce to just dimension scores, and remove prefix\ndimension_scores &lt;- minmax_geo %&gt;% \n  select(state, starts_with('dimen')) %&gt;% \n  setNames(c(str_remove(names(.), 'dimen_')))\nget_str(dimension_scores)\n\n# Pull raw metrics data\nmetrics_df &lt;- readRDS('data/metrics_df.rds')\nget_str(metrics_df)\n\n\n# Pull validation variables out of sm_data, wrangle them to match metrics_df\n# Also including covariates, gdp and population\nvalidation_vars &lt;- sm_data$metadata %&gt;% \n  select(variable_name, metric, definition, source) %&gt;% \n  filter(variable_name %in% c(\n    'foodInsecurity',\n    'communityEnvRank',\n    'happinessScore',\n    'wellbeingRank',\n    'workEnvRank',\n    'foodEnvironmentIndex',\n    'lifeExpectancy',\n    'population',\n    'gdpCurrent'\n  )) %&gt;% \n  pull(variable_name)\nvalidation_vars  \n \n# Get subset of metrics for our validation variables, get latest year only\nvalidation_metrics &lt;- sm_data$metrics %&gt;% \n  filter(\n    variable_name %in% validation_vars, \n    !is.na(value), \n    str_length(fips) == 2\n  ) %&gt;% \n  get_latest_year()\nget_str(validation_metrics)\n# All are available in 2024\n\n# Pivot wider, also get rid of trailing year\nvalidation_metrics &lt;- validation_metrics %&gt;% \n  pivot_wider(\n    id_cols = fips,\n    names_from = variable_name,\n    values_from = value\n  ) %&gt;% \n  setNames(c(str_remove(names(.), '_[0-9]{4}'))) %&gt;% \n  mutate(across(!fips, as.numeric))\nget_str(validation_metrics)\n# 00 US is missing a lot obviously\n# 11 DC is the other one with missing data\n# We will just filter down to 50 states to match metrics_df\n\n# Combine validation variables with our dimension scores using state key as the \n# bridge. Also remove DC (don't have validation metrics there)\nkey &lt;- sm_data$state_key %&gt;% \n  select(state, fips = state_code)\ndat &lt;- dimension_scores %&gt;% \n  left_join(key) %&gt;% \n  left_join(validation_metrics) %&gt;% \n  as.data.frame() %&gt;% \n  filter(state != 'DC') %&gt;% \n  select(-fips)\n\n# Make a GDP per capita variable from GDP real and population\n# It was already in millions to begin with\ndat &lt;- dat %&gt;% \n  mutate(gdp_per_cap = ((gdpCurrent / population) * 1e6) / 1000)\nget_str(dat)\n\n# Check it out\nget_str(dat)\nskimr::skim(dat)\n# Looks good\n\n# Save this for other pages\nsaveRDS(dat, 'data/metrics_df_with_vals_and_covars.rds')",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/validation.html#glmnet",
    "href": "pages/validation.html#glmnet",
    "title": "Validation",
    "section": "4.1 GLMnet",
    "text": "4.1 GLMnet\n\n\nCode\nset.seed(42)\nfood_env_glmnet &lt;- train(\n  foodEnvironmentIndex ~ economics + environment + health + production + social + gdp_per_cap,\n  data = training_data, \n  tuneGrid = expand.grid(\n    alpha = seq(0.1, 1, length = 5),\n    lambda = seq(0.0001, 0.1, length = 100)\n  ),\n  method = \"glmnet\",\n  trControl = my_control,\n  preProcess = c('zv', 'center', 'scale')\n)\n\n\n\n\nCode\nimportance &lt;- varImp(food_env_glmnet, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nIt looks like GDP is a better predictor of the Food Environment Index than anything else, but the health dimension is far more influential than any other.",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/validation.html#random-forest",
    "href": "pages/validation.html#random-forest",
    "title": "Validation",
    "section": "4.2 Random Forest",
    "text": "4.2 Random Forest\n\n\nCode\nset.seed(42)\nfood_env_rf &lt;- train(\n  foodEnvironmentIndex ~ production + social + health + economics + environment + gdp_per_cap,\n  data = training_data, \n  tuneLength = 7,\n  method = \"ranger\",\n  trControl = my_control,\n  importance = 'impurity'\n)\n\n\nOOB prediction error (MSE): 1.2917313\n\n\nCode\nimportance &lt;- varImp(food_env_rf, scale = TRUE)\nplot(importance)",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "temp/test_preso.html#getting-up",
    "href": "temp/test_preso.html#getting-up",
    "title": "Habits",
    "section": "",
    "text": "Turn off alarm\nGet out of bed"
  },
  {
    "objectID": "temp/test_preso.html#going-to-sleep",
    "href": "temp/test_preso.html#going-to-sleep",
    "title": "Habits",
    "section": "2 Going to sleep",
    "text": "2 Going to sleep\n\nGet in bed\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "temp/test_preso.html#testing-a-plot",
    "href": "temp/test_preso.html#testing-a-plot",
    "title": "Habits",
    "section": "3 Testing a plot",
    "text": "3 Testing a plot\n\n\n\nSome points before plot\nanother point before the plot\n\n\n\n\nCode\n# These fig height and width options work\nplot(mtcars$hp, mtcars$mpg)"
  },
  {
    "objectID": "pages/refined_framework.html",
    "href": "pages/refined_framework.html",
    "title": "Refined Secondary Data Framework",
    "section": "",
    "text": "*Setup complete*\nThis page shows the partially refined framework as it stands after three dimension meetings: economics, environment, and production. It also includes a selection of preliminary secondary data metrics to match those indicators. We have collected around 1500 metrics so far, although many of those are fluff. Effectively, we have around 600 meaningful metrics. Here, we are using a selection of ~129 of them to make a preliminary framework for preliminary analyses. This is more than we have been planning for the refined framework, which will give us a chance to see how aggregate scores change with different combinations of metrics and under different methods of aggregation.\nNote that where I have no metrics to represent an indicator, I have added placeholders of the format NONE_#. This does not mean that secondary data do not exist, just that I either haven’t found it or haven’t cleaned and wrangled it yet. At the time of writing, there are several of the latter, including crop failure, access to care, racial diversity, and others. If you know of any secondary data to fill in the gaps or improve on data we already have, please do reach out to let Chris know about it.\nAt the bottom of this page is a metadata table with sources and definitions for all the metrics.",
    "crumbs": [
      "Analysis",
      "Refined Framework"
    ]
  },
  {
    "objectID": "pages/refined_framework.html#partially-refined-framework",
    "href": "pages/refined_framework.html#partially-refined-framework",
    "title": "Refined Secondary Data Framework",
    "section": "1 Partially Refined Framework",
    "text": "1 Partially Refined Framework\nHere is the framework with a selection of secondary metrics, split into each dimension for ease of reading.\n\n\nCode\npacman::p_load(\n  conflicted,\n  dplyr,\n  purrr,\n  stringr,\n  readr\n)\nsource('dev/get_dimension_ggraph.R')\n\n# Load refined framework\nsm_data &lt;- readRDS('data/sm_data.rds')\nraw_frame &lt;- sm_data[['refined_tree']]\n\n# Clean up the framework df \nframe &lt;- raw_frame %&gt;% \n  select(dimension:variable_name, use) %&gt;% \n  filter(use == 'x') %&gt;% \n  select(-use) %&gt;% \n  mutate(\n    metric = ifelse(\n      str_length(metric) &gt; 45,\n      paste0(str_sub(metric, end = 45), '...'),\n      metric\n    )\n  )\n# get_str(frame)\n\n# Save frame to rds for use in subsequent scripts\nsaveRDS(frame, 'data/frame.rds')\n\n\n\n1.1 Environment\nWe have reasonable representation of the environment dimension, although some metrics are proxies that are stretched a bit too far. I do have biodiversity and sensitive habitat data, but still need to process it at the state level and add it to the collection. Some weak points are the carbon stocks indicator - so far, this is all from the TreeMap 2016 dataset. I would love to include other stocks of carbon if anyone has leads on datasets. The metrics for embodied carbon are also stretches.\nOne gap I’ve noticed since this dimension was reworked in the dimension meeting is that there is no direct treatment of soil health included anymore. We might add soil metrics for carbon stocks or forest health, but there is no clear home for it. And that being said, I have had no luck finding any reliable soil health datasets, so I’m all ears here too.\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'environment',\n  include_metrics = TRUE,\n  y_limits = c(-2, 3.25),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Economics\nIt has definitely been easier to find economics data than other dimensions. Worth noting here is that the access to land indicator is not ideal. I’m using value and farm size as a proxy for access. Use of crop insurance is also a proxy, since I could not find direct insurance claim data from FSA. So for now, we are just using the ag secretary declarations of disasters that allow for insurance claims as a proxy.\n\n\nCode\nget_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'economics',\n  include_metrics = TRUE,\n  y_limits = c(-1.5, 3.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Production\nAgricultural exports are a pretty robust dataset at the state level from ERS, although the import data only includes the values of the top five imports for each state - not ideal. Crop diversity is based on the Cropland Data Layer, a USDA NASS spatial model estimating of crop types, which I used to calculate Shannon diversity at the county and state level. The rest of the metrics come from NASS. Production is an area in which I feel better about using NASS data than some other dimensions, but there is still some risk of these data not representing VT farms appropriately.\n\n\nCode\nget_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'production',\n  include_metrics = TRUE,\n  y_limits = c(-1.75, 3),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.4 Health\nThe Food Environment Atlas has lots of data on access and nutrition, which accounts for much of the food security data, along with NASS. I threw in a slew of metrics for physical health under the temporary indicator name ‘physical health tbd’ just to differentiate it from the index. I also have a handful of established composite indices for health, including the UW County Health Rankings metrics for health factors (behavior, clinical care, social and economic factors, physical environment) and health outcomes (length of life, quality of life), as well as some established food security indices that are not included in this framework. I will instead use them to compare to dimensions scores as external validation in the Validation section.\n\n\nCode\nget_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'health',\n  include_metrics = TRUE,\n  y_limits = c(-1.7, 3),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.5 Social\nThe social dimension is admittedly slim, but it could have been a lot worse. The County Health Rankings dataset brings a few useful metrics here, like social associations and disconnected youth. Census participation and voter turnout are proxies for participatory governance in food systems - I can’t imagine finding something much more specific than that at this point. I also plan on replacing mean producer age with a diversity index for age structure among producers.\n\n\nCode\nget_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'social',\n  include_metrics = TRUE,\n  y_limits = c(-1.7, 3),\n  palette = \"ggthemes::stata_s2color\"\n)",
    "crumbs": [
      "Analysis",
      "Refined Framework"
    ]
  },
  {
    "objectID": "pages/refined_framework.html#metadata",
    "href": "pages/refined_framework.html#metadata",
    "title": "Refined Secondary Data Framework",
    "section": "2 Metadata",
    "text": "2 Metadata\nHere we pull out the set of 129 metrics from the larger collection and arrange them into a more functional, tidy dataframe:\n\n\nCode\npacman::p_load(\n  dplyr,\n  tidyr,\n  tibble\n)\n\n# Get latest year function\nsource('dev/data_pipeline_functions.R')\n\n# Load metrics data\nsm_data &lt;- readRDS('data/sm_data.rds')\n\n# Load refined framework\nraw_tree &lt;- sm_data[['refined_tree']]\n\n# Load refined framework\nframe &lt;- readRDS('data/frame.rds')\n\n\n## Join with metadata to double check the resolution of our metrics\nmeta &lt;- sm_data$metadata\n# get_str(meta)\n\ndat &lt;- frame %&gt;% \n  filter(variable_name != 'NONE') %&gt;% \n  select(variable_name) %&gt;% \n  left_join(meta, by = 'variable_name') %&gt;% \n  unique()\n# get_str(dat)\n\n# Pull it from the actual metrics data\nmetrics &lt;- sm_data$metrics %&gt;% \n  filter(\n    variable_name %in% frame$variable_name,\n    fips %in% sm_data$state_key$state_code\n  )\n# get_str(metrics)\n\n# Filter to latest year for each metric, and pivot wider\n# Also removing census participation - don't really have data at state level\n# Note to aggregate counties for this at some point\nmetrics_df &lt;- metrics %&gt;%\n  mutate(\n    value = ifelse(value == 'NaN', NA, value),\n    value = str_remove_all(value, ','),\n    value = as.numeric(value)\n  ) %&gt;%\n  get_latest_year() %&gt;% \n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;% \n  unnest(cols = !fips) %&gt;%\n  unique()\n# get_str(metrics_df)\n\n# Let's get rid of the years so they are easier to work with\nnames(metrics_df) &lt;- str_split_i(names(metrics_df), '_', 1)\n# get_str(metrics_df)\n\n# Also get rid of DC - too many missing values\nmetrics_df &lt;- metrics_df %&gt;% \n  filter(fips != '11')\n\n# Save this for use in subsequent pages\nsaveRDS(metrics_df, 'data/metrics_df.rds')\n\n\nBelow, the metrics are displayed in a table that lets you browse and explore them.\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools\n)\n\n# Pull var names from metrics_df out of full metadata\nvars &lt;- unique(frame$variable_name) %&gt;% \n  str_subset('NONE', negate = TRUE)\n\n# Load full metadata table\nmetadata &lt;- sm_data$metadata %&gt;% \n  dplyr::filter(variable_name %in% vars)\n\n# Pick out variables to display\nmetadata &lt;- metadata %&gt;% \n  select(\n    metric,\n    'Variable Name' = variable_name,\n    definition,\n    dimension,\n    index,\n    indicator,\n    units,\n    years = year,\n    'Year' = latest_year, # Renaming latest year as year, not including og year\n    source,\n    scope,\n    updates,\n    resolution,\n    url\n) %&gt;% \n  setNames(c(str_to_title(names(.))))\n\n###\nhtmltools::browsable(\n  tagList(\n    \n    tags$div(\n      style = \"display: flex; gap: 16px; margin-bottom: 20px; justify-content: center;\",\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Show/hide more columns\"),\n        onclick = \"Reactable.setHiddenColumns('metadata_table', prevColumns =&gt; {\n          return prevColumns.length === 0 ? ['Definition', 'Scope', 'Resolution', 'Url'] : []\n        })\"\n      ),\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Download as CSV\"),\n        onclick = \"Reactable.downloadDataCSV('metadata_table', 'sustainability_metadata.csv')\"\n      )\n    ),\n    \n    reactable(\n      metadata[, which(names(metadata) != 'Years')],\n      sortable = TRUE,\n      resizable = TRUE,\n      filterable = TRUE,\n      searchable = TRUE,\n      pagination = TRUE,\n      bordered = TRUE,\n      wrap = TRUE,\n      rownames = FALSE,\n      onClick = 'select',\n      striped = TRUE,\n      pageSizeOptions = c(5, 10, 25, 50, 100),\n      defaultPageSize = 5,\n      showPageSizeOptions = TRUE,\n      highlight = TRUE,\n      style = list(fontSize = \"14px\"),\n      compact = TRUE,\n      fullWidth = TRUE,\n      columns = list(\n        Metric = colDef(\n          minWidth = 200,\n          sticky = 'left'\n        ),\n        'Variable Name' = colDef(\n          minWidth = 150\n        ),\n        Definition = colDef(\n          minWidth = 250\n        ),\n        'Latest Year' = colDef(minWidth = 75),\n        Source = colDef(minWidth = 250),\n        Scope = colDef(show = FALSE),\n        Resolution = colDef(show = FALSE),\n        Url = colDef(\n          minWidth = 300,\n          show = FALSE\n        )\n      ),\n      defaultColDef = colDef(minWidth = 100),\n      elementId = \"metadata_table\",\n      details = function(index) {\n        div(\n          style = \"padding: 15px; border: 1px solid #ddd; margin: 10px 0;\n             background-color: #ecf4ed; border-radius: 10px; border-color: black;\n             box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\",\n          \n          tags$h4(\n            strong(\"Details\"), \n          ),\n          tags$p(\n            strong('Metric Name: '), \n            as.character(metadata[index, 'Metric']),\n          ),\n          tags$p(\n            strong('Variable Name: '), \n            as.character(metadata[index, 'Variable Name']),\n          ),\n          tags$p(\n            strong('Definition: '), \n            as.character(metadata[index, 'Definition']),\n          ),\n          tags$p(\n            strong('Source: '), \n            as.character(metadata[index, 'Source'])\n          ),\n          tags$p(\n            strong('Latest Year: '), \n            as.character(metadata[index, 'Year'])\n          ),\n          tags$p(\n            strong('All Years (cleaned, wrangled, and included): '), \n            as.character(metadata[index, 'Years'])\n          ),\n          tags$p(\n            strong('Updates: '), \n            str_to_title(as.character(metadata[index, 'Updates']))\n          ),\n          tags$p(\n            strong('URL: '), \n            tags$a(\n              href = as.character(metadata[index, 'Url']),\n              target = '_blank',\n              as.character(metadata[index, 'Url'])\n            )\n          )\n        )\n      }\n    )\n  )\n)\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV",
    "crumbs": [
      "Analysis",
      "Refined Framework"
    ]
  },
  {
    "objectID": "temp/test_preso.html",
    "href": "temp/test_preso.html",
    "title": "Habits",
    "section": "",
    "text": "Turn off alarm\nGet out of bed"
  },
  {
    "objectID": "pages/sensitivity.html",
    "href": "pages/sensitivity.html",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "Examples:\n\ncite bene et al 2019 (Béné et al. 2019)\n\nwhat happens when we change the number of indicators"
  },
  {
    "objectID": "pages/sensitivity.html#introduction",
    "href": "pages/sensitivity.html#introduction",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "Examples:\n\ncite bene et al 2019 (Béné et al. 2019)\n\nwhat happens when we change the number of indicators"
  },
  {
    "objectID": "pages/refine_production.html",
    "href": "pages/refine_production.html",
    "title": "Production Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the production dimension. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting.",
    "crumbs": [
      "Indicator Refinement",
      "Production"
    ]
  },
  {
    "objectID": "pages/refine_production.html#indicator-progression",
    "href": "pages/refine_production.html#indicator-progression",
    "title": "Production Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework for the dimension as described in the Wiltshire et al. paper.\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/wiltshire_tree.csv',\n  dimension_in = 'Production',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/matrix_tree.csv',\n  dimension_in = 'Production',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)",
    "crumbs": [
      "Indicator Refinement",
      "Production"
    ]
  },
  {
    "objectID": "pages/refine_production.html#survey",
    "href": "pages/refine_production.html#survey",
    "title": "Production Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the production indicator refinement meeting on January 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/prod_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    ends_with('GROUP'),\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not'\n  )) %&gt;% \n  .[-c(1:2), ]\n\nto_df &lt;- function(x) {\n  if (all(is.na(x))) {\n    return(NULL)\n  } else {\n   x %&gt;%\n    str_remove(' \\\\(joint indicator with Marketability\\\\)') %&gt;%\n    str_remove('\\\\*.*') %&gt;%\n    str_remove(' \\\\(see notes with questions') %&gt;%\n    str_split(',(?!\\\\s)') %&gt;% # Split on comma not followed by a space\n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n     arrange(desc(freq))\n  }\n}\n\nindi_out &lt;- map(dat[1:4], to_df)\nidx_out &lt;- map(dat[5:8], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n})\n\ngraph_table &lt;- graph_table %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n# Note some missing data throws off the graph table. Fix it here\ngraph_table_clean &lt;- graph_table %&gt;% \n  mutate(\n    sort_key = case_when(\n      str_detect(indicator, 'Production Species Diversity') ~ 3e6,\n      str_detect(indicator, 'Not livestock specific') ~ 1010002,\n      .default = sort_key\n    )\n  )\n\n\n\n\nCode\nggplot(graph_table_clean, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so “Must Include” is worth 3 points, “Probably Include” is worth 2 points, “Probably Not Include” is worth 1 point, and “Must Not Include” is worth 0 points. Note that the last column is the sum of proportions of “Must Include” and “Probably Include”. You can sort, search, expand, or page through the table below.\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\n# Add scores by multipliers\nmultipliers &lt;- c(3:1)\nidx_tables &lt;- map2(idx_out[1:3], multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    sort_key = ifelse(str_detect(index, 'Carbon'), 5e6, sort_key),\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:probably_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      # \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\"\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\"\n    )\n  )",
    "crumbs": [
      "Indicator Refinement",
      "Production"
    ]
  },
  {
    "objectID": "pages/refine_economics.html",
    "href": "pages/refine_economics.html",
    "title": "Economic Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the economics dimensions. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting. A final set of indicators to incorporate into the next RFP is still in the works!",
    "crumbs": [
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refine_economics.html#indicator-progression",
    "href": "pages/refine_economics.html#indicator-progression",
    "title": "Economic Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework as described in the Wiltshire et al. paper.\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_wiltshire_tree.csv',\n  dimension_in = 'economics',\n  include_metrics = FALSE,\n  y_limits = c(-1.5, 2.1)\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_tree.csv',\n  dimension_in = 'economics',\n  y_limits = c(-1.5, 2.1)\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Refinement Meeting\nFinally, the tentative set of indicators following the indicator refinement meeting on November 15th, 2024\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_meeting_tree.csv',\n  dimension_in = 'economics',\n  y_limits = c(-1.5, 2.1)\n)",
    "crumbs": [
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refine_economics.html#survey",
    "href": "pages/refine_economics.html#survey",
    "title": "Economic Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the economic indicator refinement meeting on November 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/econ_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    starts_with('Q'),\n    -ends_with('RANK')\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    paste0('add_indi_', 1:3),\n    'notes',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not',\n    paste0('add_idx_', 1:3),\n    'idx_notes',\n    'final_notes'\n  )) %&gt;% \n  .[-c(1:2), ]\n\ngroups &lt;- select(dat, indi_must:indi_must_not, idx_must:idx_probably_not)\n\nto_df &lt;- function(x) {\n  x %&gt;% \n    str_split(',') %&gt;% \n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n    arrange(desc(freq))\n}\n\nindi_out &lt;- map(groups[1:4], to_df)\nidx_out &lt;- map(groups[5:7], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\n\n\nCode\nggplot(graph_table, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so “Must Include” is worth 3 points, “Probably Include” is worth 2 points, “Probably Not Include” is worth 1 point, and “Must Not Include” is worth 0 points. Note that the last column is the sum of proportions of “Must Include” and “Probably Include”. You can sort, search, expand, or page through the table below.\n\n\nCode\n# Add category to tables\nprops &lt;- ind_tables %&gt;% \n  imap(~ .x %&gt;% mutate(cat = .y)) %&gt;% \n  bind_rows() %&gt;% \n  select(-score)\n \n# Get proportion of probably include OR must include\nprop_prob_or_must_include &lt;- props %&gt;% \n  filter(cat %in% c('indi_must', 'indi_probably')) %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(prop_include = sum(freq) / 6) %&gt;% \n  arrange(desc(prop_include))\n\n# Get proportion of must include\nprop_must_include &lt;- props %&gt;% \n  filter(cat == 'indi_must') %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(prop_must = sum(freq) / 6) %&gt;% \n  arrange(desc(prop_must))\n\n# Add up weighted scores\nind_scores &lt;- ind_tables %&gt;% \n  bind_rows() %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(score = sum(score, na.rm = TRUE)) %&gt;% \n  arrange(desc(score))\n\n# Join everything together\nscores_table &lt;- ind_scores %&gt;% \n  full_join(prop_must_include) %&gt;% \n  full_join(prop_prob_or_must_include) %&gt;% \n  arrange(desc(score)) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    across(c(3:4), ~ format(round(.x, 2), nsmall = 2))\n  ) %&gt;% \n  setNames(c('Indicator', 'Score', 'Proportion Must Include', 'Proportion Must OR Probably Include'))\n\n\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\nidx_out &lt;- map(groups[5:7], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:1)\nidx_tables &lt;- map2(idx_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:probably_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')[2:4]\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 16),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\"\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nThe indices are going through the same treatment as indicators above - scored from 3 to 0. Note that there were no indices rated as “Must Not Include”.",
    "crumbs": [
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refinement_process.html",
    "href": "pages/refinement_process.html",
    "title": "Indicator Refinement Process",
    "section": "",
    "text": "To Add:\n\nDescribe dimension meetings\nlink Wiltshire (Wiltshire et al. 2024)\nLink Bene et al 2024 (Béné et al. 2024), describe their process\nWhat is more settled, what is not (social and human)\n\n\n\n\nWhiteboard from economics dimension refinement meeting, November 15th, 2024",
    "crumbs": [
      "Indicator Refinement",
      "Process"
    ]
  },
  {
    "objectID": "pages/refinement_process.html#introduction",
    "href": "pages/refinement_process.html#introduction",
    "title": "Indicator Refinement Process",
    "section": "",
    "text": "To Add:\n\nDescribe dimension meetings\nlink Wiltshire (Wiltshire et al. 2024)\nLink Bene et al 2024 (Béné et al. 2024), describe their process\nWhat is more settled, what is not (social and human)\n\n\n\n\nWhiteboard from economics dimension refinement meeting, November 15th, 2024",
    "crumbs": [
      "Indicator Refinement",
      "Process"
    ]
  },
  {
    "objectID": "pages/refine_environment.html",
    "href": "pages/refine_environment.html",
    "title": "Environment Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the environment dimension. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting. A final set of indicators to incorporate into the next RFP is still in the works!",
    "crumbs": [
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/refine_environment.html#indicator-progression",
    "href": "pages/refine_environment.html#indicator-progression",
    "title": "Environment Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework for the dimension as described in the Wiltshire et al. paper.\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/wiltshire_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/matrix_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Refinement Meeting\nFinally, the tentative set of indicators following the indicator refinement meeting on November 22nd, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/env_meeting_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)",
    "crumbs": [
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/refine_environment.html#survey",
    "href": "pages/refine_environment.html#survey",
    "title": "Environment Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the economic indicator refinement meeting on November 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/env_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    ends_with('GROUP'),\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not'\n  )) %&gt;% \n  .[-c(1:2), ]\n\nto_df &lt;- function(x) {\n  x %&gt;%\n    str_replace_all('PFAS, PFOS', 'PFAS/PFOS') %&gt;% \n    str_replace_all('soil loss/', 'Soil loss/') %&gt;% \n    str_split(',') %&gt;% \n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n    arrange(desc(freq))\n}\n\nindi_out &lt;- map(dat[1:4], to_df)\nidx_out &lt;- map(dat[5:8], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\n\n\nCode\nggplot(graph_table, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so “Must Include” is worth 3 points, “Probably Include” is worth 2 points, “Probably Not Include” is worth 1 point, and “Must Not Include” is worth 0 points. Note that the last column is the sum of proportions of “Must Include” and “Probably Include”. You can sort, search, expand, or page through the table below.\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nidx_tables &lt;- map2(idx_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    sort_key = ifelse(str_detect(index, 'Carbon'), 5e6, sort_key),\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\",\n      'must_not'\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\",\n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nThe indices are going through the same treatment as indicators above - scored from 3 to 0. Note here that the “Carbon ($ GHGs/nutrients)” index seems to be missing a vote. So, it only has 12 points, but the proportion of votes for “Must Include” is 1.",
    "crumbs": [
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/selection.html",
    "href": "pages/selection.html",
    "title": "Variable Selection and Regression",
    "section": "",
    "text": "On this page we will take our min-max normalized, geometrically averaged scores, which look like the most reliable and approachable so far, and take a deeper dive into variable selection, regression, and PCA. From the dimension meetings, it sounds like we may have some indicators with a couple of metrics, and potentially others with dozens. Because of this, and because of our focus on developing sensible indicators, I think it will be best to do any weighting at the indicator level or above. This also reduces our variable count substantially in relation to our state count of 51, opening more doors for PCA.\nIt is worth emphasizing at the top that the metrics that are making up this secondary data framework are not a great representation of the system. There are some important holes, as well as a heap of metrics that are serving as rather uninspiring proxies. So, extrapolation of these results beyond the confines of the exercise is not recommended. The purpose here is to explore strengths and tradeoffs in methods for aggregating the data. As primary data come in and make up the bulk of the framework and secondary data are used to fill in the gaps, this should start becoming more interpretable.",
    "crumbs": [
      "Analysis",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/selection.html#component-extraction",
    "href": "pages/selection.html#component-extraction",
    "title": "Variable Selection and Regression",
    "section": "2.1 Component Extraction",
    "text": "2.1 Component Extraction\n\n\nCode\npacman::p_load(\n  psych\n)\n\n# Filter down to just indicators for PCA\npca_dat &lt;- dat %&gt;% \n  select(starts_with('indic')) %&gt;% \n  setNames(c(str_remove(names(.), 'indic_')))\n# get_str(pca_dat)\n\n# Explore how many factors to extract\nVSS(pca_dat, n = 8, rotate = 'varimax')\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.49  with  4  factors\nVSS complexity 2 achieves a maximimum of 0.71  with  6  factors\n\nThe Velicer MAP achieves a minimum of 0.04  with  8  factors \nBIC achieves a minimum of  -1322.14  with  3  factors\nSample Size adjusted BIC achieves a minimum of  352.67  with  8  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq\n1 0.35 0.00 0.050 702  1499\n2 0.44 0.54 0.045 664  1320\n3 0.46 0.65 0.038 627  1155\n4 0.47 0.67 0.040 591  1078\n5 0.46 0.68 0.039 556   989\n6 0.49 0.71 0.036 522   869\n7 0.47 0.71 0.037 489   803\n8 0.46 0.70 0.036 457   723\n                                                             prob sqresid  fit\n1 0.0000000000000000000000000000000000000000000000000000000000068      78 0.35\n2 0.0000000000000000000000000000000000000000000008490863736163976      55 0.54\n3 0.0000000000000000000000000000000008270921939207155875427301961      36 0.70\n4 0.0000000000000000000000000000006309857959306571906395177951055      28 0.76\n5 0.0000000000000000000000000093510802133250057006572109052910946      22 0.82\n6 0.0000000000000000000950845476680276789193529962673778754833620      17 0.86\n7 0.0000000000000000130642550712603611770372402656192889480735175      14 0.88\n8 0.0000000000000243818193426151022746087293491257241839775815606      11 0.91\n  RMSEA   BIC SABIC complex eChisq  SRMR eCRMS  eBIC\n1  0.15 -1275   930     1.0   2537 0.181 0.186  -237\n2  0.14 -1303   782     1.3   1649 0.146 0.155  -975\n3  0.13 -1322   647     1.6    950 0.111 0.121 -1528\n4  0.12 -1257   599     1.7    726 0.097 0.109 -1609\n5  0.12 -1208   538     1.9    533 0.083 0.096 -1664\n6  0.11 -1193   446     2.0    371 0.069 0.083 -1691\n7  0.11 -1130   406     2.1    287 0.061 0.075 -1645\n8  0.10 -1082   353     2.3    208 0.052 0.066 -1597\n\n\nCode\nfa.parallel(pca_dat)\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  6  and the number of components =  6 \n\n\nBoth MAP and Parallel Analysis determine that there should be 5 components.\n\n\nCode\npca_out &lt;- pca(pca_dat, nfactors = 5)\nplot(pca_out$values)\nabline(h = 1)\n\n\n\n\n\n\n\n\n\nThe scree plot is not all that convincing, though. One could make an argument for seven components.",
    "crumbs": [
      "Analysis",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/selection.html#run-pca",
    "href": "pages/selection.html#run-pca",
    "title": "Variable Selection and Regression",
    "section": "2.2 Run PCA",
    "text": "2.2 Run PCA\nLet’s go ahead with the recommended components from MAP and PA, which are generally more reliable than scree plots.\n\n\nCode\npca_out\n\n\nPrincipal Components Analysis\nCall: principal(r = r, nfactors = nfactors, residuals = residuals, \n    rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, \n    missing = missing, impute = impute, oblique.scores = oblique.scores, \n    method = method, use = use, cor = cor, correct = 0.5, weight = NULL)\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                        RC1   RC2   RC3   RC5   RC4   h2   u2\naccess_to_land                         0.86  0.06  0.08 -0.11 -0.15 0.79 0.21\nwealth_income_distribution             0.01 -0.03 -0.02  0.66  0.09 0.45 0.55\noperations_diversification            -0.46  0.01  0.27  0.72  0.06 0.81 0.19\nincome_stability                       0.35  0.75  0.20 -0.01  0.13 0.74 0.26\nuse_of_ag_farm_crop_insurance         -0.23  0.19  0.50 -0.08 -0.13 0.36 0.64\nfluxes                                 0.82  0.08 -0.03 -0.22 -0.13 0.75 0.25\nstocks                                 0.09  0.12  0.14 -0.08  0.67 0.49 0.51\nembodied                               0.77  0.00  0.06 -0.16 -0.07 0.62 0.38\nhealth                                -0.24 -0.14  0.26  0.22  0.64 0.60 0.40\nland_use_diversity                     0.05 -0.07 -0.15  0.10  0.59 0.39 0.61\nbiodiversity                          -0.16  0.65 -0.29 -0.15  0.06 0.56 0.44\nsensitive_or_rare_habitats            -0.01  0.65  0.03 -0.12  0.01 0.43 0.57\nquantity                               0.19  0.61  0.19  0.15  0.10 0.48 0.52\nquality                               -0.25  0.13  0.23  0.01  0.66 0.57 0.43\neducational_attainment                -0.13  0.01  0.67  0.23  0.10 0.52 0.48\nfood_access                            0.30  0.15 -0.26  0.05  0.51 0.44 0.56\naccess_to_culturally_appropriate_food -0.24  0.38 -0.02  0.52  0.02 0.47 0.53\ndietary_quality                       -0.27  0.08  0.23  0.10  0.45 0.34 0.66\nfood_affordability                    -0.18 -0.17 -0.26  0.14  0.28 0.22 0.78\nmental_health_tbd                     -0.46  0.03  0.04 -0.29  0.11 0.31 0.69\nhousing_supply_and_quality            -0.52  0.34 -0.14  0.31  0.32 0.60 0.40\naccess_to_care                        -0.20 -0.07  0.40  0.55  0.01 0.51 0.49\nphysical_health_tbd                   -0.09  0.02 -0.82 -0.26 -0.01 0.74 0.26\ntotal_quantity_exported                0.71  0.50  0.14  0.06  0.05 0.78 0.22\ntotal_quantity_imported                0.32  0.72  0.01  0.15  0.13 0.67 0.33\nproduction_species_diversity          -0.05  0.47 -0.10 -0.39  0.11 0.39 0.61\nproduction_inputs                      0.62  0.22 -0.32  0.19  0.05 0.57 0.43\ntotal_quantity_food_products          -0.20 -0.04 -0.05 -0.70 -0.05 0.53 0.47\ntotal_quantity_forest_products        -0.04  0.08 -0.43 -0.28  0.51 0.53 0.47\nvalue_added_market                    -0.27  0.00  0.25  0.69  0.13 0.63 0.37\ntotal_quantity_non_food_ag_products    0.49  0.16 -0.21 -0.20  0.30 0.44 0.56\ncrop_failure                           0.35  0.59  0.14 -0.05 -0.09 0.50 0.50\nsocial_connectedness                  -0.09 -0.02 -0.75 -0.21 -0.02 0.61 0.39\ncommunity_safety                       0.09 -0.21  0.28  0.42 -0.08 0.32 0.68\ndiverse_representation                 0.50 -0.09 -0.22  0.39  0.13 0.47 0.53\ngender_diversity                      -0.48  0.45  0.24  0.38 -0.04 0.64 0.36\nage_diversity                         -0.37  0.50 -0.22 -0.07  0.00 0.45 0.55\nracial_diversity                      -0.26  0.70 -0.31  0.15 -0.29 0.76 0.24\nparticipatory_governance              -0.06 -0.11  0.72 -0.04  0.17 0.56 0.44\n                                      com\naccess_to_land                        1.1\nwealth_income_distribution            1.0\noperations_diversification            2.0\nincome_stability                      1.7\nuse_of_ag_farm_crop_insurance         2.0\nfluxes                                1.2\nstocks                                1.2\nembodied                              1.1\nhealth                                2.1\nland_use_diversity                    1.2\nbiodiversity                          1.7\nsensitive_or_rare_habitats            1.1\nquantity                              1.6\nquality                               1.6\neducational_attainment                1.4\nfood_access                           2.4\naccess_to_culturally_appropriate_food 2.3\ndietary_quality                       2.5\nfood_affordability                    4.0\nmental_health_tbd                     1.9\nhousing_supply_and_quality            3.4\naccess_to_care                        2.2\nphysical_health_tbd                   1.2\ntotal_quantity_exported               1.9\ntotal_quantity_imported               1.6\nproduction_species_diversity          2.2\nproduction_inputs                     2.0\ntotal_quantity_food_products          1.2\ntotal_quantity_forest_products        2.6\nvalue_added_market                    1.7\ntotal_quantity_non_food_ag_products   2.8\ncrop_failure                          1.8\nsocial_connectedness                  1.2\ncommunity_safety                      2.5\ndiverse_representation                2.6\ngender_diversity                      3.4\nage_diversity                         2.3\nracial_diversity                      2.2\nparticipatory_governance              1.2\n\n                       RC1  RC2  RC3  RC5  RC4\nSS loadings           5.56 4.63 3.98 3.94 2.95\nProportion Var        0.14 0.12 0.10 0.10 0.08\nCumulative Var        0.14 0.26 0.36 0.46 0.54\nProportion Explained  0.26 0.22 0.19 0.19 0.14\nCumulative Proportion 0.26 0.48 0.67 0.86 1.00\n\nMean item complexity =  1.9\nTest of the hypothesis that 5 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n with the empirical chi square  600.72  with prob &lt;  0.092 \n\nFit based upon off diagonal values = 0.86\n\n\nThere is a lot to look at here, but here are some impressions.\nMost of our current economics indicators (access to land to income stability), are coalescing into RC1. However, access to land splits out into RC3, alongside environment indicators like carbon stocks and forest health, as well as production indicators like inputs and total quantity of food products. The access to land indicator does not have an ideal set of metrics under it currently - it is made up of land and building value per farm and acreage per farm as a proxy for access. So it seems reasonable enough that this indicator is pulling away from the rest of the economics indicators.\nThe environment indicators (carbon stocks to water quantity) mostly make up RC5, but are also split between several other components. The carbon stocks and forest health indicators are currently both derived from the USFS TreeMap dataset, so it makes sense they fall together in RC5. This is a particularly scattered dimension though - perhaps partly from hinky proxy metrics and partly for the diversity of the dimension itself.\nHealth indicators (education to physical health) fall pretty consistently into RC2. A notable exception is food access, which falls into RC5 along with TreeMap data. I have a hunch that RC5 is measuring the rural/urban divide at this point. The culturally appropriate food indicator also moves over to RC1 alongside economics indicators. Given that this indicator currently consists only of the ability of School Food Authorities to provide culturally appropriate foods, this is probably tracking with wealth.\nProduction indicators (richness to value added markets) fall mostly into RC2. However, richness (measured by crop diversity) swings out into RC4, which is a bit of a grab bag but does contain some forest indicators. The economics dimension RC1 also loads strongly onto the value added markets indicator.\nFinally, the social dimension is scattered all over the place. I can’t make much sense of it as a whole, given that it has a pretty unsatisfactory set of metrics behind it at this point.\nThe next step for this PCA path is to see how things look if we aggregate our indices and dimensions based on these PCA loadings in the style of Nicoletti (2000), rather than working with the arithmetic/geometric means as we do below.",
    "crumbs": [
      "Analysis",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/selection.html#economics",
    "href": "pages/selection.html#economics",
    "title": "Variable Selection and Regression",
    "section": "3.1 Economics",
    "text": "3.1 Economics\n\n3.1.1 Linear Model\nFirst we can try a plain old linear model to see how economics loads onto its indicators.\n\n\nCode\n# Reduce data down to dimen_economics and all indicators\necon_dat &lt;- select(dat, dimen_economics, starts_with('indic')) %&gt;% \n  setNames(c(names(.) %&gt;% str_remove('indic_|dimen_')))\nget_str(econ_dat)\n\n\nrowws_df [52 × 40] (S3: rowwise_df/tbl_df/tbl/data.frame)\n $ economics                            : num [1:52] 0.131 0.205 0.137 0.154 0..\n $ access_to_land                       : num [1:52] 0.0968 0.0388 0.0326 0.15..\n $ wealth_income_distribution           : num [1:52] 0.264 0.419 0.33 0.328 0...\n $ operations_diversification           : num [1:52] 0.01315 0.39392 0.05679 0..\n $ income_stability                     : num [1:52] 0.0665 0.5055 0.2395 0.10..\n $ use_of_ag_farm_crop_insurance        : num [1:52] 0.7686 0.7658 1 0.7014 1 ..\n $ fluxes                               : num [1:52] 0.08353 0.03885 0.07518 0..\n $ stocks                               : num [1:52] 0.1654 0.3637 0.0185 0.32..\n $ embodied                             : num [1:52] 0.05821 0.00601 0.14482 0..\n $ health                               : num [1:52] 0.25 0.4343 0.1041 0.3586..\n $ land_use_diversity                   : num [1:52] 0.954307 0.565581 0.29319..\n $ biodiversity                         : num [1:52] 0.289 0.0958 0.3523 0.168..\n $ sensitive_or_rare_habitats           : num [1:52] 0.3159 0.7319 0.3236 0.27..\n $ quantity                             : num [1:52] 0.01107 0.61685 0.09591 0..\n $ quality                              : num [1:52] 0.581 0.583 0.449 0.454 0..\n $ educational_attainment               : num [1:52] 0.2323 0.4223 0.4479 0.15..\n $ food_access                          : num [1:52] 0.339 0.356 0.116 0.324 0..\n $ access_to_culturally_appropriate_food: num [1:52] 0.1563 0.3824 0.0818 0.07..\n $ dietary_quality                      : num [1:52] 0.36 0.684 0.352 0.768 0...\n $ food_affordability                   : num [1:52] 0.579 0.51 0.38 0.229 0.4..\n $ mental_health_tbd                    : num [1:52] 0.747 0.565 0.315 0.534 0..\n $ housing_supply_and_quality           : num [1:52] 0.357 0.566 0.467 0.278 0..\n $ access_to_care                       : num [1:52] 0.189 0.436 0.348 0.222 0..\n $ physical_health_tbd                  : num [1:52] 0.487 0.382 0.431 0.454 0..\n $ total_quantity_exported              : num [1:52] 0.068031 1 0.068105 0.174..\n $ total_quantity_imported              : num [1:52] 0.01995 1 0.31967 0.01134..\n $ production_species_diversity         : num [1:52] 0.5496 0.3323 0.6537 0.37..\n $ production_inputs                    : num [1:52] 0.2 1 0.381 0.6952 0.4667..\n $ total_quantity_food_products         : num [1:52] 0.9903 0.6528 0.4495 0.72..\n $ total_quantity_forest_products       : num [1:52] 0.635061 0.000087 0.00048..\n $ value_added_market                   : num [1:52] 0.01481 0.30841 0.04466 0..\n $ total_quantity_non_food_ag_products  : num [1:52] 0.19143 0.05355 0.02043 0..\n $ crop_failure                         : num [1:52] 0.01484 1 0.0588 0.08897 ..\n $ social_connectedness                 : num [1:52] 0.659 0.573 0.375 0.691 0..\n $ community_safety                     : num [1:52] 0.25072 0.61582 0.10413 0..\n $ diverse_representation               : num [1:52] 0.498 0.449 0.284 0.648 0..\n $ gender_diversity                     : num [1:52] 0.274 0.9229 1 0.4825 0.4..\n $ age_diversity                        : num [1:52] 0.623 0.245 0.887 0.415 0..\n $ racial_diversity                     : num [1:52] 0.28454 0.34292 0.82326 0..\n $ participatory_governance             : num [1:52] 0.3192 0.514 0.4967 0.011..\n\n\nCode\nlm &lt;- lm(economics ~ ., data = econ_dat)\nsummary(lm)\n\n\n\nCall:\nlm(formula = economics ~ ., data = econ_dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.020636 -0.007162 -0.001059  0.008755  0.024751 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                           -0.047539   0.118263  -0.402    0.695\naccess_to_land                         0.502379   0.378406   1.328    0.209\nwealth_income_distribution             0.178693   0.103830   1.721    0.111\noperations_diversification            -0.039948   0.062835  -0.636    0.537\nincome_stability                       0.204744   0.127063   1.611    0.133\nuse_of_ag_farm_crop_insurance          0.023533   0.028838   0.816    0.430\nfluxes                                 0.029926   0.174821   0.171    0.867\nstocks                                 0.078485   0.073959   1.061    0.309\nembodied                              -0.071828   0.121378  -0.592    0.565\nhealth                                -0.066892   0.116062  -0.576    0.575\nland_use_diversity                     0.021275   0.028293   0.752    0.467\nbiodiversity                           0.042243   0.117001   0.361    0.724\nsensitive_or_rare_habitats            -0.049094   0.078335  -0.627    0.543\nquantity                               0.157569   0.185372   0.850    0.412\nquality                               -0.138013   0.081487  -1.694    0.116\neducational_attainment                -0.041504   0.046307  -0.896    0.388\nfood_access                            0.021315   0.085094   0.250    0.806\naccess_to_culturally_appropriate_food  0.095117   0.060361   1.576    0.141\ndietary_quality                       -0.002167   0.044016  -0.049    0.962\nfood_affordability                    -0.060733   0.055068  -1.103    0.292\nmental_health_tbd                      0.002617   0.053998   0.048    0.962\nhousing_supply_and_quality             0.065799   0.106994   0.615    0.550\naccess_to_care                         0.086609   0.116798   0.742    0.473\nphysical_health_tbd                    0.027545   0.143290   0.192    0.851\ntotal_quantity_exported               -0.114485   0.156544  -0.731    0.479\ntotal_quantity_imported                0.062363   0.044995   1.386    0.191\nproduction_species_diversity           0.069385   0.072457   0.958    0.357\nproduction_inputs                     -0.087578   0.066339  -1.320    0.211\ntotal_quantity_food_products           0.009952   0.047508   0.209    0.838\ntotal_quantity_forest_products        -0.003261   0.044606  -0.073    0.943\nvalue_added_market                     0.061485   0.049978   1.230    0.242\ntotal_quantity_non_food_ag_products    0.079406   0.080000   0.993    0.341\ncrop_failure                          -0.069609   0.120866  -0.576    0.575\nsocial_connectedness                   0.087303   0.072606   1.202    0.252\ncommunity_safety                       0.031476   0.042684   0.737    0.475\ndiverse_representation                 0.005930   0.050703   0.117    0.909\ngender_diversity                       0.088812   0.071055   1.250    0.235\nage_diversity                         -0.026434   0.038963  -0.678    0.510\nracial_diversity                      -0.097192   0.085951  -1.131    0.280\nparticipatory_governance               0.024798   0.040221   0.617    0.549\n\nResidual standard error: 0.02323 on 12 degrees of freedom\nMultiple R-squared:  0.9554,    Adjusted R-squared:  0.8104 \nF-statistic:  6.59 on 39 and 12 DF,  p-value: 0.0005957\n\n\nWe can see that most of the economics indicators (access to land, wealth and income distribution, income stability) are significant predictors, while operations diversification is close. But some surprises are access to culturally appropriate food (school food authorities serving culturally relevant food), housing supply and quality, as well as a few production indicators, like richness (crop diversity), production inputs, and value-added markets. Social connectedness from the social dimension also makes it on the list. The largest coefficients by a wide margin are for access to land and wealth and income distribution.\n\n\n3.1.2 Splitting Data\nHere we split out data into a 60/40 training/test set for cross validation with GLMnet and Random Forest models. Note that we are pushing the limits of our sample size. But this should help protect against overfitting.\n\n\nCode\npacman::p_load(\n  caret,\n  ranger,\n  glmnet\n)\n\n# Split data 60/40\nset.seed(42)\nindices &lt;- createDataPartition(econ_dat$economics, p = 0.60, list = FALSE)\ntraining_data &lt;- econ_dat[indices, ]\ntesting_data &lt;- econ_dat[-indices,]\n\nmy_folds &lt;- createFolds(training_data$economics, k = 5, list = TRUE)\n\n# Control\nmy_control &lt;- trainControl(\n  method = 'cv',\n  number = 5,\n  verboseIter = TRUE,\n  index = my_folds\n)\n\n# Check for zero variance or near zero variance indicators\nnearZeroVar(dat, names = TRUE, saveMetrics = TRUE)\n# All clear\n\n\n\n\n3.1.3 GLMnet\nHere we use a GLMnet to find an optimal balance between a ridge regression, which penalizes variables based on the magnitude of coefficients, and lasso regression, which adds a penalty based on the absolute value of coefficients. We use a tuning grid to find optimal values of alpha (0 = ridge, 1 = lasso) and lambda (the penalty parameter). Both this and the random forest model are particularly good at prediction, but also provide a metric for variable importance that can help us interpret our indicators.\n\n\nCode\nset.seed(42)\necon_glmnet &lt;- train(\n  economics ~ .,\n  data = training_data, \n  tuneGrid = expand.grid(\n    alpha = seq(0.1, 1, length = 5),\n    lambda = seq(0.0001, 0.1, length = 100)\n  ),\n  method = \"glmnet\",\n  trControl = my_control,\n  preProcess = c('zv', 'center', 'scale')\n)\n\n\n+ Fold1: alpha=0.100, lambda=0.1 \n- Fold1: alpha=0.100, lambda=0.1 \n+ Fold1: alpha=0.325, lambda=0.1 \n- Fold1: alpha=0.325, lambda=0.1 \n+ Fold1: alpha=0.550, lambda=0.1 \n- Fold1: alpha=0.550, lambda=0.1 \n+ Fold1: alpha=0.775, lambda=0.1 \n- Fold1: alpha=0.775, lambda=0.1 \n+ Fold1: alpha=1.000, lambda=0.1 \n- Fold1: alpha=1.000, lambda=0.1 \n+ Fold2: alpha=0.100, lambda=0.1 \n- Fold2: alpha=0.100, lambda=0.1 \n+ Fold2: alpha=0.325, lambda=0.1 \n- Fold2: alpha=0.325, lambda=0.1 \n+ Fold2: alpha=0.550, lambda=0.1 \n- Fold2: alpha=0.550, lambda=0.1 \n+ Fold2: alpha=0.775, lambda=0.1 \n- Fold2: alpha=0.775, lambda=0.1 \n+ Fold2: alpha=1.000, lambda=0.1 \n- Fold2: alpha=1.000, lambda=0.1 \n+ Fold3: alpha=0.100, lambda=0.1 \n- Fold3: alpha=0.100, lambda=0.1 \n+ Fold3: alpha=0.325, lambda=0.1 \n- Fold3: alpha=0.325, lambda=0.1 \n+ Fold3: alpha=0.550, lambda=0.1 \n- Fold3: alpha=0.550, lambda=0.1 \n+ Fold3: alpha=0.775, lambda=0.1 \n- Fold3: alpha=0.775, lambda=0.1 \n+ Fold3: alpha=1.000, lambda=0.1 \n- Fold3: alpha=1.000, lambda=0.1 \n+ Fold4: alpha=0.100, lambda=0.1 \n- Fold4: alpha=0.100, lambda=0.1 \n+ Fold4: alpha=0.325, lambda=0.1 \n- Fold4: alpha=0.325, lambda=0.1 \n+ Fold4: alpha=0.550, lambda=0.1 \n- Fold4: alpha=0.550, lambda=0.1 \n+ Fold4: alpha=0.775, lambda=0.1 \n- Fold4: alpha=0.775, lambda=0.1 \n+ Fold4: alpha=1.000, lambda=0.1 \n- Fold4: alpha=1.000, lambda=0.1 \n+ Fold5: alpha=0.100, lambda=0.1 \n- Fold5: alpha=0.100, lambda=0.1 \n+ Fold5: alpha=0.325, lambda=0.1 \n- Fold5: alpha=0.325, lambda=0.1 \n+ Fold5: alpha=0.550, lambda=0.1 \n- Fold5: alpha=0.550, lambda=0.1 \n+ Fold5: alpha=0.775, lambda=0.1 \n- Fold5: alpha=0.775, lambda=0.1 \n+ Fold5: alpha=1.000, lambda=0.1 \n- Fold5: alpha=1.000, lambda=0.1 \n\n\nAggregating results\nSelecting tuning parameters\nFitting alpha = 0.775, lambda = 0.0405 on full training set\n\n\nCode\nimportance &lt;- varImp(econ_glmnet, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nCode\n# Predict\n# p &lt;- predict(econ_glmnet, testing_data)\n# postResample(pred = p, obs = testing_data$economics)\n\n\nThe optimal hyperparameters from the tuning grid were alpha = 0.1 (mostly ridge regression) and lambda = 0.00313. The variable importance plot is on a relative scale of 0 (unimportant) to 100 (most important) in terms of predictive power. Curiously, it is showing that the value added market indicator from the production dimension is a better predictor of economics than any economics indicator.\n\n\n3.1.4 Random Forest\nNow we can try a random forest, which is particularly good at handling non-linear relationships. Here we use the RMSE to determine the optimal combination of mtry (the number of variables selected at each node in the decision tree), the split rule, and the minimum node size.\n\n\nCode\nset.seed(42)\necon_rf &lt;- train(\n  economics ~ .,\n  data = training_data, \n  tuneLength = 7,\n  method = \"ranger\",\n  trControl = my_control,\n  importance = 'impurity'\n)\n\n\n+ Fold1: mtry= 2, min.node.size=5, splitrule=variance \n- Fold1: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold1: mtry= 8, min.node.size=5, splitrule=variance \n- Fold1: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold1: mtry=14, min.node.size=5, splitrule=variance \n- Fold1: mtry=14, min.node.size=5, splitrule=variance \n+ Fold1: mtry=20, min.node.size=5, splitrule=variance \n- Fold1: mtry=20, min.node.size=5, splitrule=variance \n+ Fold1: mtry=26, min.node.size=5, splitrule=variance \n- Fold1: mtry=26, min.node.size=5, splitrule=variance \n+ Fold1: mtry=32, min.node.size=5, splitrule=variance \n- Fold1: mtry=32, min.node.size=5, splitrule=variance \n+ Fold1: mtry=39, min.node.size=5, splitrule=variance \n- Fold1: mtry=39, min.node.size=5, splitrule=variance \n+ Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold1: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=39, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=39, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry= 2, min.node.size=5, splitrule=variance \n- Fold2: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold2: mtry= 8, min.node.size=5, splitrule=variance \n- Fold2: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold2: mtry=14, min.node.size=5, splitrule=variance \n- Fold2: mtry=14, min.node.size=5, splitrule=variance \n+ Fold2: mtry=20, min.node.size=5, splitrule=variance \n- Fold2: mtry=20, min.node.size=5, splitrule=variance \n+ Fold2: mtry=26, min.node.size=5, splitrule=variance \n- Fold2: mtry=26, min.node.size=5, splitrule=variance \n+ Fold2: mtry=32, min.node.size=5, splitrule=variance \n- Fold2: mtry=32, min.node.size=5, splitrule=variance \n+ Fold2: mtry=39, min.node.size=5, splitrule=variance \n- Fold2: mtry=39, min.node.size=5, splitrule=variance \n+ Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold2: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=39, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=39, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry= 2, min.node.size=5, splitrule=variance \n- Fold3: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold3: mtry= 8, min.node.size=5, splitrule=variance \n- Fold3: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold3: mtry=14, min.node.size=5, splitrule=variance \n- Fold3: mtry=14, min.node.size=5, splitrule=variance \n+ Fold3: mtry=20, min.node.size=5, splitrule=variance \n- Fold3: mtry=20, min.node.size=5, splitrule=variance \n+ Fold3: mtry=26, min.node.size=5, splitrule=variance \n- Fold3: mtry=26, min.node.size=5, splitrule=variance \n+ Fold3: mtry=32, min.node.size=5, splitrule=variance \n- Fold3: mtry=32, min.node.size=5, splitrule=variance \n+ Fold3: mtry=39, min.node.size=5, splitrule=variance \n- Fold3: mtry=39, min.node.size=5, splitrule=variance \n+ Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold3: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=39, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=39, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry= 2, min.node.size=5, splitrule=variance \n- Fold4: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold4: mtry= 8, min.node.size=5, splitrule=variance \n- Fold4: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold4: mtry=14, min.node.size=5, splitrule=variance \n- Fold4: mtry=14, min.node.size=5, splitrule=variance \n+ Fold4: mtry=20, min.node.size=5, splitrule=variance \n- Fold4: mtry=20, min.node.size=5, splitrule=variance \n+ Fold4: mtry=26, min.node.size=5, splitrule=variance \n- Fold4: mtry=26, min.node.size=5, splitrule=variance \n+ Fold4: mtry=32, min.node.size=5, splitrule=variance \n- Fold4: mtry=32, min.node.size=5, splitrule=variance \n+ Fold4: mtry=39, min.node.size=5, splitrule=variance \n- Fold4: mtry=39, min.node.size=5, splitrule=variance \n+ Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold4: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=39, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=39, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry= 2, min.node.size=5, splitrule=variance \n- Fold5: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold5: mtry= 8, min.node.size=5, splitrule=variance \n- Fold5: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold5: mtry=14, min.node.size=5, splitrule=variance \n- Fold5: mtry=14, min.node.size=5, splitrule=variance \n+ Fold5: mtry=20, min.node.size=5, splitrule=variance \n- Fold5: mtry=20, min.node.size=5, splitrule=variance \n+ Fold5: mtry=26, min.node.size=5, splitrule=variance \n- Fold5: mtry=26, min.node.size=5, splitrule=variance \n+ Fold5: mtry=32, min.node.size=5, splitrule=variance \n- Fold5: mtry=32, min.node.size=5, splitrule=variance \n+ Fold5: mtry=39, min.node.size=5, splitrule=variance \n- Fold5: mtry=39, min.node.size=5, splitrule=variance \n+ Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold5: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=39, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=39, min.node.size=5, splitrule=extratrees \n\n\nAggregating results\nSelecting tuning parameters\nFitting mtry = 14, splitrule = extratrees, min.node.size = 5 on full training set\n\n\nCode\n# econ_rf\n# plot(econ_rf)\n\nimportance &lt;- varImp(econ_rf, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nCode\n# Predict\n# p &lt;- predict(model_mf, testing_data)\n# postResample(pred = p, obs = testing_data$rebl_tpm)\n\n\nThe random forest model is also picking out the value-added market indicator as the best predictor of economics dimension scores, followed closely by operations diversification, wealth and income distribution, and income stability.\nVery curious how value-added markets keep sticking out. The two metrics making up this indicator are both from NASS: the percentage of farms reporting value-added sales, and of those farms, the percentage of value-added sales out of total sales.",
    "crumbs": [
      "Analysis",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/sm-explorer.html",
    "href": "pages/sm-explorer.html",
    "title": "SM-Explorer",
    "section": "",
    "text": "Caution\n\n\n\n\n\nThe SM-Explorer is a work in progress. There are a small heap of bugs I’m already aware of, and about a hundred things I’d still like to add. If/when you find things that aren’t working properly, please feel free to let Chris know!\n\n\n\nThis is a Shiny app that allows for interactive exploration of metrics, mostly at the county level. It includes a map page, a bivariate plot explorer, and a metadata table much like what is included in this Quarto doc. It tends to work best if you open it in its own page using the button below:\n\n\n\n\nGo To SM-Explorer\n\n\n\n\nYou can also just use it here in the window. Note that some functions (like the full screen button) won’t work here.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Secondary Data",
      "SM-Explorer"
    ]
  },
  {
    "objectID": "pages/indicator_correlations.html",
    "href": "pages/indicator_correlations.html",
    "title": "Indicator Correlations",
    "section": "",
    "text": "Code\npacman::p_load(\n  dplyr,\n  conflicted\n)\n\nconflicts_prefer(\n  dplyr::select(),\n  dplyr::filter(),\n  dplyr::summarize(),\n  .quiet = TRUE\n)\n\nsource('dev/get_reactable.r')\n\n\nThis page will explore correlations between variables at the indicator level.\n\n1 Correlation Matrix\n\n\nCode\npacman::p_load(\n  dplyr,\n  tidyr,\n  tibble,\n  stringr,\n  purrr,\n  tidyr,\n  ggplot2,\n  plotly,\n  reshape,\n  Hmisc,\n  viridisLite\n)\n\n# Load indicator data.\nfinal_scores &lt;- readRDS('data/state_score_iterations.rds')\n# get_str(final_scores)\n\n# Pull out minmax geo indicators only. Also use states only, no aggregates\nminmax_geo_indicators &lt;- final_scores$minmax_geometric$indicator_scores %&gt;% \n  filter(! state %in% c('US_mean', 'US_median', 'NewEng'))\n# get_str(minmax_geo_indicators)\n\n# Make a correlation matrix using all the selected variables\nmat &lt;- minmax_geo_indicators %&gt;% \n  select(-state) %&gt;% \n  as.matrix()\n\n# Get correlations\ncor &lt;- rcorr(mat, type = 'pearson')\n\n# Melt correlation values and rename columns\ncor_r &lt;- melt(cor$r) %&gt;% \n  setNames(c('var_1', 'var_2', 'value'))\n  # mutate(across(where(is.factor), as.character))\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Make heatmap with custom text aesthetic for tooltip\nplot &lt;- cor_r %&gt;% \n  ggplot(aes(var_1, var_2, fill = value, text = paste0(\n    'Var 1: ', var_1, '\\n',\n    'Var 2: ', var_2, '\\n',\n    'Correlation: ', format(round(value, 3), nsmall = 3), '\\n',\n    'P-Value: ', format(round(p.value, 3), nsmall = 3)\n  ))) + \n  geom_tile() + \n  scale_fill_gradient2(\n    low = \"#762a83\", \n    mid = \"white\", \n    high = \"#1b7837\", \n    midpoint = 0\n  ) +\n  theme(axis.text.x = element_text(hjust = 1, angle = 45)) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = 'Correlation'\n  )\n\n# Convert to interactive plotly figure with text tooltip\nggplotly(\n  plot, \n  tooltip = 'text',\n  width = 1000,\n  height = 800\n)\n\n\n\n\nInteractive Correlation Plot\n\n\n\n\n2 Strong Correlations\nWe have many significant correlations between indicators, but we probably don’t care too much about weak correlations. Let’s isolate the correlations that are significant and &gt; 0.5. These are the ones that might suggest we are double-counting certain aspects of the food system.\n\n\nCode\npacman::p_load(\n  reactable,\n  Hmisc\n)\n\n# Isolate all significant correlations\nget_str(cor_r)\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Add p values to dataframe with correlations\ncor_r$p &lt;- cor_p$value\nget_str(cor_r)\n\n# filter for correlations over 0.5\nsig &lt;- cor_r %&gt;% \n  rowwise() %&gt;%\n  mutate(pair = paste(sort(c(var_1, var_2)), collapse = \"_\")) %&gt;%\n  ungroup() %&gt;%\n  distinct(pair, .keep_all = TRUE) %&gt;%\n  select(-pair) %&gt;% \n  filter(!is.na(p), abs(value) &gt; 0.5)\n\n# Clean up columns for table\nsig &lt;- sig %&gt;% \n  mutate(\n    value = abs(value),\n    across(where(is.numeric), ~ format(round(.x, 3), nsmall = 3))\n  ) %&gt;% \n  setNames(c('Indicator 1', 'Indicator 2', 'Correlation', 'P Value'))\nget_str(sig)\n\ntable_out &lt;- get_reactable(sig)\n\n\n\n\n\n\n\n\nThe wealth/income distribution indicator (economics) is correlating strongly with several indicators, some from the economics dimension and some from health. Note that there are several metrics in that indicator related to median earnings, which might be a proxy for gdp per capita. Now that I look at this, it might be worth including gdp per capita at least as a control variable to see how much fo the variation it accounts for.\nIt looks like all the indicators from the carbon index (embodied, fluxes, stocks) correlate with one another, which makes enough sense. I imagine that one shouldn’t be too much of a problem if they are being aggregated at the index level anyway.\nForest health and carbon stocks are currently quite highly correlated, but this is because the metrics for carbon stocks are not ideal. The metrics for carbon stocks and forest health all come from the same TreeMap dataset. I suspect that if we include a better set of metrics for carbon stocks, this won’t be a such a problem.\nValue-added markets and operations diversification are all using a very similar set of metrics as well. They mostly come from NASS, and it would be worth digging into the NASS docs to see how whether value-added sales might overlap with agritourism, direct to consumer sales, or local marketing channel sales.\nFood affordability and food security also unsurprisingly correlate strongly. The current framework here is a work in progress and a bit haphazard. It will need some reworking. Curiously, these indicators also strongly correlate with participatory governance. That’s quite an interesting finding.\nAs for what to do about highly correlating indicators in general:\n\nThey could be reworked to use metrics that don’t lead to indicator correlations. This sounds rather difficult to me, and maybe impossible. It seems likely to be the reality that aspects of the economics and health dimensions are indeed related, for example.\nThey could be weighted in their respective dimensions to account for the correlations. This might be done with PCA loadings or by expert opinion.\nWe could also leave them as is. This would mean potentially double-counting certain aspects, but may be a reasonable approximation of reality.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Analysis",
      "Indicator Correlations"
    ]
  },
  {
    "objectID": "pages/framework_maps.html",
    "href": "pages/framework_maps.html",
    "title": "Map Explorer",
    "section": "",
    "text": "Exploring scores with maps.\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/comparison.html",
    "href": "pages/comparison.html",
    "title": "Comparison of Aggregation Methods",
    "section": "",
    "text": "In the last page we created six sets of scores by state based on combinations of three normalization methods (z-scores, min max, box cox) and two aggregation methods (arithmetic, geometric). Here, we will explore differences between them in terms of state distributions and rankings.\nNote that each set of spider plots are scaled to the minimum and maximum of any single state in that dimension, given the normalization and aggregation methods. This means in the case of min-max normalization, for example, raw metrics are scaled from 0 to 1, arithmetic and geometric means consolidate values to dimension scores, and these sets of dimension scores are scaled on the plot from the lowest to the highest value of any state. A “perfect” score here means that it is the best of any state. Plots show dimension values for Vermont in green. The dotted purple polygon behind it is the median of US states and DC. Arithmetic means are on the left, and geometric on the right.\nBe aware that spider/radar charts can be hard to interpret, and sometimes misleading The Radar Chart and its Caveats. The order of variables makes a big impact on the area of chart, and area is not a terribly reliable way to show differences, as it increases quadratically as variables increase linearly. Will explore some other ways to show this information, but using these for now as they are quite popular in the literature for sustainability metrics.",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table",
    "href": "pages/comparison.html#arithmetic-table",
    "title": "Comparison of Aggregation Methods",
    "section": "1.1 Arithmetic Table",
    "text": "1.1 Arithmetic Table\n\n\nCode\nget_reactable_scores(dat, 'minmax_arithmetic')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table",
    "href": "pages/comparison.html#geometric-table",
    "title": "Comparison of Aggregation Methods",
    "section": "1.2 Geometric Table",
    "text": "1.2 Geometric Table\n\n\nCode\nget_reactable_scores(dat, 'minmax_geometric')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table-1",
    "href": "pages/comparison.html#arithmetic-table-1",
    "title": "Comparison of Aggregation Methods",
    "section": "2.1 Arithmetic Table",
    "text": "2.1 Arithmetic Table\n\n\nCode\nget_reactable_scores(dat, 'zscore_arithmetic')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table-1",
    "href": "pages/comparison.html#geometric-table-1",
    "title": "Comparison of Aggregation Methods",
    "section": "2.2 Geometric Table",
    "text": "2.2 Geometric Table\n\n\nCode\nget_reactable_scores(dat, 'zscore_geometric')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table-2",
    "href": "pages/comparison.html#arithmetic-table-2",
    "title": "Comparison of Aggregation Methods",
    "section": "3.1 Arithmetic Table",
    "text": "3.1 Arithmetic Table\n\n\nCode\nget_reactable_scores(dat, 'boxcox_arithmetic')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table-2",
    "href": "pages/comparison.html#geometric-table-2",
    "title": "Comparison of Aggregation Methods",
    "section": "3.2 Geometric Table",
    "text": "3.2 Geometric Table\n\n\nCode\nget_reactable_scores(dat, 'boxcox_geometric')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table-3",
    "href": "pages/comparison.html#arithmetic-table-3",
    "title": "Comparison of Aggregation Methods",
    "section": "4.1 Arithmetic Table",
    "text": "4.1 Arithmetic Table\n\n\nCode\nget_reactable_scores(dat, 'rank_arithmetic')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table-3",
    "href": "pages/comparison.html#geometric-table-3",
    "title": "Comparison of Aggregation Methods",
    "section": "4.2 Geometric Table",
    "text": "4.2 Geometric Table\n\n\nCode\nget_reactable_scores(dat, 'rank_geometric')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table-4",
    "href": "pages/comparison.html#arithmetic-table-4",
    "title": "Comparison of Aggregation Methods",
    "section": "5.1 Arithmetic Table",
    "text": "5.1 Arithmetic Table\n\n\nCode\nget_reactable_scores(dat, 'winsor_arithmetic')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table-4",
    "href": "pages/comparison.html#geometric-table-4",
    "title": "Comparison of Aggregation Methods",
    "section": "5.2 Geometric Table",
    "text": "5.2 Geometric Table\n\n\nCode\nget_reactable_scores(dat, 'winsor_geometric')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/aggregation.html",
    "href": "pages/aggregation.html",
    "title": "Metric Aggregation",
    "section": "",
    "text": "1 Introduction\nExploring methods of aggregating data into index and dimension scores. To Add:\n\nPrimer on methods, cite (OECD 2008)\nExamples\n\ncite Schneider (Schneider et al. 2023)\n\nrank order comparisons only\ncompare to global weighted means by groups based on GDP\nmin max scaling to show distance from global groups\n\ncite Bene et al 2019 (Béné et al. 2019)\n\nBox cox for most skewed indicators (skew &gt; 2)\nthen min max\ngeometric means for enviro and economic dimensions\narithmetic means for social and food dimensions\ngeometric mean for combining all four dimensions into one\n\ncite Nicoletti (Nicoletti 2000)\n\nUse normaliaed square loadings (indicator weights) to weight each indicator\n\ncite Gomez Limon and Sanchez (Gómez-Limón and Sanchez-Fernandez 2010)\n\nmin max normalization\nAggregation - compared several different methods - mostly correlate, no big differences\n\nweighted sum of indicators\nproduct of weighted indicators\nmuilticriteria function based on distance to ideal point\n\nWeighting - did both PCA and analytic hierarchy process\nValidation (identifying important factors) - double censored tobit - index as dependent, indicators as independent\n\ncite demelash and aremu (Adamu Demelash and Abate Alemu 2024)\n\nNormalization - distance to reference\n\nrefernce determined by quartile analysis\nnot affected by outliers, extreme values\n\nWeighting - equal\nAggregation - linear\nAdditive method for indicators within dimensions\nGeometric means for aggregate scores across four dimensions\n\n\n\n\n\n2 Imputation\nFirst, check how much missing data there are. If it is within reason, use missForest algorithm to impute missing data (Stekhoven and Bühlmann 2012). This is particularly good at handling MAR data, and does a decent job at handling MNAR data and non-linear relationships as well. If less than 5% of data are missing, just about any method for handling it is reasonable, even listwise deletion (Beaujean 2013).\n\n\nCode\npacman::p_load(\n  missForest,\n  tibble,\n  dplyr\n)\n\nmetrics_df &lt;- readRDS('data/metrics_df.rds')\nget_str(metrics_df)\n\n# Check for missing data\nskimr::skim(metrics_df)\nmis_dat &lt;- sum(is.na(metrics_df))/(nrow(metrics_df)*(ncol(metrics_df) - 1)) * 100 \nmis_dat &lt;- round(mis_dat, 3)\n\n# Change fips from column to rowname so we can impute without losing it\nmetrics_df &lt;- metrics_df %&gt;% \n  column_to_rownames('fips')\nget_str(metrics_df)\n\n# Impute missing variables\nset.seed(42)\nmf_out &lt;- metrics_df %&gt;%\n  missForest(\n    ntree = 200,\n    mtry = 10,\n    verbose = FALSE,\n    variablewise = FALSE\n  )\n# get_str(mf_out)\n\n# Extract OOB error\n(oob &lt;- mf_out$OOBerror)\n\n# Check missing again\nskimr::skim(mf_out$ximp)\n# Looks good\n\n# Save just imputed data\nimp_dat &lt;- mf_out$ximp\n\n\nWe had 0.791% missing data, which is rather little, and gives us flexibility in handling it. The Out-of-Bag (OOB) error, quantified by the normalized residual mean squared error (NRMSE) the missForest imputation algorithm was 0.\n\n\n3 Normalization\nWe are normalizing our data using three methods: min-max, Box-Cox, and Z-scores. (We might also consider Winsorizing at some point.) Results will be saved to a list of three normalized datasets so we can compare outcomes of each one and see what the consequences are.\nMin Max (OECD 2008)\nMin-maxing scales all the data from 0 to 1 by subtracting the minimum value of each variable from all cases and dividing by the range of all cases in the variable. It is rather intuitive, as 1 is the best score, and 0 is the worst. This is a linear transformation, so the relationships between the values should not change.\n\\[\\begin{equation}\nI^t_qc = \\frac{x^t_qc - min_c(x^{t_0}_q)}{max_c(x^{t_0}_q)-min_c(x^{t_0}_q)}\n\\end{equation}\\]\nWhere \\(x^t_qc\\) is the metric \\(q\\) for state \\(c\\) at time \\(t\\).\nZ-Scores (OECD 2008)\nZ-scores are stardized to have a mean of 0 and a standard deviation of 1. Larger numbers are better, but there are no caps on the highest or lowest values. A value of 2 would mean that it is 2 standard deviations greater than the mean. Again, this is a linear transformation, so relationships between variables should not change.\n\\[\\begin{equation}\nI^t_{qc} = \\frac{x^t_{qc}-x^t_{qc=\\overline{c}}}{\\sigma^t_{qc=\\overline{c}}}\n\\end{equation}\\]\nWhere \\(x^t_qc\\) is the metric \\(q\\) for state \\(c\\) at time \\(t\\).\nBox Cox (Bickel and Doksum 1981)\nBox-Cox transformations are non-linear transformations that use an optimal value of lambda to make the distribution as normal as possible. This has some strengths in that the data are easier to work with in further analyses. It also effectively pulls outliers inward toward the center of the distribution. However, it also changes relationships between the variables, so it will distort any bivariate correlations.\n\\[\\begin{equation}\n{\\rm For}\\ \\lambda\\neq0,\\ f\\lambda(x) = (sign(x)|x|^\\lambda-1)/\\lambda\n\\end{equation}\\]\n\\[\\begin{equation}\n{\\rm For}\\ \\lambda = 0,\\ f_0(x) = log(x)\n\\end{equation}\\]\nRank Order\nWinsorization\n\n\nCode\npacman::p_load(\n  forecast,\n  DescTools,\n  purrr\n)\n\n# List of results\nnormed &lt;- list()\nget_str(imp_dat)\n\n# Z scores\nnormed$zscore &lt;- imp_dat %&gt;% \n  mutate(across(everything(), ~ as.numeric(scale(.x, scale = TRUE, center = TRUE))))\n\n# Min Max\nmin_max &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\nnormed$minmax &lt;- imp_dat %&gt;% \n  mutate(across(everything(), min_max))\n\n# Box Cox. Adding 1 as constant to remove zeroes\nnormed$boxcox &lt;- imp_dat %&gt;% \n  mutate(across(everything(), ~ forecast::BoxCox(.x + 1, lambda = 'auto')))\n\n# Rank order from lowest to highest value for each var. We are coding this such\n# that higher ranks are better. So 51 should have the highest/best value and\n# rank 1 should have the worst.\nnormed$rank &lt;- map(names(imp_dat), \\(col_name) {\n  imp_dat %&gt;% \n    rownames_to_column('fips') %&gt;% \n    select(fips, col_name) %&gt;% \n    mutate(!!sym(col_name) := dense_rank(.data[[col_name]]))\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  column_to_rownames('fips')\nget_str(normed$rank)\n# get_str(normed$rank[[1]])\n# normed$rank[[1]] %&gt;% arrange(unemploymentRate)\n\n# Winsorization\nnormed$winsor &lt;- imp_dat %&gt;% \n  mutate(across(everything(), Winsorize))\n\n# Check\nmap(normed, get_str)\n\n# Save this for later\nsaveRDS(normed, 'data/normalized_metrics_df.rds')\n\n\n\n\n4 Directional Values\nHere, we are assuming that each metric has a direction that is more sustainable than the opposite. Either more of it is better, or less of it is better. This is rather problematic in that just about any metric becomes negative with too much or too little of it. What might make more sense in the long run would be to consult the expertise of our teams and develop targets or acceptable ranges for some metrics once they are settled. Still, just about every sustainability indicator framework does some variation of this one-way value system (Schneider et al. 2023; Béné et al. 2019; Nicoletti 2000; Jacobi et al. 2020; Gómez-Limón and Sanchez-Fernandez 2010).\nAlas, for now we will invert variables in each of the transformed datasets as necessary so that larger numbers are more sustainable, and smaller numbers are less sustainable. The table below shows this assignment in the desirable column. For couple of variables (vacancy rate and animal sales as a percentage of all agricultural sales) I was not comfortable assigning one direction as better than the other, so I have removed them from the refined framework.\n\n\nCode\npacman::p_load(\n  reactable,\n  purrr\n)\nsource('dev/get_reactable.R')\n\n# map(normed, get_str)\n# (names &lt;- names(normed[[1]]))\n\n# Higher numbers should be better. Reverse metrics that are the opposite, \n# where lower numbers are better. Only listing reverse here - metrics are \n# implicitly better with larger numbers otherwise.\nreverse &lt;- c(\n  'unemploymentRate',\n  'gini',\n  'lowBirthweight',\n  'teenBirths',\n  'uninsured',\n  'incomeInequality',\n  'childrenInSingleParentHouseholds',\n  'injuryDeaths',\n  'airPollutionParticulateMatter',\n  'drinkingWaterViolations',\n  'severeHousingProblems',\n  'prematureAgeAdjustedMortality',\n  'infantMortality',\n  'frequentPhysicalDistress',\n  'frequentMentalDistress',\n  'diabetesPrevalence',\n  'hivPrevalence',\n  'limitedAccessToHealthyFoods',\n  'drugOverdoseDeaths',\n  'disconnectedYouth',\n  'residentialSegregationBlackWhite',\n  'suicides',\n  'motorVehicleCrashDeaths',\n  'severeHousingCostBurden',\n  'schoolSegregation',\n  'childCareCostBurden',\n  'wicPercEligible', # Iffy on this one\n  'droughtMeanPercArea',\n  'pctAtRiskAnimalSpp',\n  'pctAtRiskPlantSpp',\n  'pctAtRiskBeeSpp',\n  'pctAtRiskOrchidSpp',\n  'pctAtRiskEcosystems',\n  'expChemicalPct',\n  'ageProducers', # Could be better to use age diversity?\n  'waterIrrSrcOffFarmExp',\n  'waterIrrSrcOffFarmExpPerAcreFt',\n  'CH4FromAg',\n  'N2OFromAg',\n  'CO2FromAg',\n  'propAreaFsaSecDisasters',\n  'totalCapConsNoDwellings',\n  'totalIntExpRealEstateNoDwellings',\n  'totalIncomeInsuranceIndemnities',\n  'totalIncomeInsuranceIndemnitiesFederal',\n  'totalValueEmergPayments',\n  'totalValueOtherAdHocEmergPayments',\n  'totalValueDairyMarginProtPayments',\n  'totalValueAllLossCoveragePayments',\n  'totalValueAgRiskCoveragePayments',\n  'totalCapExpBldgsLandNoDwellings'\n)\n\n# Iffy: landValPF, landValPerAcre - in good column for now, but unclear\n# indemnities and emergency payments - in bad column for now, but more access\n# coud be good?\n\n# Some are unclear - without clear direction, better to remove:\nremove &lt;- c(\n  'vacancyRate',\n  'salesAnimalPctSales',\n  'expHiredLaborPercOpExp',\n  'acresPF',\n  'medianAcresPF',\n  'importsTopFive'\n)\n\n## Remove the unclear ones from all three datasets\n# Then for each transformation, flip values in a way that makes sense\n# zscore: multiple by -1, easy\n# minmax: 1 - x, easy\n# rank: nrow - x, easy\n# boxcox and winsor: trickier. want to just reverse the distribution. \n#   max(x) - x + min(x)\n# Could we have just done this in the beginning, before normalization? Maybe\nout &lt;- imap(normed, \\(df, method) {\n  df %&gt;% \n    select(-matches(remove)) %&gt;% \n    mutate(across(all_of(reverse), ~ case_when(\n      method == 'zscore' ~ .x * -1,\n      method == 'minmax' ~ 1 - .x,\n      method %in% c('boxcox', 'winsor') ~ max(.x) - .x + min(.x),\n      method == 'rank' ~ max(.x) - .x + 1\n    )))\n})\n# map(out, get_str)\n# map(normed, get_str)\n\n# Compare\n# checklist &lt;- list(normed, out)\n# map(checklist, ~ .x$rank[[2]])\n\n\n# Use the table from refined metrics here instead of making it again\n\n\n\n## Show table of which metrics were set in which direction\nsm_data &lt;- readRDS('data/sm_data.rds')\nmeta &lt;- sm_data$metadata\n\n# Reactable table showing var, metric, source, and direction\ntable &lt;- meta %&gt;% \n  dplyr::filter(variable_name %in% names(imp_dat)) %&gt;% \n  mutate(desirable = case_when(\n    variable_name %in% reverse ~ 'Lower',\n    variable_name %in% remove ~ 'Removed',\n    .default = 'Higher'\n  )) %&gt;% \n  select(\n    metric, \n    variable_name, \n    dimension,\n    index,\n    indicator,\n    desirable, \n    definition, \n    source\n  )\n  \ntable %&gt;% \n  get_reactable(\n    defaultPageSize = 5,\n    columns = list(\n      'definition' = colDef(\n        minWidth = 150\n      ),\n      'source' = colDef(\n        minWidth = 150\n      )\n    )\n  )\n\n\n\n\n\n\n\n\n5 Aggregation\nHere we are combining values in each indicator, index, and dimension using both arithmetic and geometric means (OECD 2008). Arithmetic means are fully compensable, in that a strong score in one area can make up for a weak score in another. Geometric means are only somewhat compensable - it effectively applies a penalty for unbalanced scores.\nWe might also consider PCA here, as we have done with the preliminary dimension metrics previously. But the n:p ratio is not in our favor for PCA as we have more metrics than states. Will revisit this, perhaps by splitting it up into dimensions again rather than trying the whole framework at once, or possible using a sparse PCA procedure that incorporates variable selection.\nWe will end up with 10 iterations of our data (5 normalization methods * 2 aggregation methods).\nIndicator aggregation:\n\n\nCode\n# We need to attach these back to framework from metadata\n# Filter frame from earlier down to our current metrics\nframe &lt;- readRDS('data/frame.rds')\nfiltered_frame &lt;- frame %&gt;% \n  dplyr::filter(variable_name %in% names(normed[[1]])) %&gt;% \n  dplyr::select(variable_name, indicator, index, dimension)\nget_str(filtered_frame)\n\n# Save this for later - use in regression and variable selection \nsaveRDS(filtered_frame, 'data/filtered_frame.rds')\n\n# Make a list where we hold scores for indicators, indices, and dimensions\nscores &lt;- list()\n\n# Function for geometric mean\ngeometric_mean &lt;- function(x, na.rm = TRUE){\n  exp(sum(log(x[x &gt; 0]), na.rm = na.rm) / length(x))\n}\n\n# Get indicator scores across all three normalization methods\nindicator_scores &lt;- map(normed, \\(df) {\n  \n  # For each df, calculate indicator means\n  indicators_out &lt;- map(unique(filtered_frame$indicator), \\(ind) {\n  \n    # Column name based on indicator\n    ind_snake &lt;- ind\n    \n    # Split into groups by indicator, with one or more metrics each\n    variables &lt;- filtered_frame %&gt;% \n      dplyr::filter(indicator == ind) %&gt;% \n      pull(variable_name) %&gt;% \n      unique()\n    indicator_metrics &lt;- df %&gt;% \n      select(all_of(variables))\n    \n    # Get arithmetic and geo means for each indicator\n    dfs &lt;- list()\n    dfs$arithmetic &lt;- indicator_metrics %&gt;%\n      rowwise() %&gt;%\n      mutate(\n        !!sym(ind_snake) := mean(c_across(everything())),\n      ) %&gt;%\n      select(!!sym(ind_snake))\n    dfs$geometric &lt;- indicator_metrics %&gt;% \n      rowwise() %&gt;% \n      mutate(\n        !!sym(ind_snake) := geometric_mean(c_across(everything())),\n      ) %&gt;%\n      select(!!sym(ind_snake))\n    return(dfs) \n  })\n  \n  # Rearrange so we put each aggregation method (arith, geo) together\n  norm_out &lt;- list()\n  norm_out$arithmetic &lt;- map(indicators_out, ~ {\n    .x[grep(\"arithmetic\", names(.x))]\n  }) %&gt;% \n    bind_cols()\n  norm_out$geometric &lt;- map(indicators_out, ~ {\n    .x[grep(\"geometric\", names(.x))]\n  }) %&gt;% \n    bind_cols()\n  return(norm_out) \n})\n  \nget_str(indicator_scores, 3)\nget_str(indicator_scores, 4)\n\n# Test function\n# test &lt;- get_agg_indicators(normed, filtered_frame)\n# identical(indicator_scores, test)\n\n\nIndex aggregation:\n\n\nCode\n# For each set of indicator scores, calculate index scores\n# get_str(indicator_scores, 4)\nindices &lt;- unique(filtered_frame$index)\n\n# Choose aggregation function based on agg_type\nagg_function &lt;- function(x, agg_type) {\n   if (agg_type == 'geometric') {\n    geometric_mean(x)\n  } else if (agg_type == 'arithmetic') {\n    mean(x)\n  }\n}\n\nindex_scores &lt;- map(indicator_scores, \\(norm_type) {\n  imap(norm_type, \\(agg_df, agg_type) {\n    map(indices, \\(index_) {\n      # Get names of indicators for this index\n      index_indicators &lt;- filtered_frame %&gt;% \n        filter(index == index_) %&gt;% \n        pull(indicator) %&gt;% \n        unique()\n      # Get DF of indicators for this index\n      index_indicator_df &lt;- agg_df %&gt;% \n        select(all_of(index_indicators))\n      # Get arithmetic or geometric mean, based on agg_type\n      index_indicator_df %&gt;% \n        rowwise() %&gt;% \n        # mutate(mean = across(everything(), agg_function(agg_type)))\n        mutate(!!sym(index_) := agg_function(c_across(everything()), agg_type)) %&gt;% \n        select(!!sym(index_))\n    }) %&gt;% \n      bind_cols()\n  })\n})\nget_str(index_scores, 4)\n\n# Test function\n# test_indices &lt;- get_agg_indices(indicator_scores, frame)\n# identical(index_scores, test_indices)\n\n\nDimension aggregation:\n\n\nCode\nget_str(index_scores, 4)\n\n# Same process for dimensions\ndimensions &lt;- unique(filtered_frame$dimension)\n\ndimension_scores &lt;- map(index_scores, \\(norm_type) {\n  imap(norm_type, \\(agg_df, agg_type) {\n    map(dimensions, \\(dimension_) {\n      # Get names of indices for this dimension\n      dimension_indices &lt;- filtered_frame %&gt;% \n        filter(dimension == dimension_) %&gt;% \n        pull(index) %&gt;% \n        unique()\n      # Get DF of indice for this dimension\n      dimension_index_df &lt;- agg_df %&gt;% \n        select(all_of(dimension_indices))\n      # Get arithmetic or geometric mean, based on agg_type\n      dimension_index_df %&gt;% \n        rowwise() %&gt;% \n        mutate(!!sym(dimension_) := agg_function(\n          c_across(everything()), \n          agg_type\n        )) %&gt;% \n        select(!!sym(dimension_))\n    }) %&gt;% \n      bind_cols()\n  })\n})\nget_str(dimension_scores, 4)\n\n# Test function\n# test_dimensions &lt;- get_agg_dimensions(index_scores, filtered_frame)\n# identical(dimension_scores, test_dimensions)\n\n\n\n\n6 Wrangle\nHere, we organize arithmetic and geometric means for each level of the framework (indicator, index, dimension) in a way that is easier to work with. We also add means and medians for all US states as well as New England states that we can use as points of comparison.\n\n\nCode\npacman::p_load(\n  purrr\n)\n\nget_str(indicator_scores, 4)\nget_str(index_scores, 4)\nget_str(dimension_scores, 4)\n\n# Want to end up with 6 lists: 3 norm types * 2 mean types\n# Put them all together in one list to work with\nall_scores &lt;- mget(c(\n  'indicator_scores',\n  'index_scores',\n  'dimension_scores'\n))\nget_str(all_scores, 3)\n\n# Function to pull out the pieces we want\n# Also put state names back in as a column and with real names, not codes\nget_output &lt;- function(norm_type, agg_type) {\n  # Get list of each df (dimension, index, indicator) for combo\n  dfs &lt;- all_scores %&gt;% \n    map(\\(level) level[[norm_type]]) %&gt;% \n    map(\\(norm) norm[[agg_type]])\n  # Get state back into a proper column for each df\n  out &lt;- map(dfs, ~ {\n    .x %&gt;% \n      # Note that we are binding fips back in - this is hinky, note to fix\n      bind_cols(\n        metrics_df %&gt;% \n          rownames_to_column('fips') %&gt;% \n          dplyr::select(fips)\n      ) %&gt;% \n      left_join(\n        dplyr::select(sm_data$state_key, state, state_code),\n        by = join_by(fips == state_code) \n      ) %&gt;% \n      dplyr::select(-fips)\n  })\n  return(out)\n}\n\n# All combinations, also a name\ncombos &lt;- expand.grid(\n  names(all_scores[[1]]),\n  c('arithmetic', 'geometric')\n) %&gt;% \n  mutate(name = paste0(Var1, '_', Var2))\n\n# Map to pull them all out\nscores &lt;- map2(combos[[1]], combos[[2]], ~ {\n  get_output(.x, .y)\n}) %&gt;% \n  setNames(c(combos$name))\nget_str(scores, 4)\nget_str(scores, 3)\n\n# Test function\n# test_organized &lt;- get_organized_scores(all_scores, sm_data$state_key, metrics_df)\n# identical(scores, test_organized)\n\n\n## Add medians for New England states and US\nfinal_scores &lt;- map(scores, \\(method) {\n  map(method, \\(level) {\n    \n    # Mean of every US state and DC\n    us_means &lt;- level %&gt;%\n      dplyr::select(-state) %&gt;% \n      colMeans() %&gt;% \n      as.list()\n    us_means$state &lt;- 'US_mean'\n    \n    # Median of every US state and DC\n    us_medians &lt;- level %&gt;% \n      dplyr::select(-state) %&gt;% \n      map_dbl(median) %&gt;% \n      as.list()\n    us_medians$state &lt;- 'US_median'\n    \n    # Mean of just New England states\n    ne_means &lt;- level %&gt;% \n      dplyr::filter(state %in% c('VT', 'NH', 'ME', 'MA', 'CT', 'RI')) %&gt;% \n      dplyr::select(-state) %&gt;% \n      colMeans() %&gt;% \n      as.list()\n    ne_means$state &lt;- 'NE_mean'\n     \n    # Median of just New England states\n    ne_medians &lt;- level %&gt;% \n      dplyr::filter(state %in% c('VT', 'NH', 'ME', 'MA', 'CT', 'RI')) %&gt;% \n      dplyr::select(-state) %&gt;% \n      map_dbl(median) %&gt;% \n      as.list()\n    ne_medians$state &lt;- 'NE_median'\n    \n    # Return the level + US + NewEng means\n    level %&gt;% \n      bind_rows(us_means) %&gt;% \n      bind_rows(us_medians) %&gt;% \n      bind_rows(ne_means) %&gt;% \n      bind_rows(ne_medians)\n  })\n})\nget_str(final_scores, 3)\nget_str(final_scores, 4)\n\n# Test function\n# test_final_scores &lt;- get_groupings(scores)\n# get_str(test_final_scores)\n# get_str(test_final_scores, 4)\n# identical(final_scores, test_final_scores)\n# Not same because we added NE medians. That's okay.\n\n# Save this for use elsewhere\nsaveRDS(final_scores, 'data/state_score_iterations.rds')\n\n\nThis gives us a list of 10 elements, one for each combination of normalization method and aggregation method. Each element has three data frames, one for indicator, index, and dimension. Now we can compare these 6 outputs to see how the methodological differences affect scores and ranks.\n\n\n\n\n\n\n\n\n Back to top7 References\n\nAdamu Demelash, Sewareg, and Esubalew Abate Alemu. 2024. “Measuring Food System Sustainability in Ethiopia: Towards a Multi-Dimensional Perspective.” Ecological Indicators 161 (April): 111991. https://doi.org/10.1016/j.ecolind.2024.111991.\n\n\nBeaujean, A. Alexander. 2013. “Factor Analysis Using R.” https://doi.org/10.7275/Z8WR-4J42.\n\n\nBéné, Christophe, Steven D. Prager, Harold A. E. Achicanoy, Patricia Alvarez Toro, Lea Lamotte, Camila Bonilla, and Brendan R. Mapes. 2019. “Global Map and Indicators of Food System Sustainability.” Scientific Data 6 (1): 279. https://doi.org/10.1038/s41597-019-0301-5.\n\n\nBickel, Peter J., and Kjell A. Doksum. 1981. “An Analysis of Transformations Revisited.” Journal of the American Statistical Association 76 (374): 296–311. https://doi.org/10.1080/01621459.1981.10477649.\n\n\nGómez-Limón, José A., and Gabriela Sanchez-Fernandez. 2010. “Empirical Evaluation of Agricultural Sustainability Using Composite Indicators.” Ecological Economics 69 (5): 1062–75. https://doi.org/10.1016/j.ecolecon.2009.11.027.\n\n\nJacobi, Johanna, Stellah Mukhovi, Aymara Llanque, Markus Giger, Adriana Bessa, Christophe Golay, Chinwe Ifejika Speranza, et al. 2020. “A New Understanding and Evaluation of Food Sustainability in Six Different Food Systems in Kenya and Bolivia.” Scientific Reports 10 (1): 19145. https://doi.org/10.1038/s41598-020-76284-y.\n\n\nNicoletti, Giuseppe. 2000. “Summary Indicators of Product Market Regulation with an Extension to Employment Protection Legislation.” {{OECD Economics Department Working Papers}} 226. Vol. 226. OECD Economics Department Working Papers. https://doi.org/10.1787/215182844604.\n\n\nOECD. 2008. Handbook on Constructing Composite Indicators: Methodology and User Guide. Paris: Organisation for Economic Co-operation and Development.\n\n\nSchneider, Kate R., Jessica Fanzo, Lawrence Haddad, Mario Herrero, Jose Rosero Moncayo, Anna Herforth, Roseline Remans, et al. 2023. “The State of Food Systems Worldwide in the Countdown to 2030.” Nature Food 4 (12): 1090–110. https://doi.org/10.1038/s43016-023-00885-9.\n\n\nStekhoven, Daniel J., and Peter Bühlmann. 2012. “MissForest—Non-Parametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics 28 (1): 112–18. https://doi.org/10.1093/bioinformatics/btr597.",
    "crumbs": [
      "Analysis",
      "Aggregation"
    ]
  }
]