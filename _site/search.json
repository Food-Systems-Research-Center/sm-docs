[
  {
    "objectID": "pages/metric_counts.html",
    "href": "pages/metric_counts.html",
    "title": "Metric Counts",
    "section": "",
    "text": "Here we will see how much the dimension scores for Vermont change when we reduce certain indicators with many metrics to a single metric. We do this for an indicator in every dimension except social, which really doesn’t have any metrics to spare currently.\nNote that this should be redone more systematically and across all metrics to compare them at some point, rather than a picking just a few. On to-do list.\n\n\nCode\n# Load data for aggregations\nstate_key &lt;- readRDS('data/sm_data.rds')[['state_key']]\nmetrics_df &lt;- readRDS('data/metrics_df.rds')\nnormed_data &lt;- readRDS('data/normalized_metrics_df.rds')\nframework &lt;- readRDS('data/filtered_frame.rds')\n\n# Test out process\noriginal_scores &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df\n)\n\n\n\n1 Reduce Physical Health Metrics\nFirst let’s take the physical health tbd indicator and reduce it to only lifeExpectancy.\n\n\nCode\n# Remove everything from physical health tbd except life expectancy\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'physical health tbd', \n    variable_name != 'lifeExpectancy'\n  ) %&gt;% \n  dplyr::pull(variable_name)\n\n# Get dimension scores\nto_life &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\nNow the same for drugOverdoseDeaths\n\n\nCode\n# Remove everything from physical health tbd except life expectancy\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'physical health tbd', \n    variable_name != 'drugOverdoseDeaths'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_overdose &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\nFinally, we can compare the health dimension score and rank for the original framework against the physical health tbd indicator as represented by only life expectancy or only drug overdoses.\n\n\nCode\ndf &lt;- map_dfr(list(original_scores, to_life, to_overdose), ~ {\n  .x$minmax_geometric$dimension_scores %&gt;% \n    as.data.frame() %&gt;%\n    dplyr::filter(str_length(state) == 2) %&gt;% \n    mutate(\n      rank = dense_rank(health),\n    ) %&gt;% \n    dplyr::filter(state == 'VT') %&gt;% \n    dplyr::select(health, rank)\n}) %&gt;% \n  mutate(\n    health = round(health, 3),\n    iteration = c('Original', 'Life Exp Only', 'Overdoses Only')\n  )\n\n\n\n\n\n\n\n\n\n\n2 Reduce Crop Failure Metrics\nFor production, we will reduce the crop failure indicator to only the value of dairy margin protection payments and then income from insurance indemnities.\n\n\nCode\nget_str(normed_data)\nget_str(framework)\n\n## Reduce crop failure indicator to totalValueDairyMarginProtPayments\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'crop failure', \n    variable_name != 'totalValueDairyMarginProtPayments'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_dairy &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Reduce crop failure indicator to totalIncomeInsuranceIndemnities\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'crop failure', \n    variable_name != 'totalIncomeInsuranceIndemnities'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_insurance &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Put them together\nprod_df &lt;- map_dfr(list(original_scores, to_dairy, to_insurance), ~ {\n  .x$minmax_geometric$dimension_scores %&gt;% \n    as.data.frame() %&gt;%\n    dplyr::filter(str_length(state) == 2) %&gt;% \n    mutate(\n      rank = dense_rank(production),\n    ) %&gt;% \n    dplyr::filter(state == 'VT') %&gt;% \n    dplyr::select(production, rank)\n}) %&gt;% \n  mutate(\n    production = round(production, 3),\n    iteration = c('Original', 'Dairy Only', 'Insurance Only')\n  )\n\n\n\n\n\n\n\n\n\n\n3 Reduce Biodiversity Metrics\nNow for the environment dimension, we will reduce the biodiversity indicator from its current set of 8 species down to the percentage of animal species at risk and the percentage of plant species at risk.\n\n\nCode\nget_str(normed_data)\nget_str(framework)\n\n## Reduce crop failure indicator to pctAtRiskAnimalSpp\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'biodiversity', \n    variable_name != 'pctAtRiskAnimalSpp'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_animal &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Reduce biodiversity indicator to pctAtRiskPlantSpp\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'crop failure', \n    variable_name != 'pctAtRiskPlantSpp'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_plant &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Put them together\nenv_df &lt;- map_dfr(list(original_scores, to_animal, to_plant), ~ {\n  .x$minmax_geometric$dimension_scores %&gt;% \n    as.data.frame() %&gt;%\n    dplyr::filter(str_length(state) == 2) %&gt;% \n    mutate(\n      rank = dense_rank(environment),\n    ) %&gt;% \n    dplyr::filter(state == 'VT') %&gt;% \n    dplyr::select(environment, rank)\n}) %&gt;% \n  mutate(\n    environment = round(environment, 3),\n    iteration = c('Original', 'Animal Spp Only', 'Plant Spp Only')\n  )\n\n\n\n\n\n\n\n\n\n\n4 Reduce Wealth/Income Metrics\nFor the economics dimension, we will reduce the wealth/income distribution indicator down to unemployment rate and then gini index.\n\n\nCode\nget_str(normed_data)\nget_str(framework)\n\n## Reduce wealth/income indicator to unemploymentRate\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'wealth/income distribution', \n    variable_name != 'unemploymentRate'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_unemployment &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Reduce wealth/income indicator to gini index\nto_remove &lt;- framework %&gt;% \n  dplyr::filter(\n    indicator == 'wealth/income distribution', \n    variable_name != 'gini'\n  ) %&gt;% \n  pull(variable_name)\n\n# Get dimension scores\nto_gini &lt;- get_all_aggregations(\n  normed_data = normed_data['minmax'],\n  framework = framework,\n  state_key = state_key,\n  metrics_df = metrics_df,\n  to_remove = to_remove\n)\n\n\n## Put them together\necon_df &lt;- map_dfr(list(original_scores, to_unemployment, to_gini), ~ {\n  .x$minmax_geometric$dimension_scores %&gt;% \n    as.data.frame() %&gt;%\n    dplyr::filter(str_length(state) == 2) %&gt;% \n    mutate(\n      rank = dense_rank(economics),\n    ) %&gt;% \n    dplyr::filter(state == 'VT') %&gt;% \n    dplyr::select(economics, rank)\n}) %&gt;% \n  mutate(\n    economics = round(economics, 3),\n    iteration = c('Original', 'Unemployment', 'Gini')\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Analysis",
      "Metric Counts"
    ]
  },
  {
    "objectID": "pages/sensitivity.html",
    "href": "pages/sensitivity.html",
    "title": "Sensitivity and Uncertainty",
    "section": "",
    "text": "Steps",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "temp/test_preso.html",
    "href": "temp/test_preso.html",
    "title": "Habits",
    "section": "",
    "text": "Turn off alarm\nGet out of bed"
  },
  {
    "objectID": "temp/test_preso.html#getting-up",
    "href": "temp/test_preso.html#getting-up",
    "title": "Habits",
    "section": "",
    "text": "Turn off alarm\nGet out of bed"
  },
  {
    "objectID": "temp/test_preso.html#going-to-sleep",
    "href": "temp/test_preso.html#going-to-sleep",
    "title": "Habits",
    "section": "2 Going to sleep",
    "text": "2 Going to sleep\n\nGet in bed\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\nCount sheep\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "temp/test_preso.html#testing-a-plot",
    "href": "temp/test_preso.html#testing-a-plot",
    "title": "Habits",
    "section": "3 Testing a plot",
    "text": "3 Testing a plot\n\n\n\nSome points before plot\nanother point before the plot"
  },
  {
    "objectID": "pages/sm-explorer.html",
    "href": "pages/sm-explorer.html",
    "title": "SM-Explorer",
    "section": "",
    "text": "Caution\n\n\n\n\n\nThe SM-Explorer is a work in progress. There are a small heap of bugs I’m already aware of, and about a hundred things I’d still like to add. If/when you find things that aren’t working properly, please feel free to let Chris know!\n\n\n\nThis is a Shiny app that allows for interactive exploration of metrics, mostly at the county level. It includes a map page, a bivariate plot explorer, and a metadata table much like what is included in this Quarto doc. It tends to work best if you open it in its own page using the button below:\n\n\n\n\nGo To SM-Explorer\n\n\n\n\nYou can also just use it here in the window. Note that some functions (like the full screen button) won’t work here.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Secondary Data",
      "SM-Explorer"
    ]
  },
  {
    "objectID": "pages/refine_production.html",
    "href": "pages/refine_production.html",
    "title": "Production Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the production dimension. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting.",
    "crumbs": [
      "Indicator Refinement",
      "Production"
    ]
  },
  {
    "objectID": "pages/refine_production.html#indicator-progression",
    "href": "pages/refine_production.html#indicator-progression",
    "title": "Production Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework for the dimension as described in the Wiltshire et al. paper.\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/wiltshire_tree.csv',\n  dimension_in = 'Production',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/matrix_tree.csv',\n  dimension_in = 'Production',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)",
    "crumbs": [
      "Indicator Refinement",
      "Production"
    ]
  },
  {
    "objectID": "pages/refine_production.html#survey",
    "href": "pages/refine_production.html#survey",
    "title": "Production Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the production indicator refinement meeting on January 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/prod_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    ends_with('GROUP'),\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not'\n  )) %&gt;% \n  .[-c(1:2), ]\n\nto_df &lt;- function(x) {\n  if (all(is.na(x))) {\n    return(NULL)\n  } else {\n   x %&gt;%\n    str_remove(' \\\\(joint indicator with Marketability\\\\)') %&gt;%\n    str_remove('\\\\*.*') %&gt;%\n    str_remove(' \\\\(see notes with questions') %&gt;%\n    str_split(',(?!\\\\s)') %&gt;% # Split on comma not followed by a space\n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n     arrange(desc(freq))\n  }\n}\n\nindi_out &lt;- map(dat[1:4], to_df)\nidx_out &lt;- map(dat[5:8], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n})\n\ngraph_table &lt;- graph_table %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n# Note some missing data throws off the graph table. Fix it here\ngraph_table_clean &lt;- graph_table %&gt;% \n  mutate(\n    sort_key = case_when(\n      str_detect(indicator, 'Production Species Diversity') ~ 3e6,\n      str_detect(indicator, 'Not livestock specific') ~ 1010002,\n      .default = sort_key\n    )\n  )\n\n\n\n\nCode\nggplot(graph_table_clean, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so “Must Include” is worth 3 points, “Probably Include” is worth 2 points, “Probably Not Include” is worth 1 point, and “Must Not Include” is worth 0 points. Note that the last column is the sum of proportions of “Must Include” and “Probably Include”. You can sort, search, expand, or page through the table below.\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\n# Add scores by multipliers\nmultipliers &lt;- c(3:1)\nidx_tables &lt;- map2(idx_out[1:3], multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    sort_key = ifelse(str_detect(index, 'Carbon'), 5e6, sort_key),\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:probably_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      # \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\"\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\"\n    )\n  )",
    "crumbs": [
      "Indicator Refinement",
      "Production"
    ]
  },
  {
    "objectID": "pages/refine_economics.html",
    "href": "pages/refine_economics.html",
    "title": "Economic Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the economics dimensions. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting. A final set of indicators to incorporate into the next RFP is still in the works!",
    "crumbs": [
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refine_economics.html#indicator-progression",
    "href": "pages/refine_economics.html#indicator-progression",
    "title": "Economic Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework as described in the Wiltshire et al. paper.\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_wiltshire_tree.csv',\n  dimension_in = 'economics',\n  include_metrics = FALSE,\n  y_limits = c(-1.5, 2.1)\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_tree.csv',\n  dimension_in = 'economics',\n  y_limits = c(-1.5, 2.1)\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Refinement Meeting\nFinally, the tentative set of indicators following the indicator refinement meeting on November 15th, 2024\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/econ_meeting_tree.csv',\n  dimension_in = 'economics',\n  y_limits = c(-1.5, 2.1)\n)",
    "crumbs": [
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refine_economics.html#survey",
    "href": "pages/refine_economics.html#survey",
    "title": "Economic Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the economic indicator refinement meeting on November 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/econ_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    starts_with('Q'),\n    -ends_with('RANK')\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    paste0('add_indi_', 1:3),\n    'notes',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not',\n    paste0('add_idx_', 1:3),\n    'idx_notes',\n    'final_notes'\n  )) %&gt;% \n  .[-c(1:2), ]\n\ngroups &lt;- select(dat, indi_must:indi_must_not, idx_must:idx_probably_not)\n\nto_df &lt;- function(x) {\n  x %&gt;% \n    str_split(',') %&gt;% \n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n    arrange(desc(freq))\n}\n\nindi_out &lt;- map(groups[1:4], to_df)\nidx_out &lt;- map(groups[5:7], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\n\n\nCode\nggplot(graph_table, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so “Must Include” is worth 3 points, “Probably Include” is worth 2 points, “Probably Not Include” is worth 1 point, and “Must Not Include” is worth 0 points. Note that the last column is the sum of proportions of “Must Include” and “Probably Include”. You can sort, search, expand, or page through the table below.\n\n\nCode\n# Add category to tables\nprops &lt;- ind_tables %&gt;% \n  imap(~ .x %&gt;% mutate(cat = .y)) %&gt;% \n  bind_rows() %&gt;% \n  select(-score)\n \n# Get proportion of probably include OR must include\nprop_prob_or_must_include &lt;- props %&gt;% \n  filter(cat %in% c('indi_must', 'indi_probably')) %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(prop_include = sum(freq) / 6) %&gt;% \n  arrange(desc(prop_include))\n\n# Get proportion of must include\nprop_must_include &lt;- props %&gt;% \n  filter(cat == 'indi_must') %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(prop_must = sum(freq) / 6) %&gt;% \n  arrange(desc(prop_must))\n\n# Add up weighted scores\nind_scores &lt;- ind_tables %&gt;% \n  bind_rows() %&gt;% \n  group_by(indicator) %&gt;% \n  summarize(score = sum(score, na.rm = TRUE)) %&gt;% \n  arrange(desc(score))\n\n# Join everything together\nscores_table &lt;- ind_scores %&gt;% \n  full_join(prop_must_include) %&gt;% \n  full_join(prop_prob_or_must_include) %&gt;% \n  arrange(desc(score)) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    across(c(3:4), ~ format(round(.x, 2), nsmall = 2))\n  ) %&gt;% \n  setNames(c('Indicator', 'Score', 'Proportion Must Include', 'Proportion Must OR Probably Include'))\n\n\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\nidx_out &lt;- map(groups[5:7], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:1)\nidx_tables &lt;- map2(idx_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:probably_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')[2:4]\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 16),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\"\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nThe indices are going through the same treatment as indicators above - scored from 3 to 0. Note that there were no indices rated as “Must Not Include”.",
    "crumbs": [
      "Indicator Refinement",
      "Economics"
    ]
  },
  {
    "objectID": "pages/refined_framework.html",
    "href": "pages/refined_framework.html",
    "title": "Refined Secondary Data Framework",
    "section": "",
    "text": "This page shows the partially refined framework as it stands after three dimension meetings: economics, environment, and production. It also includes a selection of preliminary secondary data metrics to match those indicators. We have collected around 1500 metrics so far, although many of those are fluff. Effectively, we have around 600 meaningful metrics. Here, we are using a selection of ~129 of them to make a preliminary framework for preliminary analyses. This is more than we have been planning for the refined framework, which will give us a chance to see how aggregate scores change with different combinations of metrics and under different methods of aggregation.\nNote that where I have no metrics to represent an indicator, I have added placeholders of the format NONE_#. This does not mean that secondary data do not exist, just that I either haven’t found it or haven’t cleaned and wrangled it yet. At the time of writing, there are several of the latter, including crop failure, access to care, racial diversity, and others. If you know of any secondary data to fill in the gaps or improve on data we already have, please do reach out to let Chris know about it.\nAt the bottom of this page is a metadata table with sources and definitions for all the metrics.",
    "crumbs": [
      "Analysis",
      "Refined Framework"
    ]
  },
  {
    "objectID": "pages/refined_framework.html#partially-refined-framework",
    "href": "pages/refined_framework.html#partially-refined-framework",
    "title": "Refined Secondary Data Framework",
    "section": "1 Partially Refined Framework",
    "text": "1 Partially Refined Framework\nHere is the framework with a selection of secondary metrics, split into each dimension for ease of reading.\n\n\nCode\npacman::p_load(\n  conflicted,\n  dplyr,\n  purrr,\n  stringr,\n  readr\n)\nsource('dev/get_dimension_ggraph.R')\n\n# Load refined framework\nsm_data &lt;- readRDS('data/sm_data.rds')\nraw_frame &lt;- sm_data[['refined_tree']]\n\n# Clean up the framework df \nframe &lt;- raw_frame %&gt;% \n  select(dimension:variable_name, use) %&gt;% \n  filter(use == 'x') %&gt;% \n  select(-use) %&gt;% \n  mutate(\n    metric = ifelse(\n      str_length(metric) &gt; 45,\n      paste0(str_sub(metric, end = 45), '...'),\n      metric\n    )\n  )\nget_str(frame)\n\n# Save frame to rds for use in subsequent scripts\nsaveRDS(frame, 'data/frame.rds')\n\n# Start a list to save outputs for preso\nplots &lt;- list()\n\n\n\n1.1 Environment\nWe have reasonable representation of the environment dimension, although some metrics are proxies that are stretched a bit too far. I do have biodiversity and sensitive habitat data, but still need to process it at the state level and add it to the collection. Some weak points are the carbon stocks indicator - so far, this is all from the TreeMap 2016 dataset. I would love to include other stocks of carbon if anyone has leads on datasets. The metrics for embodied carbon are also stretches.\nOne gap I’ve noticed since this dimension was reworked in the dimension meeting is that there is no direct treatment of soil health included anymore. We might add soil metrics for carbon stocks or forest health, but there is no clear home for it. And that being said, I have had no luck finding any reliable soil health datasets, so I’m all ears here too.\n\n\nCode\nsource('dev/get_dimension_ggraph.R')\nplots$environment &lt;- get_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'environment',\n  include_metrics = TRUE,\n  y_limits = c(-2, 3.25),\n  palette = \"ggthemes::stata_s2color\"\n)\nplots$environment\n\n\n\n\n\n\n\n\n\n\n\n1.2 Economics\nIt has definitely been easier to find economics data than other dimensions. Worth noting here is that the access to land indicator is not ideal. I’m using value and farm size as a proxy for access. Use of crop insurance is also a proxy, since I could not find direct insurance claim data from FSA. So for now, we are just using the ag secretary declarations of disasters that allow for insurance claims as a proxy.\n\n\nCode\nplots$economics &lt;- get_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'economics',\n  include_metrics = TRUE,\n  y_limits = c(-1.5, 3.1),\n  palette = \"ggthemes::stata_s2color\"\n)\nplots$economics\n\n\n\n\n\n\n\n\n\n\n\n1.3 Production\nAgricultural exports are a pretty robust dataset at the state level from ERS, although the import data only includes the values of the top five imports for each state - not ideal. Crop diversity is based on the Cropland Data Layer, a USDA NASS spatial model estimating of crop types, which I used to calculate Shannon diversity at the county and state level. The rest of the metrics come from NASS. Production is an area in which I feel better about using NASS data than some other dimensions, but there is still some risk of these data not representing VT farms appropriately.\n\n\nCode\nplots$production &lt;- get_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'production',\n  include_metrics = TRUE,\n  y_limits = c(-1.75, 3),\n  palette = \"ggthemes::stata_s2color\"\n)\nplots$production\n\n\n\n\n\n\n\n\n\n\n\n1.4 Health\nThe Food Environment Atlas has lots of data on access and nutrition, which accounts for much of the food security data, along with NASS. I threw in a slew of metrics for physical health under the temporary indicator name ‘physical health tbd’ just to differentiate it from the index. I also have a handful of established composite indices for health, including the UW County Health Rankings metrics for health factors (behavior, clinical care, social and economic factors, physical environment) and health outcomes (length of life, quality of life), as well as some established food security indices that are not included in this framework. I will instead use them to compare to dimensions scores as external validation in the Validation section.\n\n\nCode\nplots$health &lt;- get_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'health',\n  include_metrics = TRUE,\n  y_limits = c(-1.7, 3),\n  palette = \"ggthemes::stata_s2color\"\n)\nplots$health\n\n\n\n\n\n\n\n\n\n\n\n1.5 Social\nThe social dimension is admittedly slim, but it could have been a lot worse. The County Health Rankings dataset brings a few useful metrics here, like social associations and disconnected youth. Census participation and voter turnout are proxies for participatory governance in food systems - I can’t imagine finding something much more specific than that at this point. I also plan on replacing mean producer age with a diversity index for age structure among producers.\n\n\nCode\nplots$social &lt;- get_dimension_ggraph(\n  framework_df = frame,\n  dimension_in = 'social',\n  include_metrics = TRUE,\n  y_limits = c(-1.7, 3),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n# Save list of plots for preso\nsaveRDS(plots, 'preso/plots/frameworks.rds')\n\nplots$social",
    "crumbs": [
      "Analysis",
      "Refined Framework"
    ]
  },
  {
    "objectID": "pages/refined_framework.html#metadata",
    "href": "pages/refined_framework.html#metadata",
    "title": "Refined Secondary Data Framework",
    "section": "2 Metadata",
    "text": "2 Metadata\nHere we pull out the set of 129 metrics from the larger collection and arrange them into a more functional, tidy dataframe:\n\n\nCode\n# pacman::p_load(\n#   dplyr,\n#   tidyr,\n#   tibble\n# )\n\n# Get latest year function\nsource('dev/data_pipeline_functions.R')\n\n# Load metrics data\nsm_data &lt;- readRDS('data/sm_data.rds')\nmetrics &lt;- sm_data$metrics\n\n# Load refined framework\nraw_tree &lt;- sm_data[['refined_tree']]\n\n# Load refined framework\nframe &lt;- readRDS('data/frame.rds')\n\n\n## Join with metadata to double check the resolution of our metrics\nmeta &lt;- sm_data$metadata\nget_str(meta)\n\ndat &lt;- frame %&gt;% \n  dplyr::filter(variable_name != 'NONE') %&gt;% \n  dplyr::select(variable_name) %&gt;% \n  left_join(meta, by = 'variable_name') %&gt;% \n  unique()\nget_str(dat)\n\n# Pull it from the actual metrics data\nmetrics &lt;- sm_data$metrics %&gt;% \n  dplyr::filter(\n    variable_name %in% frame$variable_name,\n    fips %in% sm_data$state_key$state_code\n  )\nget_str(metrics)\n\n# Filter to latest year for each metric, and pivot wider\n# Also removing census participation - don't really have data at state level\n# Note to aggregate counties for this at some point\nmetrics_df &lt;- metrics %&gt;%\n  mutate(\n    value = ifelse(value == 'NaN', NA, value),\n    value = str_remove_all(value, ','),\n    value = as.numeric(value)\n  ) %&gt;%\n  get_latest_year() %&gt;% \n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;% \n  unnest(cols = !fips) %&gt;% \n  unique()\nget_str(metrics_df)\n\n# Let's get rid of the years so they are easier to work with\nnames(metrics_df) &lt;- str_split_i(names(metrics_df), '_', 1)\nget_str(metrics_df)\n\n# Also get rid of DC - too many missing values\nmetrics_df &lt;- metrics_df %&gt;% \n  dplyr::filter(fips != '11')\n\n# Save this for use in subsequent pages\nsaveRDS(metrics_df, 'data/metrics_df.rds')\n\n\nBelow, the metrics are displayed in a table that lets you browse and explore them.\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools\n)\n\n# Pull var names from metrics_df out of full metadata\nvars &lt;- unique(frame$variable_name) %&gt;% \n  str_subset('NONE', negate = TRUE)\n\n\n## Load metadata table, but keep framework from frame []\nmetadata &lt;- sm_data$metadata %&gt;% \n  select(-c(dimension, index, indicator)) %&gt;% \n  dplyr::filter(variable_name %in% vars)\n\n# Grab the framework variables from the frame to combine with metadata\nupdated_framework &lt;- frame %&gt;% \n  dplyr::select(variable_name, dimension, index, indicator)\n\n# Combine them\nmetadata &lt;- inner_join(metadata, updated_framework, by = 'variable_name')\n# get_str(metadata)\n\n\n## Pick out variables to display\nmetadata &lt;- metadata %&gt;% \n  select(\n    metric,\n    'Variable Name' = variable_name,\n    definition,\n    dimension,\n    index,\n    indicator,\n    units,\n    years = year,\n    'Year' = latest_year, # Renaming latest year as year, not including og year\n    source,\n    scope,\n    updates,\n    resolution,\n    url\n) %&gt;% \n  setNames(c(str_to_title(names(.))))\n\n# Save this for preso\nsaveRDS(metadata, 'preso/data/meta_for_table.rds')\n\n###\nhtmltools::browsable(\n  tagList(\n    \n    tags$div(\n      style = \"display: flex; gap: 16px; margin-bottom: 20px; justify-content: center;\",\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Show/hide more columns\"),\n        onclick = \"Reactable.setHiddenColumns('metadata_table', prevColumns =&gt; {\n          return prevColumns.length === 0 ? ['Definition', 'Scope', 'Resolution', 'Url'] : []\n        })\"\n      ),\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Download as CSV\"),\n        onclick = \"Reactable.downloadDataCSV('metadata_table', 'sustainability_metadata.csv')\"\n      )\n    ),\n    \n    reactable(\n      metadata[, which(names(metadata) != 'Years')],\n      sortable = TRUE,\n      resizable = TRUE,\n      filterable = TRUE,\n      searchable = TRUE,\n      pagination = TRUE,\n      bordered = TRUE,\n      wrap = TRUE,\n      rownames = FALSE,\n      onClick = 'select',\n      striped = TRUE,\n      pageSizeOptions = c(5, 10, 25, 50, 100),\n      defaultPageSize = 5,\n      showPageSizeOptions = TRUE,\n      highlight = TRUE,\n      style = list(fontSize = \"14px\"),\n      compact = TRUE,\n      fullWidth = TRUE,\n      columns = list(\n        Metric = colDef(\n          minWidth = 200,\n          sticky = 'left'\n        ),\n        'Variable Name' = colDef(\n          minWidth = 150\n        ),\n        Definition = colDef(\n          minWidth = 250\n        ),\n        'Latest Year' = colDef(minWidth = 75),\n        Source = colDef(minWidth = 250),\n        Scope = colDef(show = FALSE),\n        Resolution = colDef(show = FALSE),\n        Url = colDef(\n          minWidth = 300,\n          show = FALSE\n        )\n      ),\n      defaultColDef = colDef(minWidth = 100),\n      elementId = \"metadata_table\",\n      details = function(index) {\n        div(\n          style = \"padding: 15px; border: 1px solid #ddd; margin: 10px 0;\n             background-color: #ecf4ed; border-radius: 10px; border-color: black;\n             box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\",\n          \n          tags$h4(\n            strong(\"Details\"), \n          ),\n          tags$p(\n            strong('Metric Name: '), \n            as.character(metadata[index, 'Metric']),\n          ),\n          tags$p(\n            strong('Variable Name: '), \n            as.character(metadata[index, 'Variable Name']),\n          ),\n          tags$p(\n            strong('Definition: '), \n            as.character(metadata[index, 'Definition']),\n          ),\n          tags$p(\n            strong('Source: '), \n            as.character(metadata[index, 'Source'])\n          ),\n          tags$p(\n            strong('Latest Year: '), \n            as.character(metadata[index, 'Year'])\n          ),\n          tags$p(\n            strong('All Years (cleaned, wrangled, and included): '), \n            as.character(metadata[index, 'Years'])\n          ),\n          tags$p(\n            strong('Updates: '), \n            str_to_title(as.character(metadata[index, 'Updates']))\n          ),\n          tags$p(\n            strong('URL: '), \n            tags$a(\n              href = as.character(metadata[index, 'Url']),\n              target = '_blank',\n              as.character(metadata[index, 'Url'])\n            )\n          )\n        )\n      }\n    )\n  )\n)\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV",
    "crumbs": [
      "Analysis",
      "Refined Framework"
    ]
  },
  {
    "objectID": "pages/metric_distributions.html",
    "href": "pages/metric_distributions.html",
    "title": "Metric Distributions",
    "section": "",
    "text": "Explore metric distributions before normalization. Use this to inform how we might want to deal with outliers or normalize data at the metric level. For now, we are leaving the metrics as is, taking arithmetic means, and saving all normalization and weighting for the indicator level.\nTransforming our data from long format to wide and making sure everything came through alright.\n\n1 Distributions\nHere we explore univariate distributions of each of our metrics. Highly skewed distributions might be good candidates for Box-Cox transformations or Winsorization. The figure below shows metrics with a skew &gt; 2 in red, while those with a skew &lt; 2 are in blue.\n\n\nCode\npacman::p_load(\n  ggplot2,\n  purrr,\n  ggpubr,\n  psych,\n  tibble\n)\n\n# Load metrics_df (created in refined_framework page)\nmetrics_df &lt;- readRDS('data/metrics_df.rds')\n\n# Get skews of variables\nskewed &lt;- psych::describe(metrics_df[, -1]) %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column('variable_name') %&gt;% \n  dplyr::select(variable_name, skew) %&gt;% \n  dplyr::filter(abs(skew) &gt; 2) %&gt;% \n  pull(variable_name)\n\nplots &lt;- map(names(metrics_df)[-1], \\(var){\n  # color based on skewness\n  if (var %in% skewed) {\n    fill &lt;- 'red'\n    color &lt;- 'darkred'\n  } else {\n    fill &lt;- 'lightblue'\n    color &lt;- 'royalblue'\n  }\n  \n  # Make plot for variable\n  metrics_df %&gt;% \n    ggplot(aes(x = !!sym(var))) + \n    geom_density(\n      fill = fill,\n      color = color,\n      alpha = 0.5\n    ) +\n    theme_classic() +\n    theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n}) \n\n# Arrange them in 4 columns\nggarrange(\n  plotlist = plots,\n  ncol = 4,\n  nrow = 33\n)\n\n\n\n\n\nDistributions of metrics at the state level.\n\n\n\n\nIt seems most of our metrics fall along respectable somewhat-normal distributions. 28 metrics are skewed out of 129 total. They include several variables related to local farm economies (agrotourism sales as a percentage of total sales, direct to consumer sales as a percentage of total sales, and value added sales as a percentage of total sales), as well as a couple of the TreeMap 2016 variables (dead standing carbon and live trees) and GHG emissions from agriculture (CH4 and CO2, with an honorable mention for N2O). Just about the whole collection of ERS metrics are skewed, including importts and exports, indemnities, and real estate expenses.\nThere might be a case for transformations here, or it might make more sense to do it at the indicator level. Another option is to weight our metrics to take into account population, number of farms, acres of farmland, GDP, or some other appropriate variable for each metric. Béné et al. (2019) used Box Cox transformations for highly skewed indicators before normalizing all indicators with Min Max transformations.\nPutting a pin in this to consider trying it with raw metrics and transformed metrics and then to compare the two.\n\n\n\n\n\n Back to topReferences\n\nBéné, Christophe, Steven D. Prager, Harold A. E. Achicanoy, Patricia Alvarez Toro, Lea Lamotte, Camila Bonilla, and Brendan R. Mapes. 2019. “Global Map and Indicators of Food System Sustainability.” Scientific Data 6 (1): 279. https://doi.org/10.1038/s41597-019-0301-5.",
    "crumbs": [
      "Analysis",
      "Metric Distributions"
    ]
  },
  {
    "objectID": "pages/metadata_table.html",
    "href": "pages/metadata_table.html",
    "title": "Metadata Table",
    "section": "",
    "text": "This page contains a metadata table for exploring the sources of secondary data used throughout this project.\nUsing the table:\n\nClick column headers to sort\nGlobal search in the top right, or column search in each header\nChange page length and page through results at the bottom\nUse the download button to download a .csv file of the filtered table\nClick the arrow on the left of each row for details, including a URL to the data source.\n\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools\n)\n\n# Load full metadata table\nmetadata_all &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\n# Pick out variables to display\nmetadata &lt;- metadata_all %&gt;% \n  select(\n    metric,\n    'Variable Name' = variable_name,\n    definition,\n    dimension,\n    index,\n    indicator,\n    units,\n    'Year' = latest_year, # Renaming latest year as year, not including og year\n    source,\n    scope,\n    resolution,\n    url\n) %&gt;% \n  setNames(c(str_to_title(names(.))))\n\n###\nhtmltools::browsable(\n  tagList(\n    \n    tags$div(\n      style = \"display: flex; gap: 16px; margin-bottom: 20px; justify-content: center;\",\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Show/hide more columns\"),\n        onclick = \"Reactable.setHiddenColumns('metadata_table', prevColumns =&gt; {\n          return prevColumns.length === 0 ? ['Definition', 'Scope', 'Resolution', 'Url'] : []\n        })\"\n      ),\n      \n      tags$button(\n        class = \"btn btn-primary\",\n        style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n        tagList(fontawesome::fa(\"download\"), \"Download as CSV\"),\n        onclick = \"Reactable.downloadDataCSV('metadata_table', 'sustainability_metadata.csv')\"\n      )\n    ),\n    \n    reactable(\n      metadata,\n      sortable = TRUE,\n      resizable = TRUE,\n      filterable = TRUE,\n      searchable = TRUE,\n      pagination = TRUE,\n      bordered = TRUE,\n      wrap = TRUE,\n      rownames = FALSE,\n      onClick = 'select',\n      striped = TRUE,\n      pageSizeOptions = c(5, 10, 25, 50, 100),\n      defaultPageSize = 5,\n      showPageSizeOptions = TRUE,\n      highlight = TRUE,\n      style = list(fontSize = \"14px\"),\n      compact = TRUE,\n      fullWidth = TRUE,\n      columns = list(\n        Metric = colDef(\n          minWidth = 200,\n          sticky = 'left'\n        ),\n        'Variable Name' = colDef(\n          minWidth = 150\n        ),\n        Definition = colDef(\n          minWidth = 250\n        ),\n        'Latest Year' = colDef(minWidth = 75),\n        Source = colDef(minWidth = 250),\n        Scope = colDef(show = FALSE),\n        Resolution = colDef(show = FALSE),\n        Url = colDef(\n          minWidth = 300,\n          show = FALSE\n        )\n      ),\n      defaultColDef = colDef(minWidth = 100),\n      elementId = \"metadata_table\",\n      details = function(index) {\n        div(\n          style = \"padding: 15px; border: 1px solid #ddd; margin: 10px 0;\n             background-color: #E0EEEE; border-radius: 10px; border-color: black;\n             box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);\",\n          \n          tags$h4(\n            strong(\"Details\"), \n          ),\n          tags$p(\n            strong('Metric Name: '), \n            as.character(metadata_all[index, 'metric']),\n          ),\n          tags$p(\n            strong('Variable Name: '), \n            as.character(metadata_all[index, 'variable_name']),\n          ),\n          tags$p(\n            strong('Definition: '), \n            as.character(metadata_all[index, 'definition']),\n          ),\n          tags$p(\n            strong('Source: '), \n            as.character(metadata_all[index, 'source'])\n          ),\n          tags$p(\n            strong('Latest Year: '), \n            as.character(metadata_all[index, 'latest_year'])\n          ),\n          tags$p(\n            strong('All Years (cleaned, wrangled, and included): '), \n            as.character(metadata_all[index, 'year'])\n          ),\n          tags$p(\n            strong('Updates: '), \n            str_to_title(as.character(metadata_all[index, 'updates']))\n          ),\n          tags$p(\n            strong('URL: '), \n            tags$a(\n              href = as.character(metadata_all[index, 'url']),\n              target = '_blank',\n              as.character(metadata_all[index, 'url'])\n            )\n          )\n        )\n      }\n    )\n  )\n)\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Secondary Data",
      "Metadata"
    ]
  },
  {
    "objectID": "pages/indicator_correlations.html",
    "href": "pages/indicator_correlations.html",
    "title": "Indicator Correlations",
    "section": "",
    "text": "Code\npacman::p_load(\n  dplyr,\n  conflicted\n)\n\nconflicts_prefer(\n  dplyr::select(),\n  dplyr::filter(),\n  dplyr::summarize(),\n  .quiet = TRUE\n)\n\nsource('dev/get_reactable.r')\n\n\nThis page will explore correlations between variables at the indicator level.\n\n1 Correlation Matrix\n\n\nCode\npacman::p_load(\n  dplyr,\n  tidyr,\n  tibble,\n  stringr,\n  purrr,\n  tidyr,\n  ggplot2,\n  plotly,\n  reshape,\n  Hmisc,\n  viridisLite\n)\n\n# Load indicator data.\nfinal_scores &lt;- readRDS('data/state_score_iterations.rds')\nget_str(final_scores)\n\n# Get filtered frame subset to be able to color indicators by dimension later\nfiltered_frame &lt;- readRDS('data/filtered_frame.rds')\ninds_and_dims &lt;- filtered_frame %&gt;% \n  select(indicator, dimension) %&gt;%\n  unique() %&gt;% \n  mutate(color = case_when(\n    dimension == 'economics' ~ 'royalblue',\n    dimension == 'environment' ~ 'darkgreen',\n    dimension == 'health' ~ 'orange',\n    dimension == 'production' ~ 'darkred',\n    dimension == 'social' ~ 'black'\n  ))\ncolor_map &lt;- setNames(inds_and_dims$color, inds_and_dims$indicator)\n\n# Pull out minmax geo indicators only. Also use states only, no aggregates\nminmax_geo_indicators &lt;- final_scores$minmax_geometric$indicator_scores %&gt;% \n  filter(! state %in% c('US_mean', 'US_median', 'NE_median', 'NE_mean'))\nget_str(minmax_geo_indicators)\n\n# Make a correlation matrix using all the selected variables\nmat &lt;- minmax_geo_indicators %&gt;% \n  select(-state) %&gt;% \n  as.matrix()\n\n# Get correlations\ncor &lt;- rcorr(mat, type = 'pearson')\n\n# Melt correlation values and rename columns\ncor_r &lt;- melt(cor$r) %&gt;% \n  setNames(c('var_1', 'var_2', 'value'))\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Make heatmap with custom text aesthetic for tooltip\nplot_out &lt;- cor_r %&gt;% \n  ggplot(aes(var_1, var_2, fill = value, text = paste0(\n    'Var 1: ', var_1, '\\n',\n    'Var 2: ', var_2, '\\n',\n    'Correlation: ', format(round(value, 3), nsmall = 3), '\\n',\n    'P-Value: ', format(round(p.value, 3), nsmall = 3)\n  ))) + \n  geom_tile() + \n  scale_fill_gradient2(\n    low = \"#762a83\", \n    mid = \"white\", \n    high = \"#1b7837\", \n    midpoint = 0\n  ) +\n  theme(\n    axis.text.x = element_text(\n      # color = color_map[levels(factor(cor_r$var_1))],\n      hjust = 1, \n      angle = 45\n    )\n    # axis.text.y = element_text(\n      # color = color_map[levels(factor(cor_r$var_2))]\n    # )\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = NULL\n  )\n\n# Save this for preso\n# preso_matrix &lt;- list(mat, color_map)\nsaveRDS(mat, 'preso/data/correlation_data.rds')\n\n\n\n\nCode\n# Convert to interactive plotly figure with text tooltip\nplot &lt;- ggplotly(\n  plot_out, \n  tooltip = 'text',\n  # width = 1000,\n  # height = 800\n  width = 850,\n  height = 650\n)\n\nplot &lt;- plot %&gt;% add_trace(xaxis = 'x2', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(xaxis = 'x3', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(xaxis = 'x4', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(xaxis = 'x5', showscale = FALSE)\n\nplot &lt;- plot %&gt;%\n  plotly::layout(\n    xaxis = list(\n      range = list(0.5, 38.5),\n      tickvals = list(1, 2, 3, 4, 5),\n      tickfont = list(color = 'royalblue')\n    ),\n    xaxis2 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'x',\n      tickangle = -45,\n      ticktext = list(\n        'carbon fluxes',\n        'carbon stocks',\n        'embodied carbon',\n        'forest health',\n        'biodiversity',\n        'land use diversity',\n        'sensitive or rare habitats',\n        'water quality',\n        'water quantity'\n      ),\n      tickvals = list(6, 7, 8, 9, 10, 11, 12, 13, 14),\n      tickfont = list(\n        color = 'red',\n        family = 'Arial',\n        size = 12\n      )\n    ),\n    xaxis3 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'x',\n      tickangle = -45,\n      ticktext = list(\n        'educational attainment',\n        'access to culturally appropriate food',\n        'dietary quality',\n        'food access',\n        'food affordability',\n        'mental health tbd',\n        'access to care',\n        'housing supply and quality',\n        'physical health tbd'\n      ),\n      tickvals = list(15, 16, 17, 18, 19, 20, 21, 22, 23),\n      tickfont = list(\n        color = 'darkgreen',\n        family = 'Arial',\n        size = 12\n      )\n    ),\n    xaxis4 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'x',\n      tickangle = -45,\n      ticktext = list(\n        'total quantity exported',\n        'production species diversity',\n        'production inputs',\n        'total quantity food products',\n        'total quantity forest products',\n        'total quantity non-food ag products',\n        'value added market',\n        'crop failure'\n      ),\n      tickvals = list(24, 25, 26, 27, 28, 29, 30, 31),\n      tickfont = list(\n        color = 'darkorange',\n        family = 'Arial',\n        size = 12\n      )\n    ),\n    xaxis5 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'x',\n      tickangle = -45,\n      ticktext = list(\n        'social connectedness',\n        'community safety',\n        'diverse representation',\n        'age diversity',\n        'gender diversity',\n        'racial diversity',\n        'participatory governance'\n      ),\n      tickvals = list(32, 33, 34, 35, 36, 37, 38),\n      tickfont = list(\n        color = 'black',\n        family = 'Arial',\n        size = 12\n      )\n    )\n  )\n\nplot &lt;- plot %&gt;% add_trace(yaxis = 'y2', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(yaxis = 'y3', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(yaxis = 'y4', showscale = FALSE)\nplot &lt;- plot %&gt;% add_trace(yaxis = 'y5', showscale = FALSE)\n\nplot &lt;- plot %&gt;%\n  plotly::layout(\n    yaxis = list(\n      range = list(0.5, 38.5),\n      tickvals = list(1, 2, 3, 4, 5),\n      tickfont = list(color = 'royalblue')\n    ),\n    yaxis2 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'y',\n      tickangle = 0,\n      ticktext = list(\n        'carbon fluxes',\n        'carbon stocks',\n        'embodied carbon',\n        'forest health',\n        'biodiversity',\n        'land use diversity',\n        'sensitive or rare habitats',\n        'water quality',\n        'water quantity'\n      ),\n      tickvals = list(6, 7, 8, 9, 10, 11, 12, 13, 14),\n      tickfont = list(\n        color = 'red',\n        family = 'Arial',\n        size = 12\n      )\n    ),\n    yaxis3 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'y',\n      tickangle = 0,\n      ticktext = list(\n        'educational attainment',\n        'access to culturally appropriate food',\n        'dietary quality',\n        'food access',\n        'food affordability',\n        'mental health tbd',\n        'access to care',\n        'housing supply and quality',\n        'physical health tbd'\n      ),\n      tickvals = list(15, 16, 17, 18, 19, 20, 21, 22, 23),\n      tickfont = list(\n        color = 'darkgreen',\n        family = 'Arial',\n        size = 12\n      )\n    ),\n    yaxis4 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'y',\n      tickangle = 0,\n      ticktext = list(\n        'total quantity exported',\n        'production species diversity',\n        'production inputs',\n        'total quantity food products',\n        'total quantity forest products',\n        'total quantity non-food ag products',\n        'value added market',\n        'crop failure'\n      ),\n      tickvals = list(24, 25, 26, 27, 28, 29, 30, 31),\n      tickfont = list(\n        color = 'darkorange',\n        family = 'Arial',\n        size = 12\n      )\n    ),\n    yaxis5 = list(\n      range = list(0.5, 38.5),\n      overlaying = 'y',\n      tickangle = 0,\n      ticktext = list(\n        'social connectedness',\n        'community safety',\n        'diverse representation',\n        'age diversity',\n        'gender diversity',\n        'racial diversity',\n        'participatory governance'\n      ),\n      tickvals = list(32, 33, 34, 35, 36, 37, 38),\n      tickfont = list(\n        color = 'black',\n        family = 'Arial',\n        size = 12\n      )\n    )\n  )\n\n# Save this for preso\nhtmlwidgets::saveWidget(plot, 'preso/plots/correlation_plotly.html')\n\nplot\n\n\n\n\nInteractive Correlation Plot\n\n\n\n\n2 Strong Correlations\nWe have many significant correlations between indicators, but we probably don’t care too much about weak correlations. Let’s isolate the correlations that are significant and &gt; 0.5. These are the ones that might suggest we are double-counting certain aspects of the food system.\n\n\nCode\npacman::p_load(\n  reactable,\n  Hmisc\n)\n\n# Isolate all significant correlations\nget_str(cor_r)\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Add p values to dataframe with correlations\ncor_r$p &lt;- cor_p$value\nget_str(cor_r)\n\n# filter for correlations over 0.5\nsig &lt;- cor_r %&gt;% \n  rowwise() %&gt;%\n  mutate(pair = paste(sort(c(var_1, var_2)), collapse = \"_\")) %&gt;%\n  ungroup() %&gt;%\n  distinct(pair, .keep_all = TRUE) %&gt;%\n  select(-pair) %&gt;% \n  filter(!is.na(p), abs(value) &gt; 0.5)\n\n# Clean up columns for table\nsig &lt;- sig %&gt;% \n  mutate(\n    value = abs(value),\n    across(where(is.numeric), ~ format(round(.x, 3), nsmall = 3))\n  ) %&gt;% \n  setNames(c('Indicator 1', 'Indicator 2', 'Correlation', 'P Value'))\nget_str(sig)\n\n# Get reactable table for next cell\ntable_out &lt;- get_reactable(sig)\n\n# But while we're at it, count how many times each indicator appears\ncor_counts &lt;- c(sig[[1]], sig[[2]]) %&gt;% \n  table() %&gt;%\n  sort(decreasing = TRUE) %&gt;% \n  as.data.frame() %&gt;% \n  setNames(c('indicator', 'n_cors'))\n\n# Save this for preso  \nsaveRDS(cor_counts, 'preso/data/correlation_counts.rds')\n\n\n\n\n\n\n\n\nThe wealth/income distribution indicator (economics) is correlating strongly with several indicators, some from the economics dimension and some from health. Note that there are several metrics in that indicator related to median earnings, which might be a proxy for gdp per capita. Now that I look at this, it might be worth including gdp per capita at least as a control variable to see how much fo the variation it accounts for.\nIt looks like all the indicators from the carbon index (embodied, fluxes, stocks) correlate with one another, which makes enough sense. I imagine that one shouldn’t be too much of a problem if they are being aggregated at the index level anyway.\nForest health and carbon stocks are currently quite highly correlated, but this is because the metrics for carbon stocks are not ideal. The metrics for carbon stocks and forest health all come from the same TreeMap dataset. I suspect that if we include a better set of metrics for carbon stocks, this won’t be a such a problem.\nValue-added markets and operations diversification are all using a very similar set of metrics as well. They mostly come from NASS, and it would be worth digging into the NASS docs to see how whether value-added sales might overlap with agritourism, direct to consumer sales, or local marketing channel sales.\nFood affordability and food security also unsurprisingly correlate strongly. The current framework here is a work in progress and a bit haphazard. It will need some reworking. Curiously, these indicators also strongly correlate with participatory governance. That’s quite an interesting finding.\nAs for what to do about highly correlating indicators in general:\n\nThey could be reworked to use metrics that don’t lead to indicator correlations. This sounds rather difficult to me, and maybe impossible. It seems likely to be the reality that aspects of the economics and health dimensions are indeed related, for example.\nThey could be weighted in their respective dimensions to account for the correlations. This might be done with PCA loadings or by expert opinion.\nWe could also leave them as is. This would mean potentially double-counting certain aspects, but may be a reasonable approximation of reality.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Analysis",
      "Indicator Correlations"
    ]
  },
  {
    "objectID": "pages/framework_maps.html",
    "href": "pages/framework_maps.html",
    "title": "Map Explorer",
    "section": "",
    "text": "Exploring scores with maps.\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/data_environment_overview.html",
    "href": "pages/data_environment_overview.html",
    "title": "Environment: Overview",
    "section": "",
    "text": "The first plot shows all the environment indicators from both the current studies and the original framework in the y-axis. Purple indicates that the indicator is only being used in the current studies, orange that it is only included in the Wiltshire framework, and green that the indicator is used in both the framework and current studies.\nThe x-axis shows the number of secondary data metrics that have been collected to represent those indicators. You can see that there are some indicators for which there exist many data, but many indicators for which I have found little to represent them.\nNASS figures are used to cover on-farm water use, energy efficiency, and acres in conservation practices. I used the National Aquatic Resource Surveys aggregated at the state level to measure water quality. Land use diversity is pretty well represented by Multi-Resolution Land Characteristics LULC layers, which I also aggregated at the county level. Greenhouse gas emissions come from EPA figures by state, broken down by economic sector. Finally, the USFS TreeMap dataset accounts for aboveground biomass and would do reasonably well in tree vigor. There is more to pull out here than I have so far.\nOtherwise, if anyone has ideas for secondary datasets to cover the rest of the indicators, please do let me know.\nCode\npacman::p_load(\n  dplyr,\n  ggplot2,\n  stringr,\n  plotly,\n  RColorBrewer\n)\n\n## Load data for tree and metrics\nenv_tree &lt;- readRDS('data/trees/env_tree.rds')\n\nmeta &lt;- readRDS('data/sm_data.rds')[['metadata']] %&gt;% \n  filter(dimension == 'environment')\n\n# Format to match Wiltshire framework\nmeta &lt;- meta %&gt;% \n  mutate(\n    indicator = str_to_sentence(indicator),\n    indicator = case_when(\n      str_detect(indicator, '^Above') ~ 'Aboveground biomass',\n      str_detect(indicator, '^Water') ~ 'Water use / irrigation efficiency',\n      TRUE ~ indicator\n    )\n  ) \n\n# Counts of secondary data metrics\ncounts &lt;- meta %&gt;% \n  group_by(indicator) %&gt;% \n  dplyr::summarize(count = n())\n\n# Join to Wiltshire framework\ncolors &lt;- RColorBrewer::brewer.pal(n = 3, name = 'Dark2')\ndat &lt;- full_join(env_tree, counts, by = join_by(Indicator == indicator)) %&gt;% \n  mutate(\n    count = ifelse(is.na(count), 0, count),\n    label_color = case_when(\n      Use == 'both' ~ colors[1],\n      Use == 'wiltshire_only' ~ colors[2],\n      Use == 'current_only' ~ colors[3]\n    )\n  )\n\n# Plot\ndat %&gt;%\n  ggplot(aes(x = Indicator, y = count)) +\n  geom_col(\n    color = 'black',\n    fill = 'grey'\n  ) +\n  geom_point(\n    data = dat,\n    aes(x = 1, y = 1, color = Use),\n    inherit.aes = FALSE,\n    alpha = 0,\n    size = -1\n  ) +\n  scale_color_manual(\n    name = \"Indicator Use:\",\n    values = c(\n      \"both\" = colors[1],\n      \"current_only\" = colors[3],\n      \"wiltshire_only\" = colors[2]\n    ),\n    labels = c(\n      'Both',\n      'Current Only',\n      'Framework Only'\n    )\n  ) +\n  theme_classic() +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.text.y = element_text(color = dat$label_color),\n    axis.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    legend.title = element_text(size = 12),\n    legend.position = \"bottom\",\n    plot.margin = margin(t = 10, r = 75, b = 10, l = 10)\n  ) +\n  guides(\n    color = guide_legend(override.aes = list(size = 4, alpha = 1))\n  ) +\n  coord_flip() +\n  labs(y = 'Secondary Data Count')"
  },
  {
    "objectID": "pages/data_environment_overview.html#distribution-plots",
    "href": "pages/data_environment_overview.html#distribution-plots",
    "title": "Environment: Overview",
    "section": "1 Distribution Plots",
    "text": "1 Distribution Plots\n\n1.1 By County\nNote that while most of the available secondary data is at the county level, the environment dimension includes a fair amount at the state level as well. This includes greenhouse gas emissions and water quality surveys. For now, I’ll just show these separately, but some creative aggregation will have to happen eventually.\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr\n)\nsource('dev/data_pipeline_functions.R')\nsource('dev/filter_fips.R')\nmetrics &lt;- readRDS('data/sm_data.rds')[['metrics']]\nmetadata &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\n# Use metadata to get help filter by dimension\nenv_meta &lt;- metadata %&gt;%\n  filter(dimension == 'environment')\n\n# Filter to economics dimension\nenv_metrics &lt;- metrics %&gt;%\n  filter(variable_name %in% env_meta$variable_name)\n\n# env_metrics$variable_name %&gt;% unique\n# get_str(env_metrics)\n\n# Filter to latest year and new (post-2024) counties\n# And pivot wider so it is easier to get correlations\nenv_county &lt;- env_metrics %&gt;%\n  filter_fips(scope = 'counties') %&gt;%\n  get_latest_year() %&gt;%\n  select(fips, variable_name, value) %&gt;%\n  mutate(variable_name = str_split_i(variable_name, '_', 1)) %&gt;%\n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;%\n  unnest(!fips) %&gt;%\n  mutate(across(c(2:last_col()), as.numeric))\n\n# Save temp file for use in analysis script\nsaveRDS(env_county, 'data/temp/env_county.rds')\n\n## Plot\nplots &lt;- map(names(env_county)[-1], \\(var){\n  if (is.character(env_county[[var]])) {\n    env_county %&gt;%\n      ggplot(aes(x = !!sym(var))) +\n      geom_bar(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else if (is.numeric(env_county[[var]])) {\n    env_county %&gt;%\n      ggplot(aes(x = !!sym(var))) +\n      geom_density(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else {\n    return(NULL)\n  }\n})\n\n\n# Arrange them in 4 columns\nggarrange(\n  plotlist = plots,\n  ncol = 4,\n  nrow = 11\n)\n\n\n\n\n1.2 By State\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr,\n  stringr\n)\n\nconflicted::conflicts_prefer(\n  dplyr::select(),\n  dplyr::mutate(),\n  dplyr::summarize(),\n  dplyr::rename(),\n  .quiet = TRUE\n)\n\nstate_codes &lt;- readRDS('data/sm_data.rds')[['fips_key']] %&gt;%\n  dplyr::select(fips, state_code)\n\nenv_state &lt;- env_metrics %&gt;%\n  filter_fips(scope = 'states') %&gt;%\n  get_latest_year() %&gt;%\n  dplyr::select(fips, variable_name, value) %&gt;%\n  dplyr::mutate(variable_name = stringr::str_split_i(variable_name, '_', 1)) %&gt;% \n  tidyr::complete(fips, variable_name) %&gt;% \n  tidyr::pivot_wider(\n    id_cols = fips,\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;% \n  dplyr::left_join(state_codes, by = 'fips') %&gt;% \n  dplyr::mutate(across(!c(fips, state_code), as.numeric))\n\n# Save temp data file for use in analysis script\nsaveRDS(env_state, 'data/temp/env_state.rds')\n\n# Variables to map. Take out some that didn't come through well.\nvars &lt;- names(env_state)[-1] %&gt;%\n  stringr::str_subset(\n    'lakesAcidCond|lakesCylsperEpaCond|lakesMicxEpaCond|state_code|waterIrrSrcOffFarmExp|waterIrrReclaimedAcreFt|waterIrrReclaimedOpenAcres',\n    negate = TRUE\n  ) %&gt;% \n  stringr::str_subset('^CH4(?!FromAg)|^N2O(?!FromAg)|^CO2(?!FromAg)|^SubSector', negate = TRUE)\n\n## Plot\nstate_plots &lt;- purrr::map(vars, \\(var){\n  env_state %&gt;%\n    ggplot(aes(y = !!sym(var), x = state_code, color = state_code)) +\n    geom_point(\n      alpha = 0.5,\n      size = 3\n    ) +\n    theme_classic() +\n    theme(\n      plot.margin = unit(c(rep(0.5, 4)), 'cm'),\n      legend.position = 'none'\n    ) +\n    labs(\n      x = 'State'\n    )\n})\n\n# Arrange them in 4 columns\nggpubr::ggarrange(\n  plotlist = state_plots,\n  ncol = 4,\n  nrow = 22\n)"
  },
  {
    "objectID": "pages/data_environment_overview.html#bivariate-plots",
    "href": "pages/data_environment_overview.html#bivariate-plots",
    "title": "Environment: Overview",
    "section": "2 Bivariate Plots",
    "text": "2 Bivariate Plots\nUsing a selection of variables at the county level. The variable names are a bit hard to fit in here, but from left to right across the top they are LULC diversity, mean live above-ground forest biomass, conservation income per farm, conservatino easement acres per farm, conservation tillage: no-till acres per farm, conservation tillage: excluding no-till acres per farm, and cover cropping: excluding CRP acres per farm.\n\n\nCode\npacman::p_load(\n  GGally\n)\n\n# Neat function for mapping colors to ggpairs plots\n# https://stackoverflow.com/questions/45873483/ggpairs-plot-with-heatmap-of-correlation-values\nmap_colors &lt;- function(data,\n                       mapping,\n                       method = \"p\",\n                       use = \"pairwise\",\n                       ...) {\n  # grab data\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n\n  # calculate correlation\n  corr &lt;- cor(x, y, method = method, use = use)\n  colFn &lt;- colorRampPalette(c(\"blue\", \"white\", \"red\"), interpolate = 'spline')\n  fill &lt;- colFn(100)[findInterval(corr, seq(-1, 1, length = 100))]\n\n  # correlation plot\n  ggally_cor(data = data, mapping = mapping, color = 'black', ...) +\n    theme_void() +\n    theme(panel.background = element_rect(fill = fill))\n}\n\nlower_function &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(color = \"blue\", fill = \"grey\", ...) +\n    theme_bw()\n}\n\n# Rename variables to be shorter\nenv_county %&gt;%\n  select(\n    LULC = lulcDiversity,\n    # Biomass = meanAboveGrndForBiomass,\n    consIncomePF,\n    consEasementAcresPF,\n    consTillNoTillAcresPF,\n    consTillExclNoTillAcresPF,\n    coverCropExclCrpAcresPF\n  ) %&gt;%\n  ggpairs(\n    upper = list(continuous = map_colors),\n    lower = list(continuous = lower_function),\n    axisLabels = 'show'\n  ) +\n  theme(\n    strip.text = element_text(size = 5),\n    axis.text = element_text(size = 5),\n    legend.text = element_text(size = 5)\n  )\n\n\nIt looks like there are a few non-linear relationships, conservation income per farm in particular, but for the most part, linear relationships do a decent job here."
  },
  {
    "objectID": "pages/data_environment_overview.html#sec-correlations",
    "href": "pages/data_environment_overview.html#sec-correlations",
    "title": "Environment: Overview",
    "section": "3 Correlations",
    "text": "3 Correlations\nOnly showing correlations by county because we don’t have enough observations to run it by state.\n\n\nCode\npacman::p_load(\n  dplyr,\n  tidyr,\n  tibble,\n  stringr,\n  purrr,\n  tidyr,\n  ggplot2,\n  plotly,\n  reshape,\n  Hmisc,\n  viridisLite\n)\n\n# get_str(env_county)\n\ncor &lt;- env_county %&gt;%\n  select(-fips) %&gt;%\n  as.matrix() %&gt;%\n  rcorr()\n\n# Melt correlation values and rename columns\ncor_r &lt;- melt(cor$r) %&gt;%\n  setNames(c('var_1', 'var_2', 'value'))\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Make heatmap with custom text aesthetic for tooltip\nplot &lt;- cor_r %&gt;%\n  ggplot(aes(var_1, var_2, fill = value, text = paste0(\n    'Var 1: ', var_1, '\\n',\n    'Var 2: ', var_2, '\\n',\n    'Correlation: ', format(round(value, 3), nsmall = 3), '\\n',\n    'P-Value: ', format(round(p.value, 3), nsmall = 3)\n  ))) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  theme(axis.text.x = element_text(hjust = 1, angle = 45)) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = 'Correlation'\n  )\n\n# Convert to interactive plotly figure with text tooltip\nggplotly(\n  plot,\n  tooltip = 'text',\n  width = 1000,\n  height = 800\n)"
  },
  {
    "objectID": "pages/data_environment_analysis.html",
    "href": "pages/data_environment_analysis.html",
    "title": "Environment: Analysis",
    "section": "",
    "text": "This section will serve as a first pass at using some methods in the literature to aggregate metrics. I should say at the start that we have a pretty narrow selection of metrics to work with so far, which do not do a great job at capturing the breadth of the dimension. I’m also working with just the county-level data here. This provides some opportunities to use data-driven analyses like PCA, but it is worth noting that these will not get us to the holistic, system-wide measurements of sustainability we are after without including some normative judgments as to how to combine geographic areas as well as our five dimensions. So, let’s just go through the motions here, see how the process unfolds, and note anything worth digging into more down the road."
  },
  {
    "objectID": "pages/data_environment_analysis.html#imputation",
    "href": "pages/data_environment_analysis.html#imputation",
    "title": "Environment: Analysis",
    "section": "1 Imputation",
    "text": "1 Imputation\nPCA requires complete data, so we either have to impute, delete, or use PPCA. I’m choosing to impute with missing forest here as it is pretty good at handling MAR and non-linear data, but PPCA is certainly worth exploring.\n\n\nCode\npacman::p_load(\n  missForest,\n  tibble\n)\nsource('dev/filter_fips.R')\nenv_county &lt;- readRDS('data/temp/env_county.rds')\n\n# Wrangle dataset. Need all numeric vars or factor vars. And can't be tibble\n# Also removing character vars - can't use these in PCA\n# Using old Connecticut counties - some lulc data is missing for them though\ndat &lt;- env_county %&gt;%\n  filter_fips('old') %&gt;%\n  select(fips, where(is.numeric)) %&gt;%\n  column_to_rownames('fips') %&gt;%\n  as.data.frame()\n# get_str(dat)\n# skimr::skim(dat)\n\n# Remove variables with most missing data - too much to impute.\n# Also remove the proportional LULC values - keeping diversity though\ndat &lt;- dat %&gt;%\n  select(-matches('consIncome'), -matches('^lulcProp'))\n\n# Impute missing variables\nset.seed(42)\nmf_out &lt;- dat %&gt;%\n  missForest(\n    ntree = 200,\n    mtry = 10,\n    verbose = FALSE,\n    variablewise = FALSE\n  )\n\n# Save imputed dataset\nimp &lt;- mf_out$ximp\n\n# Print OOB\nmf_out$OOBerror"
  },
  {
    "objectID": "pages/data_environment_analysis.html#standardization",
    "href": "pages/data_environment_analysis.html#standardization",
    "title": "Environment: Analysis",
    "section": "2 Standardization",
    "text": "2 Standardization\nCentering and scaling to give every variable a mean of 0 and SD of 1.\n\n\nCode\ndat &lt;- map_dfc(imp, ~ scale(.x, center = TRUE, scale = TRUE))\n\n\nNow that we have standardized variables, we have to make normative decisions about what constitutes a good or bad value. This will certainly be a collaborative process where we seek input from teams to come to some kind of consensus once we have primary data. But until then, I’m going to make some heroic assumptions that LULC diversity is good, above ground forest biomass is good, conservation practices and easements are good, and fertilizer expenses are bad. Open to thoughts here as always.\nWith that, we can recode our normalized variables accordingly.\n\n\nCode\nnormed &lt;- dat %&gt;%\n  mutate(across(c(matches('^fert')), ~ -.x))"
  },
  {
    "objectID": "pages/data_environment_analysis.html#component-extraction",
    "href": "pages/data_environment_analysis.html#component-extraction",
    "title": "Environment: Analysis",
    "section": "3 Component Extraction",
    "text": "3 Component Extraction\nDetermine the number of components to extract using a few tools: very simple structure (VSS), Velicer’s minimum average partial (MAP) test, parallel analysis, and a scree plot.\n\n\nCode\npacman::p_load(\n  psych\n)\nVSS(normed)\nfa.parallel(normed)\npca_out &lt;- pca(normed, nfactors = 3, rotate = 'varimax')\nplot(pca_out$values)\nabline(h = 1)\n\n\nThis scree plot shows the eigenvalues (unit variance explained) of each principal component (y-axis) against each component (x-axis). The first few components explain lots of variance, but there is a decent elbow around the fourth component.\nVSS suggests 1 or 2, MAP suggests 8, parallel analysis shows 3. I’m going with 3 here, which will be explained further below."
  },
  {
    "objectID": "pages/data_environment_analysis.html#principal-components-analysis",
    "href": "pages/data_environment_analysis.html#principal-components-analysis",
    "title": "Environment: Analysis",
    "section": "4 Principal Components Analysis",
    "text": "4 Principal Components Analysis\nNow we let’s look run the PCA.\n\n\nCode\n(pca_out &lt;- pca(normed, nfactors = 3, rotate = 'varimax'))\n\n\nRecommendations for creating composite indices are to extract components that each have eigenvalues &gt; 1, explained variance &gt; 0.10, and such that the proportion of explained variance for the total set is &gt; 0.60 (Nicoletti 2000; OECD 2008).\nOur total cumulative variance is explained is 0.74, and our component that explains the least variance is RC4 with 0.11. Note that extracting four or more components here gives us a component with less than 0.10, so this is why we are sticking to three. The first component (RC1) explains 38% of the variance in the data. The second component is respectable at 0.26, while the third is barely above the threshold at 0.11.\nLooking at the metrics, we can see that the first component loads mostly onto the conservation practices, no-till acres, cover cropping, drainage, and total fertilizer expenses. The second component leads onto mean above-ground biomass (although there is cross-loading with the first component), operations with silvapasture, operations with easements, rotational grazing operations, and operations with fertilizer expenses. This seems to be catching more of the population-related metrics. The last component only loads onto a few metrics: easement acres, easement acres per farm, and silvapasture operations (which has some heavy cross-loading)."
  },
  {
    "objectID": "pages/data_environment_analysis.html#aggregation",
    "href": "pages/data_environment_analysis.html#aggregation",
    "title": "Environment: Analysis",
    "section": "5 Aggregation",
    "text": "5 Aggregation\nHere, we follow Nicoletti and calculate the normalized sum of square factor loadings, which represent the proportion of total unit variance of the metrics that is explained by the component.\n\n\nCode\n## Get metric weights following Nicoletti 2000\n# Pull out metric loadings\nloadings &lt;- pca_out$weights %&gt;%\n  as.data.frame()\n\n# For each set of loadings, get squares, and then normalized proportions\nsq_loadings &lt;- map(loadings, ~ .x^2)\nmetric_weights &lt;- map(sq_loadings, ~ .x / sum(.x))\nhead(as.data.frame(metric_weights))\n\n\nNow we can use these to weight metrics and aggregate them into a component score for each county.\n\n\nCode\n# Component scores for each component across each county\ncomponent_scores &lt;- map(metric_weights, \\(x) {\n  as.matrix(normed) %*% x\n}) %&gt;%\n  as.data.frame()\nhead(component_scores)\n\n\nAn alternative method here is regression scores, which are native to PCA. I’ll calculate these as well to compare to the component scores above.\n\n\nCode\n# Get regression scores from pca output\nregression_scores &lt;- as.data.frame(pca_out$scores)\nhead(regression_scores)\n\n\nRunning a correlation to see how similar they are:\n\n\nCode\ncoefs &lt;- map2_dbl(component_scores, regression_scores, \\(x, y) cor(x, y)) %&gt;%\n  round(3)\ncat(paste0(\n  'Pearson Correlation Coefficients:\\n',\n  'RC1: ', coefs[1], '\\n',\n  'RC2: ', coefs[2], '\\n',\n  'RC3: ', coefs[3]\n))\n\n\nIt looks like they are reasonably similar, although RC2 and RC3 have substantially lower correlation coefficients. It will be worth noting this and coming back to explore the differences at some point.\nFor now, let’s keep following Nicoletti and aggregate the component scores into a single variable.\n\n\nCode\nsum_sq_loadings &lt;- map_dbl(sq_loadings, ~ sum(.x))\n(factor_weights &lt;- map_dbl(sum_sq_loadings, ~ .x / (sum(sum_sq_loadings))))\n\n\nCurious that the component that accounted for the most variance is weighted the lowest. Worth doing a dive here at some point and figuring out why that is.\nWe will use these to weight each component to combine them.\n\n\nCode\ndimension_scores &lt;- component_scores %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    dimension_score = sum(RC1, RC2, RC3),\n    across(everything(), ~ round(.x, 3))\n  ) %&gt;%\n  bind_cols(rownames(imp)) %&gt;%\n  select(fips = 5, everything())\nhead(dimension_scores)\n\n\nNow that we have all three component scores and the dimension score, let’s take a look at a map. Select the data to display with the layer button on the left.\n\n\nCode\npacman::p_load(\n  mapview,\n  leaflet,\n  leafpop\n)\nmap_dat &lt;- readRDS('data/sm_spatial.rds')[['ne_counties_2021']] %&gt;%\n  right_join(dimension_scores) %&gt;%\n  left_join(fips_key) %&gt;%\n  select(\n    fips,\n    RC1:RC3,\n    'Dimension Score' = dimension_score,\n    County = county_name,\n    State = state_name,\n    geometry\n  )\n\nmap_dat %&gt;%\n  mapview(\n    zcol = c(\n      'Dimension Score',\n      'RC1',\n      'RC2',\n      'RC3'\n    ),\n    burst = FALSE,\n    hide = c(FALSE, rep(TRUE, 3)),\n    popup = popupTable(\n      map_dat,\n      zcol = names(map_dat)[-length(map_dat)],\n      row.numbers = FALSE,\n      feature.id = FALSE\n  )\n)\n\n\nKeep in mind there are lots of caveats with this very preliminary analysis, the most egregious being a set of metrics that does not well represent the dimension it purports to measure. Missing data and various branching paths of decisions in the index scoring also deserve further scrutiny.\nStill, there is plenty to look at here as a first pass at aggregating dimension scores. The first component, RC1, was heavily influenced by the geography - it loads the strongest onto metrics measuring acres or acres per farm. I presume this is why Aroostook county shows up so high on this scale. RC2 loaded strongly onto the number of operations using various conservation practices (easements, no-till, rotational grazing). It seems to track a little bit with county size, but is highest near relatively urban areas. RC3 was most associated with conservation easement acres and easement acres per farm, and consequently seems to track with rural areas.\nI don’t think that the dimension score inspires much confidence as it is now. The weighting method for combining components is hard to interpret intuitively, and I think more expert driven normative decisions might make more sense at that point. On the bright side, it is a good expedition into the kinds of ambiguous decisions that will need to be made to aggregate this data across the whole system."
  },
  {
    "objectID": "pages/comparison.html",
    "href": "pages/comparison.html",
    "title": "Comparison of Aggregation Methods",
    "section": "",
    "text": "In the last page we created six sets of scores by state based on combinations of three normalization methods (z-scores, min max, box cox) and two aggregation methods (arithmetic, geometric). Here, we will explore differences between them in terms of state distributions and rankings.\nNote that each set of spider plots are scaled to the minimum and maximum of any single state in that dimension, given the normalization and aggregation methods. This means in the case of min-max normalization, for example, raw metrics are scaled from 0 to 1, arithmetic and geometric means consolidate values to dimension scores, and these sets of dimension scores are scaled on the plot from the lowest to the highest value of any state. A “perfect” score here means that it is the best of any state. Plots show dimension values for Vermont in green. The dotted purple polygon behind it is the median of US states and DC. Arithmetic means are on the left, and geometric on the right.\nBe aware that spider/radar charts can be hard to interpret, and sometimes misleading The Radar Chart and its Caveats. The order of variables makes a big impact on the area of chart, and area is not a terribly reliable way to show differences, as it increases quadratically as variables increase linearly. Will explore some other ways to show this information, but using these for now as they are quite popular in the literature for sustainability metrics.",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table",
    "href": "pages/comparison.html#arithmetic-table",
    "title": "Comparison of Aggregation Methods",
    "section": "1.1 Arithmetic Table",
    "text": "1.1 Arithmetic Table\n\n\nCode\nget_reactable_scores(dat, 'minmax_arithmetic')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table",
    "href": "pages/comparison.html#geometric-table",
    "title": "Comparison of Aggregation Methods",
    "section": "1.2 Geometric Table",
    "text": "1.2 Geometric Table\n\n\nCode\nget_reactable_scores(dat, 'minmax_geometric')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table-1",
    "href": "pages/comparison.html#arithmetic-table-1",
    "title": "Comparison of Aggregation Methods",
    "section": "2.1 Arithmetic Table",
    "text": "2.1 Arithmetic Table\n\n\nCode\nget_reactable_scores(dat, 'zscore_arithmetic')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table-1",
    "href": "pages/comparison.html#geometric-table-1",
    "title": "Comparison of Aggregation Methods",
    "section": "2.2 Geometric Table",
    "text": "2.2 Geometric Table\n\n\nCode\nget_reactable_scores(dat, 'zscore_geometric')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table-2",
    "href": "pages/comparison.html#arithmetic-table-2",
    "title": "Comparison of Aggregation Methods",
    "section": "3.1 Arithmetic Table",
    "text": "3.1 Arithmetic Table\n\n\nCode\nget_reactable_scores(dat, 'boxcox_arithmetic')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table-2",
    "href": "pages/comparison.html#geometric-table-2",
    "title": "Comparison of Aggregation Methods",
    "section": "3.2 Geometric Table",
    "text": "3.2 Geometric Table\n\n\nCode\nget_reactable_scores(dat, 'boxcox_geometric')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table-3",
    "href": "pages/comparison.html#arithmetic-table-3",
    "title": "Comparison of Aggregation Methods",
    "section": "4.1 Arithmetic Table",
    "text": "4.1 Arithmetic Table\n\n\nCode\nget_reactable_scores(dat, 'rank_arithmetic')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table-3",
    "href": "pages/comparison.html#geometric-table-3",
    "title": "Comparison of Aggregation Methods",
    "section": "4.2 Geometric Table",
    "text": "4.2 Geometric Table\n\n\nCode\nget_reactable_scores(dat, 'rank_geometric')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#arithmetic-table-4",
    "href": "pages/comparison.html#arithmetic-table-4",
    "title": "Comparison of Aggregation Methods",
    "section": "5.1 Arithmetic Table",
    "text": "5.1 Arithmetic Table\n\n\nCode\nget_reactable_scores(dat, 'winsor_arithmetic')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "pages/comparison.html#geometric-table-4",
    "href": "pages/comparison.html#geometric-table-4",
    "title": "Comparison of Aggregation Methods",
    "section": "5.2 Geometric Table",
    "text": "5.2 Geometric Table\n\n\nCode\nget_reactable_scores(dat, 'winsor_geometric')",
    "crumbs": [
      "Analysis",
      "Comparisons"
    ]
  },
  {
    "objectID": "dev/quarto_page_template.html",
    "href": "dev/quarto_page_template.html",
    "title": "Quarto Page Template",
    "section": "",
    "text": "Remember to remove execute: false from the YAML header.\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/aggregation.html",
    "href": "pages/aggregation.html",
    "title": "Metric Aggregation",
    "section": "",
    "text": "1 Introduction\nExploring methods of aggregating data into index and dimension scores. To Add:\n\nPrimer on methods, cite (OECD 2008)\nExamples\n\ncite Schneider (Schneider et al. 2023)\n\nrank order comparisons only\ncompare to global weighted means by groups based on GDP\nmin max scaling to show distance from global groups\n\ncite Bene et al 2019 (Béné et al. 2019)\n\nBox cox for most skewed indicators (skew &gt; 2)\nthen min max\ngeometric means for enviro and economic dimensions\narithmetic means for social and food dimensions\ngeometric mean for combining all four dimensions into one\n\ncite Nicoletti (Nicoletti 2000)\n\nUse normaliaed square loadings (indicator weights) to weight each indicator\n\ncite Gomez Limon and Sanchez (Gómez-Limón and Sanchez-Fernandez 2010)\n\nmin max normalization\nAggregation - compared several different methods - mostly correlate, no big differences\n\nweighted sum of indicators\nproduct of weighted indicators\nmuilticriteria function based on distance to ideal point\n\nWeighting - did both PCA and analytic hierarchy process\nValidation (identifying important factors) - double censored tobit - index as dependent, indicators as independent\n\ncite demelash and aremu (Adamu Demelash and Abate Alemu 2024)\n\nNormalization - distance to reference\n\nrefernce determined by quartile analysis\nnot affected by outliers, extreme values\n\nWeighting - equal\nAggregation - linear\nAdditive method for indicators within dimensions\nGeometric means for aggregate scores across four dimensions\n\n\n\n\n\n2 Imputation\nFirst, check how much missing data there are. If it is within reason, use missForest algorithm to impute missing data (Stekhoven and Bühlmann 2012). This is particularly good at handling MAR data, and does a decent job at handling MNAR data and non-linear relationships as well. If less than 5% of data are missing, just about any method for handling it is reasonable, even listwise deletion (Beaujean 2013).\n\n\nCode\npacman::p_load(\n  missForest,\n  tibble,\n  dplyr\n)\n\nmetrics_df &lt;- readRDS('data/metrics_df.rds')\nget_str(metrics_df)\n\n# Check for missing data\nskimr::skim(metrics_df)\nmis_dat &lt;- sum(is.na(metrics_df))/(nrow(metrics_df)*(ncol(metrics_df) - 1)) * 100 \nmis_dat &lt;- round(mis_dat, 3)\n\n# Change fips from column to rowname so we can impute without losing it\nmetrics_df &lt;- metrics_df %&gt;% \n  column_to_rownames('fips')\nget_str(metrics_df)\n\n# Impute missing variables\nset.seed(42)\nmf_out &lt;- metrics_df %&gt;%\n  missForest(\n    ntree = 200,\n    mtry = 10,\n    verbose = FALSE,\n    variablewise = FALSE\n  )\n# get_str(mf_out)\n\n# Extract OOB error\n(oob &lt;- mf_out$OOBerror)\n\n# Check missing again\nskimr::skim(mf_out$ximp)\n# Looks good\n\n# Save just imputed data\nimp_dat &lt;- mf_out$ximp\n\n\nWe had 0.785% missing data, which is rather little, and gives us flexibility in handling it. The Out-of-Bag (OOB) error, quantified by the normalized residual mean squared error (NRMSE) the missForest imputation algorithm was 0.\n\n\n3 Rescaling\nWe are rescaling our data using five methods: rank order, winsorizing, Min-Max, Box-Cox, and Z-scores. Results will be saved to a list of five rescaled datasets so we can compare outcomes of each one and see what the consequences are.\nRank Order\nWinsorization\nMin Max (OECD 2008)\nMin-maxing scales all the data from 0 to 1 by subtracting the minimum value of each variable from all cases and dividing by the range of all cases in the variable. It is rather intuitive, as 1 is the best score, and 0 is the worst. This is a linear transformation, so the relationships between the values should not change.\n\\[\\begin{equation}\nI^t_qc = \\frac{x^t_qc - min_c(x^{t_0}_q)}{max_c(x^{t_0}_q)-min_c(x^{t_0}_q)}\n\\end{equation}\\]\nWhere \\(x^t_qc\\) is the metric \\(q\\) for state \\(c\\) at time \\(t\\).\nZ-Scores (OECD 2008)\nZ-scores are stardized to have a mean of 0 and a standard deviation of 1. Larger numbers are better, but there are no caps on the highest or lowest values. A value of 2 would mean that it is 2 standard deviations greater than the mean. Again, this is a linear transformation, so relationships between variables should not change.\n\\[\\begin{equation}\nI^t_{qc} = \\frac{x^t_{qc}-x^t_{qc=\\overline{c}}}{\\sigma^t_{qc=\\overline{c}}}\n\\end{equation}\\]\nWhere \\(x^t_qc\\) is the metric \\(q\\) for state \\(c\\) at time \\(t\\).\nBox Cox (Bickel and Doksum 1981)\nBox-Cox transformations are non-linear transformations that use an optimal value of lambda to make the distribution as normal as possible. This has some strengths in that the data are easier to work with in further analyses. It also effectively pulls outliers inward toward the center of the distribution. However, it also changes relationships between the variables, so it will distort any bivariate correlations.\n\\[\\begin{equation}\n{\\rm For}\\ \\lambda\\neq0,\\ f\\lambda(x) = (sign(x)|x|^\\lambda-1)/\\lambda\n\\end{equation}\\]\n\\[\\begin{equation}\n{\\rm For}\\ \\lambda = 0,\\ f_0(x) = log(x)\n\\end{equation}\\]\n\n\nCode\npacman::p_load(\n  forecast,\n  DescTools,\n  purrr\n)\n\n# List of results\nnormed &lt;- list()\nget_str(imp_dat)\n\n# Z scores\nnormed$zscore &lt;- imp_dat %&gt;% \n  mutate(across(everything(), ~ as.numeric(scale(.x, scale = TRUE, center = TRUE))))\n\n# Min Max\nmin_max &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\nnormed$minmax &lt;- imp_dat %&gt;% \n  mutate(across(everything(), min_max))\n\n# Box Cox. Adding 1 as constant to remove zeroes\nnormed$boxcox &lt;- imp_dat %&gt;% \n  mutate(across(everything(), ~ forecast::BoxCox(.x + 1, lambda = 'auto')))\n\n# Rank order from lowest to highest value for each var. We are coding this such\n# that higher ranks are better. So 51 should have the highest/best value and\n# rank 1 should have the worst.\nnormed$rank &lt;- map(names(imp_dat), \\(col_name) {\n  imp_dat %&gt;% \n    rownames_to_column('fips') %&gt;% \n    select(fips, col_name) %&gt;% \n    mutate(!!sym(col_name) := dense_rank(.data[[col_name]]))\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  column_to_rownames('fips')\nget_str(normed$rank)\n# get_str(normed$rank[[1]])\n# normed$rank[[1]] %&gt;% arrange(unemploymentRate)\n\n# Winsorization\nnormed$winsor &lt;- imp_dat %&gt;% \n  mutate(across(everything(), Winsorize))\n\n# Check\nmap(normed, get_str)\n\n\n\n\n4 Directional Values\nHere, we are assuming that each metric has a direction that is more sustainable than the opposite. Either more of it is better, or less of it is better. This is rather problematic in that just about any metric becomes negative with too much or too little of it. What might make more sense in the long run would be to consult the expertise of our teams and develop targets or acceptable ranges for some metrics once they are settled. Still, just about every sustainability indicator framework does some variation of this one-way value system (Schneider et al. 2023; Béné et al. 2019; Nicoletti 2000; Jacobi et al. 2020; Gómez-Limón and Sanchez-Fernandez 2010).\nAlas, for now we will invert variables in each of the transformed datasets as necessary so that larger numbers are more sustainable, and smaller numbers are less sustainable. The table below shows this assignment in the desirable column. For couple of variables (vacancy rate and animal sales as a percentage of all agricultural sales) I was not comfortable assigning one direction as better than the other, so I have removed them from the refined framework.\n\n\nCode\npacman::p_load(\n  reactable,\n  purrr\n)\nsource('dev/get_reactable.R')\n\n# Higher numbers should be better. Reverse metrics that are the opposite, \n# where lower numbers are better. Only listing reverse here - metrics are \n# implicitly better with larger numbers otherwise.\nreverse &lt;- c(\n  'unemploymentRate',\n  'gini',\n  'lowBirthweight',\n  'teenBirths',\n  'uninsured',\n  'incomeInequality',\n  'childrenInSingleParentHouseholds',\n  'injuryDeaths',\n  'airPollutionParticulateMatter',\n  'drinkingWaterViolations',\n  'severeHousingProblems',\n  'prematureAgeAdjustedMortality',\n  'infantMortality',\n  'frequentPhysicalDistress',\n  'frequentMentalDistress',\n  'diabetesPrevalence',\n  'hivPrevalence',\n  'limitedAccessToHealthyFoods',\n  'drugOverdoseDeaths',\n  'disconnectedYouth',\n  'residentialSegregationBlackWhite',\n  'suicides',\n  'motorVehicleCrashDeaths',\n  'severeHousingCostBurden',\n  'schoolSegregation',\n  'childCareCostBurden',\n  'wicPercEligible', # Iffy on this one\n  'droughtMeanPercArea',\n  'pctAtRiskAnimalSpp',\n  'pctAtRiskPlantSpp',\n  'pctAtRiskBeeSpp',\n  'pctAtRiskOrchidSpp',\n  'pctAtRiskEcosystems',\n  'expChemicalPct',\n  'ageProducers', # Could be better to use age diversity?\n  'waterIrrSrcOffFarmExp',\n  'waterIrrSrcOffFarmExpPerAcreFt',\n  'CH4FromAg',\n  'N2OFromAg',\n  'CO2FromAg',\n  'propAreaFsaSecDisasters',\n  'totalCapConsNoDwellings',\n  'totalIntExpRealEstateNoDwellings',\n  'totalIncomeInsuranceIndemnities',\n  'totalIncomeInsuranceIndemnitiesFederal',\n  'totalValueEmergPayments',\n  'totalValueOtherAdHocEmergPayments',\n  'totalValueDairyMarginProtPayments',\n  'totalValueAllLossCoveragePayments',\n  'totalValueAgRiskCoveragePayments',\n  'totalCapExpBldgsLandNoDwellings',\n  'alcoholImpairedDrivingDeaths' \n)\n\n# Iffy: landValPF, landValPerAcre - in good column for now, but unclear\n# indemnities and emergency payments - in bad column for now, but more access\n# coud be good?\n\n# Some are unclear - without clear direction, better to remove:\nremove &lt;- c(\n  'vacancyRate',\n  'salesAnimalPctSales',\n  'expHiredLaborPercOpExp',\n  'acresPF',\n  'medianAcresPF',\n  'importsTopFive'\n)\n\n## Remove the unclear ones from all three datasets\n# Then for each transformation, flip values in a way that makes sense\n# zscore: multiple by -1, easy\n# minmax: 1 - x, easy\n# rank: nrow - x, easy\n# boxcox and winsor: trickier. want to just reverse the distribution. \n#   max(x) - x + min(x)\n# Could we have just done this in the beginning, before normalization? Maybe\nvalued_data &lt;- imap(normed, \\(df, method) {\n  df %&gt;% \n    select(-matches(remove)) %&gt;% \n    mutate(\n      across(any_of(reverse), ~ case_when(\n        method == 'zscore' ~ .x * -1,\n        method == 'minmax' ~ 1 - .x,\n        method %in% c('boxcox', 'winsor') ~ max(.x) - .x + min(.x),\n        method == 'rank' ~ max(.x) - .x + 1\n      )),\n      across(everything(), as.numeric)\n    )\n})\nmap(valued_data, get_str)\n\n# Compare\nchecklist &lt;- list(normed, valued_data)\nmap(checklist, ~ .x$rank[[2]])\n\n# Check one that should get flipped\nvalued_data$minmax$unemploymentRate\nnormed$minmax$unemploymentRate\n# Looks good\n\n# Save this as our 'normalized data' that we use for building scores\nsaveRDS(valued_data, 'data/valued_rescaled_metrics.rds')\n\n\n\n\nCode\n## Show table of which metrics were set in which direction\nsm_data &lt;- readRDS('data/sm_data.rds')\nmeta &lt;- sm_data$metadata\n\n# Reactable table showing var, metric, source, and direction\ntable &lt;- meta %&gt;% \n  dplyr::filter(variable_name %in% names(imp_dat)) %&gt;% \n  mutate(desirable = case_when(\n    variable_name %in% reverse ~ 'Lower',\n    variable_name %in% remove ~ 'Removed',\n    .default = 'Higher'\n  )) %&gt;% \n  select(\n    metric, \n    variable_name, \n    dimension,\n    index,\n    indicator,\n    desirable, \n    definition, \n    source\n  )\n\n# Save this desirable direction table for preso\nsaveRDS(table, 'preso/data/desirable_directions_table.rds')\n\n\n\n\nCode\n# Make reactable table\ntable %&gt;% \n  get_reactable(\n    defaultPageSize = 5,\n    columns = list(\n      'definition' = colDef(\n        minWidth = 150\n      ),\n      'source' = colDef(\n        minWidth = 150\n      )\n    )\n  )\n\n\n\n\n\n\n\n\n5 Aggregation\nHere we are combining values in each indicator, index, and dimension using both arithmetic and geometric means (OECD 2008). Arithmetic means are fully compensable, in that a strong score in one area can make up for a weak score in another. Geometric means are only somewhat compensable - it effectively applies a penalty for unbalanced scores.\nWe might also consider PCA here, as we have done with the preliminary dimension metrics previously. But the n:p ratio is not in our favor for PCA as we have more metrics than states. Will revisit this, perhaps by splitting it up into dimensions again rather than trying the whole framework at once, or possible using a sparse PCA procedure that incorporates variable selection.\nWe will end up with 10 iterations of our data (5 normalization methods * 2 aggregation methods).\nIndicator aggregation:\n\n\nCode\n# We need to attach these back to framework from metadata\n# Filter frame from earlier down to our current metrics\n# We are also removing the 'remove' metrics without clear directional values\nframe &lt;- readRDS('data/frame.rds')\nfiltered_frame &lt;- frame %&gt;% \n  dplyr::filter(variable_name %in% names(valued_data[[1]])) %&gt;% \n  dplyr::select(variable_name, indicator, index, dimension)\nget_str(filtered_frame)\n\n# Save this for later - use in regression and variable selection \nsaveRDS(filtered_frame, 'data/filtered_frame.rds')\n\n# Make a list where we hold scores for indicators, indices, and dimensions\nscores &lt;- list()\n\n# Function for geometric mean\ngeometric_mean &lt;- function(x, na.rm = TRUE){\n  if (all(x &gt; 0)) {\n    exp(mean(log(x), na.rm = na.rm))\n  } else if (any(x &lt;= 0)) {\n    horizontal_shift &lt;- abs(min(x)) + 1\n    exp(mean(log(x + horizontal_shift), na.rm = na.rm)) - horizontal_shift\n  } \n}\n# Get indicator scores across all three normalization methods\nindicator_scores &lt;- map(valued_data, \\(df) {\n  \n  # For each df, calculate indicator means\n  indicators_out &lt;- map(unique(filtered_frame$indicator), \\(ind) {\n  \n    # Column name based on indicator\n    ind_snake &lt;- ind\n    \n    # Split into groups by indicator, with one or more metrics each\n    variables &lt;- filtered_frame %&gt;% \n      dplyr::filter(indicator == ind) %&gt;% \n      pull(variable_name) %&gt;% \n      unique()\n    indicator_metrics &lt;- df %&gt;% \n      dplyr::select(any_of(variables))\n    \n    # Get arithmetic and geo means for each indicator\n    dfs &lt;- list()\n    dfs$arithmetic &lt;- indicator_metrics %&gt;%\n      rowwise() %&gt;%\n      mutate(\n        !!sym(ind_snake) := mean(c_across(everything())),\n      ) %&gt;%\n      dplyr::select(!!sym(ind_snake))\n    dfs$geometric &lt;- indicator_metrics %&gt;% \n      rowwise() %&gt;% \n      mutate(\n        !!sym(ind_snake) := geometric_mean(c_across(everything())),\n      ) %&gt;%\n      select(!!sym(ind_snake))\n    return(dfs) \n  })\n  \n  # Rearrange so we put each aggregation method (arith, geo) together\n  norm_out &lt;- list()\n  norm_out$arithmetic &lt;- map(indicators_out, ~ {\n    .x[grep(\"arithmetic\", names(.x))]\n  }) %&gt;% \n    bind_cols()\n  norm_out$geometric &lt;- map(indicators_out, ~ {\n    .x[grep(\"geometric\", names(.x))]\n  }) %&gt;% \n    bind_cols()\n  return(norm_out) \n})\n  \nget_str(indicator_scores, 3)\nget_str(indicator_scores, 4)\n\n# Test function\n# test &lt;- get_agg_indicators(normed, filtered_frame)\n# identical(indicator_scores, test)\n\n\nIndex aggregation:\n\n\nCode\n# For each set of indicator scores, calculate index scores\n# get_str(indicator_scores, 4)\nindices &lt;- unique(filtered_frame$index)\n\n# Choose aggregation function based on agg_type\nagg_function &lt;- function(x, agg_type) {\n   if (agg_type == 'geometric') {\n    geometric_mean(x)\n  } else if (agg_type == 'arithmetic') {\n    mean(x)\n  }\n}\n\nindex_scores &lt;- map(indicator_scores, \\(norm_type) {\n  imap(norm_type, \\(agg_df, agg_type) {\n    map(indices, \\(index_) {\n      # Get names of indicators for this index\n      index_indicators &lt;- filtered_frame %&gt;% \n        filter(index == index_) %&gt;% \n        pull(indicator) %&gt;% \n        unique()\n      # Get DF of indicators for this index\n      index_indicator_df &lt;- agg_df %&gt;% \n        select(all_of(index_indicators))\n      # Get arithmetic or geometric mean, based on agg_type\n      index_indicator_df %&gt;% \n        rowwise() %&gt;% \n        # mutate(mean = across(everything(), agg_function(agg_type)))\n        mutate(!!sym(index_) := agg_function(c_across(everything()), agg_type)) %&gt;% \n        select(!!sym(index_))\n    }) %&gt;% \n      bind_cols()\n  })\n})\nget_str(index_scores, 4)\n\n# Test function\n# test_indices &lt;- get_agg_indices(indicator_scores, frame)\n# identical(index_scores, test_indices)\n\n\nDimension aggregation:\n\n\nCode\nget_str(index_scores, 4)\n\n# Same process for dimensions\ndimensions &lt;- unique(filtered_frame$dimension)\n\ndimension_scores &lt;- map(index_scores, \\(norm_type) {\n  imap(norm_type, \\(agg_df, agg_type) {\n    map(dimensions, \\(dimension_) {\n      # Get names of indices for this dimension\n      dimension_indices &lt;- filtered_frame %&gt;% \n        filter(dimension == dimension_) %&gt;% \n        pull(index) %&gt;% \n        unique()\n      # Get DF of indice for this dimension\n      dimension_index_df &lt;- agg_df %&gt;% \n        select(all_of(dimension_indices))\n      # Get arithmetic or geometric mean, based on agg_type\n      dimension_index_df %&gt;% \n        rowwise() %&gt;% \n        mutate(!!sym(dimension_) := agg_function(\n          c_across(everything()), \n          agg_type\n        )) %&gt;% \n        select(!!sym(dimension_))\n    }) %&gt;% \n      bind_cols()\n  })\n})\nget_str(dimension_scores, 4)\n\n# Test function\n# test_dimensions &lt;- get_agg_dimensions(index_scores, filtered_frame)\n# identical(dimension_scores, test_dimensions)\n\n\n\n\n6 Wrangle\nHere, we organize arithmetic and geometric means for each level of the framework (indicator, index, dimension) in a way that is easier to work with. We also add means and medians for all US states as well as New England states that we can use as points of comparison.\n\n\nCode\npacman::p_load(\n  purrr\n)\n\nget_str(indicator_scores, 4)\nget_str(index_scores, 4)\nget_str(dimension_scores, 4)\n\n# Want to end up with 6 lists: 3 norm types * 2 mean types\n# Put them all together in one list to work with\nall_scores &lt;- mget(c(\n  'indicator_scores',\n  'index_scores',\n  'dimension_scores'\n))\nget_str(all_scores, 3)\n\n# Function to pull out the pieces we want\n# Also put state names back in as a column and with real names, not codes\nget_output &lt;- function(norm_type, agg_type) {\n  # Get list of each df (dimension, index, indicator) for combo\n  dfs &lt;- all_scores %&gt;% \n    map(\\(level) level[[norm_type]]) %&gt;% \n    map(\\(norm) norm[[agg_type]])\n  # Get state back into a proper column for each df\n  out &lt;- map(dfs, ~ {\n    .x %&gt;% \n      # Note that we are binding fips back in - this is hinky, note to fix\n      bind_cols(\n        metrics_df %&gt;% \n          rownames_to_column('fips') %&gt;% \n          dplyr::select(fips)\n      ) %&gt;% \n      left_join(\n        dplyr::select(sm_data$state_key, state, state_code),\n        by = join_by(fips == state_code) \n      ) %&gt;% \n      dplyr::select(-fips)\n  })\n  return(out)\n}\n\n# All combinations, also a name\ncombos &lt;- expand.grid(\n  names(all_scores[[1]]),\n  c('arithmetic', 'geometric')\n) %&gt;% \n  mutate(name = paste0(Var1, '_', Var2))\n\n# Map to pull them all out\nscores &lt;- map2(combos[[1]], combos[[2]], ~ {\n  get_output(.x, .y)\n}) %&gt;% \n  setNames(c(combos$name))\nget_str(scores, 4)\nget_str(scores, 3)\n\n# Test function\n# test_organized &lt;- get_organized_scores(all_scores, sm_data$state_key, metrics_df)\n# identical(scores, test_organized)\n\n\n## Add medians for New England states and US\nfinal_scores &lt;- map(scores, \\(method) {\n  map(method, \\(level) {\n    \n    # Mean of every US state and DC\n    us_means &lt;- level %&gt;%\n      dplyr::select(-state) %&gt;% \n      colMeans() %&gt;% \n      as.list()\n    us_means$state &lt;- 'US_mean'\n    \n    # Median of every US state and DC\n    us_medians &lt;- level %&gt;% \n      dplyr::select(-state) %&gt;% \n      map_dbl(median) %&gt;% \n      as.list()\n    us_medians$state &lt;- 'US_median'\n    \n    # Mean of just New England states\n    ne_means &lt;- level %&gt;% \n      dplyr::filter(state %in% c('VT', 'NH', 'ME', 'MA', 'CT', 'RI')) %&gt;% \n      dplyr::select(-state) %&gt;% \n      colMeans() %&gt;% \n      as.list()\n    ne_means$state &lt;- 'NE_mean'\n     \n    # Median of just New England states\n    ne_medians &lt;- level %&gt;% \n      dplyr::filter(state %in% c('VT', 'NH', 'ME', 'MA', 'CT', 'RI')) %&gt;% \n      dplyr::select(-state) %&gt;% \n      map_dbl(median) %&gt;% \n      as.list()\n    ne_medians$state &lt;- 'NE_median'\n    \n    # Return the level + US + NewEng means\n    level %&gt;% \n      bind_rows(us_means) %&gt;% \n      bind_rows(us_medians) %&gt;% \n      bind_rows(ne_means) %&gt;% \n      bind_rows(ne_medians)\n  })\n})\nget_str(final_scores, 3)\nget_str(final_scores, 4)\n\n# Test function\n# test_final_scores &lt;- get_groupings(scores)\n# get_str(test_final_scores)\n# get_str(test_final_scores, 4)\n# identical(final_scores, test_final_scores)\n# Not same because we added NE medians. That's okay.\n\n# Save this for use elsewhere\nsaveRDS(final_scores, 'data/state_score_iterations.rds')\n\n\nThis gives us a list of 10 elements, one for each combination of normalization method and aggregation method. Each element has three data frames, one for indicator, index, and dimension. Now we can compare these 6 outputs to see how the methodological differences affect scores and ranks.\n\n\n\n\n\n\n\n\n Back to top7 References\n\nAdamu Demelash, Sewareg, and Esubalew Abate Alemu. 2024. “Measuring Food System Sustainability in Ethiopia: Towards a Multi-Dimensional Perspective.” Ecological Indicators 161 (April): 111991. https://doi.org/10.1016/j.ecolind.2024.111991.\n\n\nBeaujean, A. Alexander. 2013. “Factor Analysis Using R.” https://doi.org/10.7275/Z8WR-4J42.\n\n\nBéné, Christophe, Steven D. Prager, Harold A. E. Achicanoy, Patricia Alvarez Toro, Lea Lamotte, Camila Bonilla, and Brendan R. Mapes. 2019. “Global Map and Indicators of Food System Sustainability.” Scientific Data 6 (1): 279. https://doi.org/10.1038/s41597-019-0301-5.\n\n\nBickel, Peter J., and Kjell A. Doksum. 1981. “An Analysis of Transformations Revisited.” Journal of the American Statistical Association 76 (374): 296–311. https://doi.org/10.1080/01621459.1981.10477649.\n\n\nGómez-Limón, José A., and Gabriela Sanchez-Fernandez. 2010. “Empirical Evaluation of Agricultural Sustainability Using Composite Indicators.” Ecological Economics 69 (5): 1062–75. https://doi.org/10.1016/j.ecolecon.2009.11.027.\n\n\nJacobi, Johanna, Stellah Mukhovi, Aymara Llanque, Markus Giger, Adriana Bessa, Christophe Golay, Chinwe Ifejika Speranza, et al. 2020. “A New Understanding and Evaluation of Food Sustainability in Six Different Food Systems in Kenya and Bolivia.” Scientific Reports 10 (1): 19145. https://doi.org/10.1038/s41598-020-76284-y.\n\n\nNicoletti, Giuseppe. 2000. “Summary Indicators of Product Market Regulation with an Extension to Employment Protection Legislation.” {{OECD Economics Department Working Papers}} 226. Vol. 226. OECD Economics Department Working Papers. https://doi.org/10.1787/215182844604.\n\n\nOECD. 2008. Handbook on Constructing Composite Indicators: Methodology and User Guide. Paris: Organisation for Economic Co-operation and Development.\n\n\nSchneider, Kate R., Jessica Fanzo, Lawrence Haddad, Mario Herrero, Jose Rosero Moncayo, Anna Herforth, Roseline Remans, et al. 2023. “The State of Food Systems Worldwide in the Countdown to 2030.” Nature Food 4 (12): 1090–110. https://doi.org/10.1038/s43016-023-00885-9.\n\n\nStekhoven, Daniel J., and Peter Bühlmann. 2012. “MissForest—Non-Parametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics 28 (1): 112–18. https://doi.org/10.1093/bioinformatics/btr597.",
    "crumbs": [
      "Analysis",
      "Aggregation"
    ]
  },
  {
    "objectID": "pages/data_economics.html",
    "href": "pages/data_economics.html",
    "title": "Economics",
    "section": "",
    "text": "Shown in the diagram below are a total of 45 indicators within the economics dimension. Indices are labeled within the diagram. 17 indicators are both included in the Wiltshire et al. framework as well as being studied by one or more teams (red), 9 are included in the Wiltshire et al. but not currently belong studied (green), while 19 were not in the original framework, but have been added by one or more teams (blue).\nThe points beside each indicator name represent the number of secondary data metrics that have been aggregated for each indicator. Sources include USDA NASS, BLS, ERS, Census Bureau, and others. The quality and appropriateness of these metrics vary widely - I do not mean to suggest that having more of them means an indicator is more accurately better represented. For more information on the data sources, head to the Tables page to see metadata.\nOne other point to note here is that I removed several dozen metrics from BLS wage labor data broken down by NAICS industry code so as not to inflate that indicator relative to the others.\n\n\nCode\n## Load packages\npacman::p_load(\n  ggraph,\n  igraph,\n  dplyr,\n  RColorBrewer,\n  viridisLite,\n  ggrepel,\n  stringr\n)\n\nconflicted::conflicts_prefer(\n  dplyr::as_data_frame(),\n  .quiet = TRUE\n)\n\n## Load data for tree and metrics\ndat &lt;- readRDS('data/trees/econ_tree.rds') %&gt;% \n  select(Dimension:Source)\nmetadata_all &lt;- readRDS('data/sm_data.rds')[['metadata']]\nmeta &lt;- metadata_all %&gt;% \n  filter(\n    dimension == 'economics'\n  )\n\n# Rename metadata so it fits into formatting of tree data\n# This is quite not ideal - Note to harmonize this properly later\nmeta &lt;- meta %&gt;% \n  mutate(\n    indicator = str_to_sentence(indicator),\n    indicator = case_when(\n      str_detect(indicator, '^Assets') ~ 'Balance sheet (assets and liabilities)',\n      str_detect(indicator, '^Business failure') ~ 'Business failure rate of food business',\n      str_detect(indicator, '^Direct') ~ '% direct-to-consumer sales',\n      str_detect(indicator, '^Job avail') ~ 'Availability of good-paying jobs in food systems',\n      str_detect(indicator, '^Local sales') ~ '% local sales',\n      str_detect(indicator, '^Operator salary') ~ 'Operator salary / wage',\n      str_detect(indicator, '^Total sales') ~ 'Total sales / revenue',\n      str_detect(indicator, '^Wealth/income') ~ 'Wealth / income distribution',\n      TRUE ~ indicator\n    )\n  ) \n\n# Join counts of secondary data metrics to original dataset\n# Remove the NAICS variables - there are so many of them, don't add much\ncounts &lt;- meta %&gt;% \n  filter(str_detect(variable_name, '^lq|lvl|Lvl|Naics', negate = TRUE)) %&gt;% \n  group_by(indicator) %&gt;% \n  dplyr::summarize(count = n())\n\n\n## Make edges\n# include groupings by dimension, then combine them\nedges &lt;- list()\nedges$dim_ind &lt;- dat %&gt;% \n  select(Dimension, Index) %&gt;% \n  unique() %&gt;% \n  dplyr::rename(from = Dimension, to = Index) %&gt;% \n  mutate(group = to)\nedges$ind_ind &lt;- dat %&gt;% \n  select(Index, Indicator) %&gt;% \n  unique() %&gt;% \n  dplyr::rename(from = Index, to = Indicator) %&gt;% \n  mutate(group = from)\nedges &lt;- bind_rows(edges)\n\n# Add column for use (will use in colors of text?)\nedges$group &lt;- c(rep(NA, 10), dat$Source)\n\n\n## Make vertices\n# Each line is a single vertex (dimension, index, or indicator)\n# We are just giving them random values to control point size for now\nvertices = data.frame(\n  name = unique(c(as.character(edges$from), as.character(edges$to)))\n) %&gt;% \n  left_join(counts, by = join_by(name == indicator)) %&gt;% \n  dplyr::rename('value' = count)\n\n# Add the dimension groupings to the vertices as well\nvertices$group = edges$group[match(vertices$name, edges$to)]\n\n# Calculate the angles to arrange indicator labels\nvertices$id = NA\nmyleaves = which(is.na(match(vertices$name, edges$from)))\nnleaves = length(myleaves)\nvertices$id[myleaves] = seq(1:nleaves)\nvertices$angle = 90 - 360 * vertices$id / nleaves\n\n# Calculate alignment of indicator labels\nvertices$hjust &lt;- ifelse(vertices$angle &lt; -90, 1, 0)\n\n# Flip label angles around 180 degrees if they are facing the wrong way\nvertices$angle &lt;- ifelse(vertices$angle &lt; -90, vertices$angle + 180, vertices$angle)\n\n\n## Create graph\n# Make ggraph object from edges and vertices\ngraph &lt;- graph_from_data_frame(edges, vertices = vertices)\n\n# Plot the graph\nggraph(graph, layout = 'dendrogram', circular = TRUE) +\n  \n  # Color edges by dimension\n  geom_edge_diagonal(color = 'black', width = 0.5) +\n  \n  # Create text for indicators using angles, hjust, and dimension groupings\n  geom_node_text(\n    aes(\n      x = x * 1.15,\n      y = y * 1.15,\n      filter = leaf,\n      label = name,\n      angle = angle,\n      hjust = hjust,\n      colour = group\n    ),\n    size = 3,\n    alpha = 1\n  ) +\n  \n  # Label indices within graph\n  geom_label_repel(\n    aes(\n      x = x,\n      y = y,\n      label = ifelse(name %in% unique(dat$Index), name, NA)\n    ),\n    label.padding = unit(0.15, \"lines\"),\n    label.r = unit(0.3, \"lines\"),\n    label.size = 0.05,\n    size = 2.25,\n    force = 0.1,    \n    force_pull = 1, \n    max.overlaps = 10 \n  ) +\n  \n  # Make the points for indicators based on secondary metric count\n  geom_node_point(\n    aes(\n      filter = leaf,\n      x = x * 1.07,\n      y = y * 1.07,\n      colour = group,\n      size = value\n    ),\n    alpha = 0.4\n  ) +\n  \n  # Various formatting options\n  scale_colour_manual(values = brewer.pal(3, 'Set1')) +\n  # scale_size_continuous(range = c(0.1, 7)) +\n  theme_void() +\n  theme(\n    plot.margin = unit(c(0, 0, 0, 0), \"cm\")\n  ) +\n  scale_colour_manual(\n    name = \"Indicator Use\",\n    values = brewer.pal(3, 'Set1'),\n    labels = c(\"Both\", \"Current Only\", \"Wiltshire Only\")\n  ) +\n  expand_limits(x = c(-2.5, 2.5), y = c(-2.5, 2.5))"
  },
  {
    "objectID": "pages/data_economics.html#dimension-overview",
    "href": "pages/data_economics.html#dimension-overview",
    "title": "Economics",
    "section": "",
    "text": "Shown in the diagram below are a total of 45 indicators within the economics dimension. Indices are labeled within the diagram. 17 indicators are both included in the Wiltshire et al. framework as well as being studied by one or more teams (red), 9 are included in the Wiltshire et al. but not currently belong studied (green), while 19 were not in the original framework, but have been added by one or more teams (blue).\nThe points beside each indicator name represent the number of secondary data metrics that have been aggregated for each indicator. Sources include USDA NASS, BLS, ERS, Census Bureau, and others. The quality and appropriateness of these metrics vary widely - I do not mean to suggest that having more of them means an indicator is more accurately better represented. For more information on the data sources, head to the Tables page to see metadata.\nOne other point to note here is that I removed several dozen metrics from BLS wage labor data broken down by NAICS industry code so as not to inflate that indicator relative to the others.\n\n\nCode\n## Load packages\npacman::p_load(\n  ggraph,\n  igraph,\n  dplyr,\n  RColorBrewer,\n  viridisLite,\n  ggrepel,\n  stringr\n)\n\nconflicted::conflicts_prefer(\n  dplyr::as_data_frame(),\n  .quiet = TRUE\n)\n\n## Load data for tree and metrics\ndat &lt;- readRDS('data/trees/econ_tree.rds') %&gt;% \n  select(Dimension:Source)\nmetadata_all &lt;- readRDS('data/sm_data.rds')[['metadata']]\nmeta &lt;- metadata_all %&gt;% \n  filter(\n    dimension == 'economics'\n  )\n\n# Rename metadata so it fits into formatting of tree data\n# This is quite not ideal - Note to harmonize this properly later\nmeta &lt;- meta %&gt;% \n  mutate(\n    indicator = str_to_sentence(indicator),\n    indicator = case_when(\n      str_detect(indicator, '^Assets') ~ 'Balance sheet (assets and liabilities)',\n      str_detect(indicator, '^Business failure') ~ 'Business failure rate of food business',\n      str_detect(indicator, '^Direct') ~ '% direct-to-consumer sales',\n      str_detect(indicator, '^Job avail') ~ 'Availability of good-paying jobs in food systems',\n      str_detect(indicator, '^Local sales') ~ '% local sales',\n      str_detect(indicator, '^Operator salary') ~ 'Operator salary / wage',\n      str_detect(indicator, '^Total sales') ~ 'Total sales / revenue',\n      str_detect(indicator, '^Wealth/income') ~ 'Wealth / income distribution',\n      TRUE ~ indicator\n    )\n  ) \n\n# Join counts of secondary data metrics to original dataset\n# Remove the NAICS variables - there are so many of them, don't add much\ncounts &lt;- meta %&gt;% \n  filter(str_detect(variable_name, '^lq|lvl|Lvl|Naics', negate = TRUE)) %&gt;% \n  group_by(indicator) %&gt;% \n  dplyr::summarize(count = n())\n\n\n## Make edges\n# include groupings by dimension, then combine them\nedges &lt;- list()\nedges$dim_ind &lt;- dat %&gt;% \n  select(Dimension, Index) %&gt;% \n  unique() %&gt;% \n  dplyr::rename(from = Dimension, to = Index) %&gt;% \n  mutate(group = to)\nedges$ind_ind &lt;- dat %&gt;% \n  select(Index, Indicator) %&gt;% \n  unique() %&gt;% \n  dplyr::rename(from = Index, to = Indicator) %&gt;% \n  mutate(group = from)\nedges &lt;- bind_rows(edges)\n\n# Add column for use (will use in colors of text?)\nedges$group &lt;- c(rep(NA, 10), dat$Source)\n\n\n## Make vertices\n# Each line is a single vertex (dimension, index, or indicator)\n# We are just giving them random values to control point size for now\nvertices = data.frame(\n  name = unique(c(as.character(edges$from), as.character(edges$to)))\n) %&gt;% \n  left_join(counts, by = join_by(name == indicator)) %&gt;% \n  dplyr::rename('value' = count)\n\n# Add the dimension groupings to the vertices as well\nvertices$group = edges$group[match(vertices$name, edges$to)]\n\n# Calculate the angles to arrange indicator labels\nvertices$id = NA\nmyleaves = which(is.na(match(vertices$name, edges$from)))\nnleaves = length(myleaves)\nvertices$id[myleaves] = seq(1:nleaves)\nvertices$angle = 90 - 360 * vertices$id / nleaves\n\n# Calculate alignment of indicator labels\nvertices$hjust &lt;- ifelse(vertices$angle &lt; -90, 1, 0)\n\n# Flip label angles around 180 degrees if they are facing the wrong way\nvertices$angle &lt;- ifelse(vertices$angle &lt; -90, vertices$angle + 180, vertices$angle)\n\n\n## Create graph\n# Make ggraph object from edges and vertices\ngraph &lt;- graph_from_data_frame(edges, vertices = vertices)\n\n# Plot the graph\nggraph(graph, layout = 'dendrogram', circular = TRUE) +\n  \n  # Color edges by dimension\n  geom_edge_diagonal(color = 'black', width = 0.5) +\n  \n  # Create text for indicators using angles, hjust, and dimension groupings\n  geom_node_text(\n    aes(\n      x = x * 1.15,\n      y = y * 1.15,\n      filter = leaf,\n      label = name,\n      angle = angle,\n      hjust = hjust,\n      colour = group\n    ),\n    size = 3,\n    alpha = 1\n  ) +\n  \n  # Label indices within graph\n  geom_label_repel(\n    aes(\n      x = x,\n      y = y,\n      label = ifelse(name %in% unique(dat$Index), name, NA)\n    ),\n    label.padding = unit(0.15, \"lines\"),\n    label.r = unit(0.3, \"lines\"),\n    label.size = 0.05,\n    size = 2.25,\n    force = 0.1,    \n    force_pull = 1, \n    max.overlaps = 10 \n  ) +\n  \n  # Make the points for indicators based on secondary metric count\n  geom_node_point(\n    aes(\n      filter = leaf,\n      x = x * 1.07,\n      y = y * 1.07,\n      colour = group,\n      size = value\n    ),\n    alpha = 0.4\n  ) +\n  \n  # Various formatting options\n  scale_colour_manual(values = brewer.pal(3, 'Set1')) +\n  # scale_size_continuous(range = c(0.1, 7)) +\n  theme_void() +\n  theme(\n    plot.margin = unit(c(0, 0, 0, 0), \"cm\")\n  ) +\n  scale_colour_manual(\n    name = \"Indicator Use\",\n    values = brewer.pal(3, 'Set1'),\n    labels = c(\"Both\", \"Current Only\", \"Wiltshire Only\")\n  ) +\n  expand_limits(x = c(-2.5, 2.5), y = c(-2.5, 2.5))"
  },
  {
    "objectID": "pages/data_economics.html#distributions",
    "href": "pages/data_economics.html#distributions",
    "title": "Economics",
    "section": "2 Distributions",
    "text": "2 Distributions\nWe are taking out the abundant but largely redundant BLS NAICS wage data variables to leave us with a more approachable set of 46 variables to explore here. First just show univariate distributions by county.\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr\n)\n\nsource('dev/data_pipeline_functions.R')\nsource('dev/filter_fips.R')\nmetrics &lt;- readRDS('data/sm_data.rds')[['metrics']]\nmetadata &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\n# Use metadata to get help filter by dimension\necon_meta &lt;- metadata %&gt;% \n  filter(dimension == 'economics')\n\n# Filter to economics dimension\necon_metrics &lt;- metrics %&gt;% \n  filter(variable_name %in% econ_meta$variable_name)\n\n# Filter to latest year and new (post-2024) counties\n# Also remove NAICS variables to leave us with an approachable number\n# And pivot wider so it is easier to get correlations\necon_metrics_latest &lt;- econ_metrics %&gt;%\n  filter_fips(scope = 'new') %&gt;% \n  get_latest_year() %&gt;% \n  filter(\n    str_detect(\n      variable_name, \n      'Naics|NAICS|^lq|^avgEmpLvl|expHiredLaborPercOpExp', \n      negate = TRUE\n    )\n  )\n\n# Pivot wider for easier correlations below\necon_metrics_latest &lt;- econ_metrics_latest %&gt;% \n  select(fips, variable_name, value) %&gt;% \n  unique() %&gt;% \n  mutate(variable_name = str_split_i(variable_name, '_', 1)) %&gt;% \n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;% \n  unnest(!fips) %&gt;% \n  mutate(across(c(civLaborForce:last_col()), as.numeric))\n\n\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr\n)\n\nplots &lt;- map(names(econ_metrics_latest)[-1], \\(var){\n  if (is.character(econ_metrics_latest[[var]])) {\n    econ_metrics_latest %&gt;% \n      ggplot(aes(x = !!sym(var))) + \n      geom_bar(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else if (is.numeric(econ_metrics_latest[[var]])) {\n    econ_metrics_latest %&gt;% \n      ggplot(aes(x = !!sym(var))) + \n      geom_density(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else {\n    return(NULL)\n  }\n}) \n\n# Arrange them in 4 columns\nggarrange(\n  plotlist = plots,\n  ncol = 4,\n  nrow = 12\n)"
  },
  {
    "objectID": "pages/data_economics.html#correlation-heatmap",
    "href": "pages/data_economics.html#correlation-heatmap",
    "title": "Economics",
    "section": "3 Correlation Heatmap",
    "text": "3 Correlation Heatmap\nThrowing those same variables into a correlation matrix. Hover to see variable names, Pearson correlation, and p-values.\n\n\nCode\npacman::p_load(\n  dplyr,\n  tidyr,\n  tibble,\n  stringr,\n  purrr,\n  tidyr,\n  ggplot2,\n  plotly,\n  reshape,\n  Hmisc,\n  viridisLite\n)\n\n# Arrange variables in some halfway reasonable order\ncor_dat &lt;- econ_metrics_latest %&gt;% \n  select(\n    matches('Code_|metro'),\n    matches('employ|abor|Worker'),\n    matches('Sales'),\n    matches('Earn|Income'),\n    everything(),\n    -fips,\n    -matches('expHiredLaborPercOpExp') # This one didn't come through\n  )\n\n# Make a correlation matrix using all the selected variables\ncor &lt;- cor_dat %&gt;% \n  as.matrix() %&gt;% \n  rcorr()\n\n# Melt correlation values and rename columns\ncor_r &lt;- melt(cor$r) %&gt;% \n  setNames(c('var_1', 'var_2', 'value'))\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Make heatmap with custom text aesthetic for tooltip\nplot &lt;- cor_r %&gt;% \n  ggplot(aes(var_1, var_2, fill = value, text = paste0(\n    'Var 1: ', var_1, '\\n',\n    'Var 2: ', var_2, '\\n',\n    'Correlation: ', format(round(value, 3), nsmall = 3), '\\n',\n    'P-Value: ', format(round(p.value, 3), nsmall = 3)\n  ))) + \n  geom_tile() + \n  scale_fill_viridis_c() + \n  theme(axis.text.x = element_text(hjust = 1, angle = 45)) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = 'Correlation'\n  )\n\n# Convert to interactive plotly figure with text tooltip\nggplotly(\n  plot, \n  tooltip = 'text',\n  width = 1000,\n  height = 800\n)"
  },
  {
    "objectID": "pages/data_economics.html#pca",
    "href": "pages/data_economics.html#pca",
    "title": "Economics",
    "section": "4 PCA",
    "text": "4 PCA\nPCA is a popular tool in this area for exploring unique variation with many collinear variables. It is a way to reduce the dimensionality of the data into fewer, more interpretable principal components.\nIt also requires complete data, which we do not have. So we either have to run a probabililistic PCA or run imputations. I’m using a random forest algorithm to impute data here as a first pass (Stekhoven and Bühlmann 2012). This really warrants a deeper dive into the type and severity of missingness though, and PPCA is likely the better option in the end.\n\n\nCode\npacman::p_load(\n  missForest\n)\n\n# Wrangle dataset. Need all numeric vars or factor vars. And can't be tibble\n# Also removing character vars - can't use these in PCA\ndat &lt;- econ_metrics_latest %&gt;%\n  select(where(is.numeric)) %&gt;%\n  as.data.frame()\n# get_str(dat)\n\n# Check missing variables\n# skimr::skim(dat)\n\n# Impute missing variables\nset.seed(42)\nmf_out &lt;- dat %&gt;%\n  missForest(\n    ntree = 200,\n    mtry = 10,\n    verbose = FALSE,\n    variablewise = FALSE\n  )\n\n# Save imputed dataset\nimp &lt;- mf_out$ximp\n\n# Print OOB\nmf_out$OOBerror\n\n\nOut of bag error is shown as normalized root mean square error. Now we can explore how many composite factors is appropriate for the data.\n\n\nCode\npacman::p_load(\n  psych\n)\nVSS(imp)\nfa.parallel(imp)\n\n\nVSS gives a wide range from 2 to 8, MAP shows 7, parallel analysis shows 4. I tend to trust PA the most, so let’s go with 4.\n\n\nCode\n(pca_out &lt;- pca(imp, nfactors = 4))\n\nplot(pca_out$values)\nabline(h = 1)\n\n\nFrom the scree plot and eigenvalues it looks like the first three components bear lots of unique variance, but after that there is no clear elbow where a qualitative decision can be made to choose a certain number of components. The Kaiser-Guttman rule suggests keeping any compents with an eigenvalue &gt; 1 (at the horizontal line), but we can see here that this is a rather dubious distinction.\nIf we look at the output from the PCA call, we can see how closely each variable (row) correlates with each component (columns 1-4). The variables most associated with Component #1 are the farm labor variables - numbers of workers, labor expenses, etc. They also tend to be raw figures, and probably have more to do with population than anything else. Component #2 is made up mostly of generic employment figures - total civilian labor force, total employed, total unemployed. These are not specific to food systems. Component #3 has a curious collection of median earnings variables and ‘per farm’ variables like acres per farm, income per farm, and local and direct-to-consumer sales. Component #4 does not represent much unique variance, and loooks like a grab bag of variables.\nA couple of early takeaways here are that the raw figures that are tied to population probably shouldn’t be mixed with other variables like proportions. We could try normalizing all the variables so that raw variables are not disproportionately weighted. But it might make more sense to avoid raw counts and dollar amounts entirely."
  },
  {
    "objectID": "pages/data_environment_maps.html",
    "href": "pages/data_environment_maps.html",
    "title": "Environment: Maps",
    "section": "",
    "text": "Taking a quick tour through some of the spatial data here. Most of these metrics will also be available to peruse on the Shiny app, with the exception of those that are hard to aggregate, like biodiversity hotspots."
  },
  {
    "objectID": "pages/data_environment_maps.html#land-use",
    "href": "pages/data_environment_maps.html#land-use",
    "title": "Environment: Maps",
    "section": "1 Land Use",
    "text": "1 Land Use\nThis is the MRLC 30m LULC layer from 2023. Below the map, you can find a table with codes and descriptions. Sort or expand to see all the values.\n\n\nCode\nlulc &lt;- readRDS('data/sm_data.rds')[['mrlc_lulc_ne']]\n# sm_data &lt;- readRDS('data/sm_data.rds')\ncounties &lt;- readRDS('data/sm_data.rds')[['ne_counties_2024']]\n\nlulc_map &lt;- lulc %&gt;% \n  mapview(\n    layer.name = 'LULC'\n  ) + \n  mapview(\n    counties,\n    alpha.regions = 0,\n    color = 'black',\n    col.regions = 'black',\n    lwd = 1.25,\n    layer.name = 'Counties'\n  )\n\nlulc_map@map %&gt;% \n  addFullscreenControl()\n\n\n\n\nCode\npacman::p_load(\n  reactable,\n  dplyr,\n  stringr\n)\n\nmeta &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\nlulc_codes &lt;- meta %&gt;% \n  filter(\n    str_detect(variable_name, '^lulc'),\n    str_detect(variable_name, 'NoData|Diversity', negate = TRUE)\n  ) %&gt;% \n  select(definition) %&gt;% \n  mutate(\n    Value = c(11, 12, 21, 22, 23, 24, 31, 41, 42, 43, 52, 71, 81, 82, 90, 95),\n    Class = c(\n      rep('Water', 2),\n      rep('Developed', 4),\n      'Barren',\n      rep('Forest', 3),\n      'Shrubland',\n      'Herbaceous',\n      rep('Planted/Cultivated', 2),\n      rep('Wetlands', 2)\n    ),\n    Type = c(\n      'Open Water',\n      'Ice or Snow',\n      'Developed, Open Space',\n      'Developed, Low Intensity',\n      'Developed, Medium Intensity',\n      'Developed, High Intensity',\n      'Barren Land (Rock / Sand / Clay)',\n      'Deciduous Forest',\n      'Evergreen Forest',\n      'Mixed Forest',\n      'Shrub / Scrub',\n      'Grassland / Herbaceous',\n      'Pasture / Hay',\n      'Cultivated Crops',\n      'Woody Wetlands',\n      'Emergent Herbaceous Wetlands'\n    )\n  ) %&gt;% \n  select(\n    Value,\n    Class,\n    Type,\n    Description = definition\n  )\n\nreactable(\n  lulc_codes,\n  sortable = TRUE,\n  resizable = TRUE,\n  filterable = TRUE,\n  searchable = FALSE,\n  pagination = TRUE,\n  bordered = TRUE,\n  wrap = TRUE,\n  rownames = FALSE,\n  striped = TRUE,\n  defaultPageSize = 5,\n  showPageSizeOptions = FALSE,\n  highlight = TRUE,\n  style = list(fontSize = \"14px\"),\n  compact = TRUE,\n  columns = list(\n    Value = colDef(minWidth = 40),\n    Class = colDef(minWidth = 100),\n    Type = colDef(minWidth = 100),\n    Description = colDef(minWidth = 500)\n  )\n)"
  },
  {
    "objectID": "pages/data_environment_maps.html#land-use-diversity",
    "href": "pages/data_environment_maps.html#land-use-diversity",
    "title": "Environment: Maps",
    "section": "2 Land Use Diversity",
    "text": "2 Land Use Diversity\nLULC Diversity is derived from the MRLC LULC layer above. LULC types are aggregated by category (water, developed, barren, forest, shrubland, herbaceous, cultivated, wetlands) and Shannon diversity is calculated for each county. It makes for an interesting metric, but I’m not sure it makes for a strong normative metric. If anyone has thoughts on what the “right” amount of LULC diversity is, I’d love to hear from you.\n\n\nCode\ndiv &lt;- readRDS('data/sm_data.rds')[['lulc_div']]\n\ndiv_map &lt;- mapview(\n  div,\n  zcol = 'lulc_div',\n  label = 'county_name',\n  layer.name = 'LULC Diversity',\n  popup = popupTable(\n    div,\n    zcol = c(\n      'county_name',\n      'lulc_div'\n    ),\n    row.numbers = FALSE,\n    feature.id = FALSE\n  )\n)\n\ndiv_map@map %&gt;% \n  addFullscreenControl()"
  },
  {
    "objectID": "pages/data_environment_maps.html#rare-threatened-and-endangered-species",
    "href": "pages/data_environment_maps.html#rare-threatened-and-endangered-species",
    "title": "Environment: Maps",
    "section": "3 Rare, Threatened and Endangered Species",
    "text": "3 Rare, Threatened and Endangered Species\nThe Vermont ANR Biofinder has lots of great layers. Technical abstracts for these layers can be found here. Below is a map of rare, threatened, and endangered species polygons statewide. Note that these are lumped together into a multi-polygon to save some space, but the individual polygons didn’t provide a whole lot useful information anyway.\n\n\nCode\npacman::p_load(\n  mapview,\n  leaflet.extras,\n  sf\n)\nrte &lt;- readRDS('data/sm_data.rds')[['biofinder_rte_spp']] %&gt;% \n  summarize()\nrte_map &lt;- mapview(\n  rte,\n  layer.name = 'RTE Species',\n  col.regions = '#154734'\n)\nrte_map@map %&gt;%\n  addFullscreenControl()"
  },
  {
    "objectID": "pages/data_environment_maps.html#uncommon-species",
    "href": "pages/data_environment_maps.html#uncommon-species",
    "title": "Environment: Maps",
    "section": "4 Uncommon Species",
    "text": "4 Uncommon Species\nBiofinder also lists uncommon species as those facing a “moderate risk of extinction or extirpation due to restricted range, relatively few populations (often 80 or fewer), recent widespread declines, and other factors.” Same as above, these are lumped together into a single polygon for convenience.\n\n\nCode\nuncommon &lt;- readRDS('data/sm_data.rds')[['biofinder_uncommon_spp']] %&gt;% \n  summarize()\nuncommon_map &lt;- mapview(\n  uncommon,\n  layer.name = 'Uncommon Species',\n  col.regions = '#154734'\n)\nuncommon_map@map %&gt;%\n  addFullscreenControl()"
  },
  {
    "objectID": "pages/data_environment_maps.html#forest-biomass",
    "href": "pages/data_environment_maps.html#forest-biomass",
    "title": "Environment: Maps",
    "section": "5 Forest Biomass",
    "text": "5 Forest Biomass\nThe TreeMap 2016 dataset is quite comprehensive national survey of forest health and diversity. Updates are infrequent, but this is the best layer I’ve found to address biomass. The raster is at 30m.\n\n\nCode\ntreemap &lt;- readRDS('data/sm_data.rds')[['treemap_biomass']]\ncounties &lt;- readRDS('data/sm_data.rds')[['ne_counties_2024']]\n\ntreemap_map &lt;- treemap %&gt;%\n  mapview(\n    layer.name = 'Biomass (tons per acre)',\n    col.regions = viridis(n = 256)\n  ) +\n  mapview(\n    counties,\n    alpha.regions = 0,\n    color = 'black',\n    col.regions = 'black',\n    lwd = 1.25,\n    layer.name = 'Counties'\n  )\ntreemap_map@map %&gt;%\n  addFullscreenControl()\n\n\nShown below is the mean live above-ground biomass aggregated by county so that it plays well with other metrics. Note that it is measured in tons per acre of forest, non-forest cells were removed from analysis. So, it is not showing density of forest, just biomass in existing forest. This is why the more urban counties still show a reasonable density of live biomass. There is lots more that can be pulled out of this dataset, like dead/down carbon, tree stocking, live canopy cover, height, volume, tree per acre, etc. More info can be found here.\n\n\nCode\npacman::p_load(\n  mapview,\n  dplyr,\n  sf,\n  viridisLite,\n  leaflet,\n  leafpop,\n  stars\n)\n\nbiomass &lt;- readRDS('data/sm_data.rds')[['mean_biomass']]\nbiomass_map &lt;- mapview(\n  biomass,\n  zcol = 'mean_biomass',\n  layer.name = 'Mean Live Above&lt;br&gt;Ground Biomass&lt;br&gt;(tons per acre)',\n  label = 'county_name',\n  popup = popupTable(\n    biomass,\n    zcol = c(\n      'county_name',\n      'mean_biomass'\n    ),\n    feature.id = FALSE,\n    row.numbers = FALSE\n  )\n)"
  },
  {
    "objectID": "pages/data_production.html",
    "href": "pages/data_production.html",
    "title": "Production",
    "section": "",
    "text": "The first plot shows all the production indicators from both the current studies and the original framework in the y-axis. Orange indicates that the indicator is only being used in the current studies, purple that it is only included in the Wiltshire framework, and green that the indicator is used in both the framework and current studies.\nThe x-axis shows the number of secondary data metrics that have been collected to represent those indicators. You can see that there are some indicators for which there exist many data, but many indicators for which I have found little to represent them.\nValue-added market indicators are pulled from various NASS, as are the total quantity of food and forest products and production inputs. There is plenty more that might be pulled from NASS here. Imports and exports are from the Economic Research Service. The exports data are far more detailed than the imports. The former are disaggregated by category at the state level (fresh fruit, processed fruit, dairy…) which is why there are a heap of metrics for it. The import data is weak - I could only find the value of the top five agricultural imports for each state, not a total. Recalls are from FDA records, but I have not any helpful information the impact of recalls in terms of food safety. Crop diversity is represented in the richness indicator by the Cropland CROS data set, which provides estimates of the area of farmland devoted to specific crops across the US. I have disaggregated these at the county and state levels here.\nYou can see there is plenty more in the frameworks that are not represented by secondary data here, particularly related to the consumer side - marketability, nutrition, food waste, and safety. I suspect some of these indicators will migrate toward other dimensions in the refinement process as well. But this does help identify some gaps in the data.\nCode\npacman::p_load(\n  dplyr,\n  ggplot2,\n  stringr,\n  plotly,\n  RColorBrewer\n)\n\n# Load production tree with use notes\nprod_tree &lt;- read.csv('data/trees/prod_tree_with_use.csv')\n\n# Counts of secondary data metrics\ncounts &lt;- meta %&gt;% \n  group_by(Indicator) %&gt;% \n  dplyr::summarize(count = n())\n\n# Join to Wiltshire framework\ncolors &lt;- RColorBrewer::brewer.pal(n = 3, name = 'Dark2')\ndat &lt;- full_join(prod_tree, counts, by = join_by(Indicator == Indicator)) %&gt;% \n  arrange(Indicator) %&gt;% \n  mutate(\n    count = ifelse(is.na(count), 0, count),\n    label_color = case_when(\n      Use == 'both' ~ colors[1],\n      Use == 'wiltshire' ~ colors[3],\n      Use == 'current' ~ colors[2]\n    )\n  )\n# [1] \"#1B9E77\" \"#D95F02\" \"#7570B3\"\n\n# Plot\ndat %&gt;%\n  ggplot(aes(x = Indicator, y = count)) +\n  geom_col(\n    color = 'black',\n    fill = 'grey'\n  ) +\n  geom_point(\n    data = dat,\n    aes(x = 1, y = 1, color = Use),\n    inherit.aes = FALSE,\n    alpha = 0,\n    size = -1\n  ) +\n  scale_color_manual(\n    name = \"Indicator Use:\",\n    values = c(\n      \"both\" = colors[1],\n      \"wiltshire\" = colors[2],\n      \"current\" = colors[3]\n    ),\n    labels = c(\n      'Both',\n      'Wiltshire Only',\n      'Current Only'\n    )\n  ) +\n  theme_classic() +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.text.y = element_text(color = rev(dat$label_color)),\n    axis.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    legend.title = element_text(size = 12),\n    legend.position = \"bottom\",\n    plot.margin = margin(t = 10, r = 75, b = 10, l = 10)\n  ) +\n  guides(\n    color = guide_legend(override.aes = list(size = 4, alpha = 1))\n  ) +\n  coord_flip() +\n  labs(y = 'Secondary Data Count')\nOtherwise, I won’t be diving into the usual PCA exploration for the production dataset because we have collected enough metrics to put together a mostly full, mostly coherent example framework with which we can try aggregating data. This should be coming in January."
  },
  {
    "objectID": "pages/data_production.html#crop-diversity",
    "href": "pages/data_production.html#crop-diversity",
    "title": "Production",
    "section": "1 Crop Diversity",
    "text": "1 Crop Diversity\nI wanted to highlight this cropland data layer from USDA NASS in collaboration with USGS, NRCS, and FSA, among other agencies. It’s a crop-specific LULC layer derived from satellite imagery and ground-truthing. It seems to be about the best thrust at crop diversity across regions that I’ve found, but it also is certainly tailored toward primary crops, and may not represent New England very well. I’d love to hear thoughts on how useful this would be in New England.\n\n\nCode\npacman::p_load(\n  mapview,\n  sf,\n  stars,\n  leaflet,\n  leaflet.extras,\n  leafpop\n)\ncounties_sf &lt;- readRDS('data/sm_data.rds')[['ne_counties_2024']]\nfips_key &lt;- readRDS('data/sm_data.rds')[['fips_key']]\ncrop &lt;- readRDS('data/sm_data.rds')[['cropland_cros']]\n\ncounties &lt;- left_join(counties_sf, fips_key)\n\ndiv_map &lt;- mapview(\n  crop,\n  zcol = '2023_30m_cdls',\n  layer.name = 'Cropland Data Layer'\n) + \n  mapview(\n    counties,\n    label = 'county_name',\n    alpha.regions = 0\n  )\n\ndiv_map@map %&gt;% \n  addFullscreenControl()\n\n\nI went on to use this layer to calculate Shannon diversity for crop types at the county and state levels. Here is what it looks like:\n\n\nCode\npacman::p_load(\n  mapview,\n  leaflet,\n  stringr,\n  sf\n)\nsource('dev/data_pipeline_functions.R')\n\ndat &lt;- readRDS('data/sm_data.rds')\n\ndiv &lt;- dat$metrics %&gt;% \n  filter(\n    variable_name == 'cropDiversity',\n    str_length(fips) == 5\n  ) %&gt;% \n  get_latest_year() %&gt;% \n  mutate(value = round(as.numeric(value), 3))\n\ndiv &lt;- left_join(dat$ne_counties_2021, div)\nmapview(\n  div,\n  zcol = 'value',\n  label = 'value',\n  layer.name = 'Crop Diversity'\n)\n\n\nSimilarly, we could pull crop richness out of this dataset, but I have a feeling that the bias toward commodity crops would make that a bit more problematic."
  },
  {
    "objectID": "pages/data_production.html#distribution-plots",
    "href": "pages/data_production.html#distribution-plots",
    "title": "Production",
    "section": "2 Distribution Plots",
    "text": "2 Distribution Plots\n\n2.1 By County\nNote that while most of the available secondary data is at the county level, the environment dimension includes a fair amount at the state level as well. This includes greenhouse gas emissions and water quality surveys. For now, I’ll just show these separately, but some creative aggregation will have to happen eventually.\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr\n)\nsource('dev/data_pipeline_functions.R')\nsource('dev/filter_fips.R')\nmetrics &lt;- readRDS('data/sm_data.rds')[['metrics']]\nmetadata &lt;- readRDS('data/sm_data.rds')[['metadata']]\n\n# Use metadata to get help filter by dimension\nprod_meta &lt;- metadata %&gt;%\n  filter(dimension == 'production')\n\n# Filter to economics dimension\nprod_metrics &lt;- metrics %&gt;%\n  filter(variable_name %in% prod_meta$variable_name)\n\n# env_metrics$variable_name %&gt;% unique\n# get_str(env_metrics)\n\n# Filter to latest year and new (post-2024) counties\n# And pivot wider so it is easier to get correlations\nprod_county &lt;- prod_metrics %&gt;%\n  filter_fips(scope = 'counties') %&gt;%\n  get_latest_year() %&gt;%\n  select(fips, variable_name, value) %&gt;%\n  mutate(variable_name = str_split_i(variable_name, '_', 1)) %&gt;%\n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;%\n  unnest(!fips) %&gt;%\n  mutate(across(c(2:last_col()), as.numeric))\n\n# Save temp file for use in analysis script\nsaveRDS(prod_county, 'data/temp/prod_county.rds')\n\n## Plot\nplots &lt;- map(names(prod_county)[-1], \\(var){\n  if (is.character(prod_county[[var]])) {\n    env_county %&gt;%\n      ggplot(aes(x = !!sym(var))) +\n      geom_bar(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else if (is.numeric(prod_county[[var]])) {\n    prod_county %&gt;%\n      ggplot(aes(x = !!sym(var))) +\n      geom_density(\n        fill = 'lightblue',\n        color = 'royalblue',\n        alpha = 0.5\n      ) +\n      theme_classic() +\n      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))\n  } else {\n    return(NULL)\n  }\n})\n\n\n# Arrange them in 4 columns\nggarrange(\n  plotlist = plots,\n  ncol = 3,\n  nrow = 4\n)\n\n\n\n\n2.2 By State\n\n\nCode\npacman::p_load(\n  dplyr,\n  purrr,\n  ggplot2,\n  rlang,\n  ggpubr,\n  tidyr\n)\n\nstate_codes &lt;- readRDS('data/sm_data.rds')[['fips_key']] %&gt;%\n  select(fips, state_code)\n\nprod_state &lt;- prod_metrics %&gt;%\n  filter_fips(scope = 'state') %&gt;%\n  get_latest_year() %&gt;%\n  select(fips, variable_name, value) %&gt;%\n  mutate(variable_name = str_split_i(variable_name, '_', 1)) %&gt;%\n  pivot_wider(\n    names_from = 'variable_name',\n    values_from = 'value'\n  ) %&gt;%\n  unnest(!fips) %&gt;%\n  mutate(across(c(2:last_col()), as.numeric)) %&gt;%\n  left_join(state_codes, by = 'fips')\n\n# Save temp data file for use in analysis script\nsaveRDS(prod_state, 'data/temp/prod_state.rds')\n\n# Variables to map. \nvars &lt;- names(prod_state)[-c(1, 43)]\n\n## Plot\nplots &lt;- map(vars, \\(var){\n  prod_state %&gt;%\n    ggplot(aes(y = !!sym(var), x = state_code, color = state_code)) +\n    geom_point(\n      alpha = 0.5,\n      size = 3\n    ) +\n    theme_classic() +\n    theme(\n      plot.margin = unit(c(rep(0.5, 4)), 'cm'),\n      legend.position = 'none'\n    ) +\n    labs(\n      x = 'State'\n    )\n})\n\n# Arrange them in 4 columns\nggarrange(\n  plotlist = plots,\n  ncol = 4,\n  nrow = 11\n)"
  },
  {
    "objectID": "pages/data_production.html#bivariate-plots",
    "href": "pages/data_production.html#bivariate-plots",
    "title": "Production",
    "section": "3 Bivariate Plots",
    "text": "3 Bivariate Plots\nUsing a selection of variables at the county level.\n\n\nCode\npacman::p_load(\n  GGally\n)\n\n# Neat function for mapping colors to ggpairs plots\n# https://stackoverflow.com/questions/45873483/ggpairs-plot-with-heatmap-of-correlation-values\nmap_colors &lt;- function(data,\n                       mapping,\n                       method = \"p\",\n                       use = \"pairwise\",\n                       ...) {\n  # grab data\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n\n  # calculate correlation\n  corr &lt;- cor(x, y, method = method, use = use)\n  colFn &lt;- colorRampPalette(c(\"blue\", \"white\", \"red\"), interpolate = 'spline')\n  fill &lt;- colFn(100)[findInterval(corr, seq(-1, 1, length = 100))]\n\n  # correlation plot\n  ggally_cor(data = data, mapping = mapping, color = 'black', ...) +\n    theme_void() +\n    theme(panel.background = element_rect(fill = fill))\n}\n\nlower_function &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(color = \"blue\", fill = \"grey\", ...) +\n    theme_bw()\n}\n\n# Rename variables to be shorter\nprod_county %&gt;%\n  select(-fips) %&gt;% \n  ggpairs(\n    upper = list(continuous = map_colors),\n    lower = list(continuous = lower_function),\n    axisLabels = 'show'\n  ) +\n  theme(\n    strip.text = element_text(size =  5),\n    axis.text = element_text(size =   5),\n    legend.text = element_text(size = 5)\n  )"
  },
  {
    "objectID": "pages/data_production.html#sec-correlations",
    "href": "pages/data_production.html#sec-correlations",
    "title": "Production",
    "section": "4 Correlations",
    "text": "4 Correlations\nOnly showing correlations by county because we don’t have enough observations to run it by state.\n\n\nCode\npacman::p_load(\n  dplyr,\n  tidyr,\n  tibble,\n  stringr,\n  purrr,\n  tidyr,\n  ggplot2,\n  plotly,\n  reshape,\n  Hmisc,\n  viridisLite\n)\n\n# get_str(env_county)\n\ncor &lt;- prod_county %&gt;%\n  select(-fips) %&gt;%\n  as.matrix() %&gt;%\n  rcorr()\n\n# Melt correlation values and rename columns\ncor_r &lt;- melt(cor$r) %&gt;%\n  setNames(c('var_1', 'var_2', 'value'))\n\n# Save p values\ncor_p &lt;- melt(cor$P)\np.value &lt;- cor_p$value\n\n# Make heatmap with custom text aesthetic for tooltip\nplot &lt;- cor_r %&gt;%\n  ggplot(aes(var_1, var_2, fill = value, text = paste0(\n    'Var 1: ', var_1, '\\n',\n    'Var 2: ', var_2, '\\n',\n    'Correlation: ', format(round(value, 3), nsmall = 3), '\\n',\n    'P-Value: ', format(round(p.value, 3), nsmall = 3)\n  ))) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  theme(axis.text.x = element_text(hjust = 1, angle = 45)) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = 'Correlation'\n  )\n\n# Convert to interactive plotly figure with text tooltip\nggplotly(\n  plot,\n  tooltip = 'text',\n  width = 800,\n  height = 500\n)"
  },
  {
    "objectID": "pages/home.html",
    "href": "pages/home.html",
    "title": "Sustainability Metrics",
    "section": "",
    "text": "Caution\n\n\n\n\n\nThe Sustainability Metrics project, as well as this site itself, are works in progress. All data and analyses shown here are preliminary. If you have any questions, comments, or suggestions about this site or the accompanying Shiny app, feel free to reach out to Chris at christopher.donovan@uvm.edu.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/home.html#sec-intro",
    "href": "pages/home.html#sec-intro",
    "title": "Sustainability Metrics",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n\n\nIntervale Center, Burlington, Vermont. Copyright: Sally McCay, UVM Photo.\n\n\nResilient food systems are increasingly recognized as essential, not only in meeting human needs, but in doing so within planetary bounds (Conijn et al. 2018). Approximately 42% of world’s population depend on agriculture for employment, which is a challenging endeavor in the face of farm consolidation, changing consumption patterns, and climate change (Giller et al. 2021; Aznar-Sánchez et al. 2019). Food systems themselves are responsible for one-third of greenhouse gas emissions, while anthropogenic climate change has reduced agricultural output by 21% in the last 60 years (Crippa et al. 2021; Ortiz-Bobea et al. 2021).\nTools for diagnosing and monitoring the sustainability of food systems are thus vital (Fanzo et al. 2021). However, there is little consensus on how to define, let alone measure food system sustainability (Allen and Prosperi 2016; Béné et al. 2019). And while there is an abundance of research at the global level (Bathaei and Štreimikienė 2023; Chaudhary, Gustafson, and Mathys 2018), there exist gaps in understanding at the local, regional, and landscape levels (Dale et al. 2012).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/home.html#sustainability-metrics",
    "href": "pages/home.html#sustainability-metrics",
    "title": "Sustainability Metrics",
    "section": "2 Sustainability Metrics",
    "text": "2 Sustainability Metrics\n\n\n\nSpread from the Climate Kitchen harvest dinner. Photo credit: Colleen Goodhue, FSRC.\n\n\nThe Sustainability Metrics project is an effort to develop both the conceptual and methodological frameworks to define and measure regional food system sustainability in New England. The framework could be used to monitor sustainability over time and inform interventions at the policy and farm levels, creating a healthier and more resilient food system for both social and ecological ends.\nThe project is led by the Food Systems Research Center at the University of Vermont in partnership with, and funded by, the USDA ARS Food Systems Research Unit in Burlington, Vermont. Five teams of researchers and numerous community partners are currently conducting primary research on the development and measurement of indicators for food system sustainability. You can find more information about this work at the UVM FSRC Sustainability Metrics website. For now, what you will find here is a growing collection of secondary data, visualizations, and exploratory analyses to help support the project.\nMetadata and citations will be provided throughout the document, but it is worth appreciating the work of the folks at USDA AMS Food and Agriculture Mapper and Explorer in particular, as many of the data shown here were cleaned and compiled in their data warehouse. Considerable inspiration was also taken from the Food Systems Dashboard, developed by the Global Alliance for Improved Nutrition.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/home.html#about-fsrc",
    "href": "pages/home.html#about-fsrc",
    "title": "Sustainability Metrics",
    "section": "3 About FSRC",
    "text": "3 About FSRC\nThe Food Systems Research Center at the University of Vermont is transforming the research landscape by funding collaborative projects that put people and the planet first, break down traditional academic silos and are integrated with and responsive to the needs of the communities we serve, including decision-makers, farmers, and food systems actors.\nRooted in the belief that no one group can find the answers alone, FSRC empowers researchers to work together across disciplines to address critical issues like soil health, food security, and climate resilience. Instead of funding research that leads to short-term fixes, our commitment is to give researchers the freedom, resources, and time they need to do relevant research that will inform policies, practices, and programs that will long outlast their work.\nFSRC considers the relationship of food systems across scales from local to global and is a partnership between UVM and the U.S. Department of Agriculture (USDA) Agricultural Research Service (ARS). FSRC’s transdisciplinary approach prioritizes research that studies food systems as a whole, including the networks of people, institutions, physical infrastructure, and natural resources through which food is grown, processed, distributed, sold, prepared, and eaten.\nLearn more about us at the Food Systems Research Center website.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/home.html#license",
    "href": "pages/home.html#license",
    "title": "Sustainability Metrics",
    "section": "4 License",
    "text": "4 License\n\n    This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. \n\n\n    The code is licensed under the GNU General Public License v3.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/indicator_distributions.html",
    "href": "pages/indicator_distributions.html",
    "title": "Indicator Distributions",
    "section": "",
    "text": "Now that our metrics have been aggregated into indicators, we can observe the univariate distributions of the indicators themselves. We will do this for all six sets of transformations.\n\n1 Summary\nBelow are counts of skewed indicator distributions for each transformation.\n\n\nCode\n# Load scores data for all transformations\nscores &lt;- readRDS('data/state_score_iterations.rds')\nget_str(scores)\n\n# Rearrange data to make a single DF as table\nout &lt;- map_vec(scores, ~ {\n  .x[['indicator_scores']] %&gt;% \n    dplyr::filter(!state %in% c('US_mean', 'US_median', 'NewEng')) %&gt;% \n    dplyr::select(-state) %&gt;% \n    psych::describe() %&gt;% \n    dplyr::select(skew) %&gt;% \n    dplyr::filter(abs(skew) &gt; 2) %&gt;% \n    nrow()\n}) %&gt;% \n  as.data.frame() %&gt;% \n  tibble::rownames_to_column() %&gt;% \n  setNames(c('Transformation', 'Indicators with Skew &gt; 2')) %&gt;% \n  arrange(Transformation)\nout\n\n\n\n\nCode\nget_reactable(\n  out, \n  fullWidth = FALSE,\n  searchable = FALSE,\n  defaultColDef = colDef(\n    minWidth = 200\n  )\n)\n\n\n\n\n\n\nIt looks like the Box Cox procedure transformed distributions at the metric level enough that there are no skewed distributions at the indicator level. Both the min-max and z-score transformations left us with only three skewed indicators: access to culturally appropriate food, total quantity exported, and total quantity imported. The latter two are not surprising, as big agricultural states likely have a disproportionate impact. The Z-score with geometric aggregation is a slightly nonsensical combination of methods that we will ignore for now.\nThis is providing some evidence that Box-Cox transformations could be warranted on at least these three problematic indicators (or the metrics therein). Below are the indicator distributions for each transformation. Inidicators with skew &gt; 2 are shown in red.\n\n\n2 Rank Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'rank_arithmetic')\n\n\n\n\n\nDistributions of indicators at the state level for the rank arithmetic transformations\n\n\n\n\n\n\n3 Rank Geometric\n\n\nCode\nget_indicator_distributions(scores, 'rank_geometric')\n\n\n\n\n\nDistributions of indicators at the state level for the rank geometric transformations\n\n\n\n\n\n\n4 Winsor Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'winsor_arithmetic')\n\n\n\n\n\nDistributions of indicators at the state level for the winsor arithmetic transformations\n\n\n\n\n\n\n5 Winsor Geometric\n\n\nCode\nget_indicator_distributions(scores, 'winsor_geometric')\n\n\n\n\n\nDistributions of indicators at the state level for the winsor geometric transformations\n\n\n\n\n\n\n6 Min Max Arithmetic\n\n\nCode\n# Load custom function\nsource('dev/get_indicator_distributions.R')\n\n# Plot them\nget_indicator_distributions(scores, 'minmax_arithmetic')\n\n\n\n\n\nDistributions of indicators at the state level for the min-max arithmetic transformations.\n\n\n\n\n\n\n7 Min Max Geometric\n\n\nCode\ndists &lt;- get_indicator_distributions(scores, 'minmax_geometric')\nggsave(\n  'preso/plots/indic_dists.png',\n  plot = dists,\n  width = 10,\n  height = 15,\n  units = 'in'\n)\ndists\n\n\n\n\n\nDistributions of indicators at the state level for the min-max geometric transformations\n\n\n\n\n\n\n8 Z-Score Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'zscore_arithmetic')\n\n\n\n\n\nDistributions of indicators at the state level for the Z-score arithmetic transformations\n\n\n\n\n\n\n9 Z-Score Geometric\n\n\nCode\nget_indicator_distributions(scores, 'zscore_geometric')\n\n\n\n\n\nDistributions of indicators at the state level for the Z-score geometric transformations\n\n\n\n\n\n\n10 Box Cox Arithmetic\n\n\nCode\nget_indicator_distributions(scores, 'boxcox_arithmetic')\n\n\n\n\n\nDistributions of indicators at the state level for the Box Cox arithmetic transformations\n\n\n\n\n\n\n11 Box Cox Geometric\n\n\nCode\nget_indicator_distributions(scores, 'boxcox_geometric')\n\n\n\n\n\nDistributions of indicators at the state level for the Box Cox geometric transformations\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Analysis",
      "Indicator Distributions"
    ]
  },
  {
    "objectID": "pages/metrics_table.html",
    "href": "pages/metrics_table.html",
    "title": "Metrics Table",
    "section": "",
    "text": "On this page you can download a bulk .csv file for all the secondary data metrics collected so far in the project (with the exception of ~1,000 NAICS metrics). The file is ~ 40MB. Use the Download Bulk CSV button below to download it. Note that the metadata table from the last page can be used to identify and define the the variable names. To download a key to match FIPS codes to state and county names, use the Download FIPS Key button.\nSoon to come on this page is an interactive table of metrics. The file size is large enough that manipulating it in the Quarto page is unwieldy, so it will have to link to a separate database. Coming soon.\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools,\n  readr\n)\n\nmetrics_file_name = paste0(Sys.Date(), \"_bulk_metrics.csv\")\nfips_file_name = paste0(Sys.Date(), \"_fips_key.csv\")\n\ntagList(\n  tags$div(\n    style = \"display: flex; gap: 100px; margin-bottom: 20px; justify-content: center;\",\n    tags$a(\n      class = \"btn btn-primary\",\n      style = \"display: flex; width: 200px; justify-content: center; align-items: center;\",\n      href = '../data/bulk_metrics.csv',\n      download = metrics_file_name,\n      tagList(fontawesome::fa(\"download\"), \"Download Bulk .CSV\")\n    ),\n   tags$a(\n      class = \"btn btn-primary\",\n      style = \"display: flex; width: 200px; justify-content: center; align-items: center;\",\n      href = '../data/all_fips_key.csv',\n      download = fips_file_name,\n      tagList(fontawesome::fa(\"download\"), \"Download FIPS Key\")\n    )\n  )\n)\n\n\n\n\n\nDownload Bulk .CSV\n\n\n\nDownload FIPS Key\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Secondary Data",
      "Metrics Data"
    ]
  },
  {
    "objectID": "pages/overview.html",
    "href": "pages/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Dr. Josh Taylor (left) and Dr. David Conner (right) at the FSRC Sustainability Metrics workshop in 2024. Photo by Colleen Goodhue, FSRC.\n\n\nThe original framework of dimensions, indices, and indicators representing food system sustainability was developed through a transdisciplinary team science process described in detail by Wiltshire et al. (2024). The figure below shows the structure of this collaborative process.\nAs the project progressed, a collection of new indicators were proposed across all dimensions, yielding a total of 135 indicators. This full set of indicators is shown in Section 2. Starting in July of 2024, the FSRC has been using a collaborative and transparent process to reduce the number of indicators to manageable amount that can comprehensively represent the food system while being tractable enough to be interpretable and actionable.\nMore information about this refinement process can be found in the Indicator Refinement pages, including results from surveys on indicator and index importance. Subsequent analyses in the Refined Framework Analysis use this reduce set of indicators. The selection of metrics to represent those indicators is tentative. For now, we are using a larger set of metrics than might be otherwise ideal to give us flexibility to explore which best represent the system and what the consequences are given different numbers and configurations of metrics.\n\n\n\nTeam science diagram from Wiltshire et al., 2024.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "pages/overview.html#introduction",
    "href": "pages/overview.html#introduction",
    "title": "Overview",
    "section": "",
    "text": "Dr. Josh Taylor (left) and Dr. David Conner (right) at the FSRC Sustainability Metrics workshop in 2024. Photo by Colleen Goodhue, FSRC.\n\n\nThe original framework of dimensions, indices, and indicators representing food system sustainability was developed through a transdisciplinary team science process described in detail by Wiltshire et al. (2024). The figure below shows the structure of this collaborative process.\nAs the project progressed, a collection of new indicators were proposed across all dimensions, yielding a total of 135 indicators. This full set of indicators is shown in Section 2. Starting in July of 2024, the FSRC has been using a collaborative and transparent process to reduce the number of indicators to manageable amount that can comprehensively represent the food system while being tractable enough to be interpretable and actionable.\nMore information about this refinement process can be found in the Indicator Refinement pages, including results from surveys on indicator and index importance. Subsequent analyses in the Refined Framework Analysis use this reduce set of indicators. The selection of metrics to represent those indicators is tentative. For now, we are using a larger set of metrics than might be otherwise ideal to give us flexibility to explore which best represent the system and what the consequences are given different numbers and configurations of metrics.\n\n\n\nTeam science diagram from Wiltshire et al., 2024.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "pages/overview.html#sec-framework_overview",
    "href": "pages/overview.html#sec-framework_overview",
    "title": "Overview",
    "section": "2 Framework Overview",
    "text": "2 Framework Overview\nBelow is a diagram of all 135 indicators in the framework as of July, 2024. Colors represent dimensions, and splits occur at the dimension and index level. See the table in Section 3 for a more detailed look at indicators.\n\n\nCode\n## Load packages\npacman::p_load(\n  ggraph,\n  igraph,\n  dplyr,\n  RColorBrewer,\n  viridisLite\n)\n\n\n## Load data and add an origin level\ndat &lt;- readRDS('data/trees/tree_dat.rds') %&gt;% \n  mutate(Framework = 'Sustainability') %&gt;% \n  select(Framework, Dimension:Indicator)\n\n\n## Make edges\n# include groupings by dimension, then combine them\nedges &lt;- list()\nedges$sm_dim &lt;- dat %&gt;% \n  select(Framework, Dimension) %&gt;% \n  unique() %&gt;% \n  rename(from = Framework, to = Dimension) %&gt;% \n  mutate(group = to)\nedges$dim_ind &lt;- dat %&gt;% \n  select(Dimension, Index) %&gt;% \n  unique() %&gt;% \n  rename(from = Dimension, to = Index) %&gt;% \n  mutate(group = from)\nedges$ind_ind &lt;- dat %&gt;% \n  select(Index, Indicator) %&gt;% \n  unique() %&gt;% \n  rename(from = Index, to = Indicator) %&gt;% \n  mutate(group = edges$dim_ind$from[match(.$from, edges$dim_ind$to)])\nedges &lt;- bind_rows(edges)\n\n\n## Make vertices\n# Each line is a single vertex (dimension, index, or indicator)\n# We are just giving them random values to control point size for now\nvertices = data.frame(\n  name = unique(c(as.character(edges$from), as.character(edges$to))) , \n  value = runif(nrow(edges) + 1)\n) \n\n# Add the dimension groupings to the vertices as well\nvertices$group = edges$group[match(vertices$name, edges$to)]\n\n# Calculate the angles to arrange indicator labels\nvertices$id = NA\nmyleaves = which(is.na(match(vertices$name, edges$from)))\nnleaves = length(myleaves)\nvertices$id[myleaves] = seq(1:nleaves)\nvertices$angle = 90 - 360 * vertices$id / nleaves\n\n# Calculate alignment of indicator labels\nvertices$hjust &lt;- ifelse(vertices$angle &lt; -90, 1, 0)\n\n# Flip label angles around 180 degrees if they are facing the wrong way\nvertices$angle &lt;- ifelse(vertices$angle &lt; -90, vertices$angle + 180, vertices$angle)\n\n\n## Create graph\n# Make ggraph object from edges and vertices\ngraph &lt;- graph_from_data_frame(edges, vertices = vertices)\n\n# Plot the graph\nggraph(graph, layout = 'dendrogram', circular = TRUE) +\n  \n  # Color edges by dimension\n  geom_edge_diagonal(aes(color = group), width = 0.5) +\n  \n  # Create text for indicators using angles, hjust, and dimension groupings\n  geom_node_text(\n    aes(\n      x = x * 1.04,\n      y = y * 1.04,\n      filter = leaf,\n      label = name,\n      angle = angle,\n      hjust = hjust,\n      colour = group\n    ),\n    size = 2.7,\n    alpha = 1\n  ) +\n  \n  # Make the points for indicators based on dimension groupings\n  # geom_node_point(aes(\n  #   filter = leaf,\n  #   x = x * 1.07,\n  #   y = y * 1.07,\n  #   colour = group,\n  #   size = value,\n  #   alpha = 0.2\n  # )) +\n  \n  # Label the dimensions within the graph\n  geom_node_label(\n    aes(label = ifelse(name == group, name, NA)),\n    label.padding = unit(0.2, \"lines\"),\n    label.r = unit(0.3, \"lines\"),\n    label.size = 0.1,\n    size = 3\n  ) +\n  \n  # Various formatting options\n  scale_colour_manual(values = brewer.pal(5, 'Set1')) +\n  scale_edge_color_manual(values = brewer.pal(5, 'Set1')) +\n  scale_size_continuous(range = c(0.1, 7)) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.margin = unit(c(0, 0, 0, 0), \"cm\")\n  ) +\n  expand_limits(x = c(-2, 2), y = c(-2, 2))\n\n\n\n\n\nRadial dendrogram of Sustainability Metrics framework",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "pages/overview.html#sec-framework_table",
    "href": "pages/overview.html#sec-framework_table",
    "title": "Overview",
    "section": "3 Full Indicator Table",
    "text": "3 Full Indicator Table\nBelow is an interactive table with the full set of 135 indicators from July of 2024. You can search, filter, and page through the table, and download the filtered set of data as a .csv file using the download button.\n\n\nCode\npacman::p_load(\n  dplyr,\n  reactable,\n  stringr,\n  htmltools\n)\n\n# Load framework data as a tree\ntree &lt;- readRDS('data/trees/tree_dat.rds')\n\n# Load custom reactable table function\nsource('dev/get_reactable.R')\n\n# Pick out variables to display\ndat &lt;- tree %&gt;% \n  select(-c(tooltip, count_))\n\n# Make reactable table\nhtmltools::browsable(\n  tagList(\n    tags$div(\n      style = \"display: flex; margin-bottom: 20px; justify-content: center;\",\n      tags$button(\n          class = \"btn btn-primary\",\n          style = \"display: flex; align-items: center; gap: 8px; padding: 8px 12px;\",\n          tagList(fontawesome::fa(\"download\"), \"Download as CSV\"),\n          onclick = \"Reactable.downloadDataCSV('indicator_table', 'indicator_framework.csv')\"\n      )\n    ),\n    get_reactable(\n      dat,\n      elementId = \"indicator_table\",\n      columns = list(\n        Dimension = colDef(minWidth = 75),\n        Index = colDef(minWidth = 100),\n        Indicator = colDef(minWidth = 200)\n      )\n    )\n  )\n)\n\n\n\n\n\nDownload as CSV",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "pages/refinement_process.html",
    "href": "pages/refinement_process.html",
    "title": "Indicator Refinement Process",
    "section": "",
    "text": "To Add:\n\nDescribe dimension meetings\nlink Wiltshire (Wiltshire et al. 2024)\nLink Bene et al 2024 (Béné et al. 2024), describe their process\nWhat is more settled, what is not (social and human)\n\n\n\n\nWhiteboard from economics dimension refinement meeting, November 15th, 2024",
    "crumbs": [
      "Indicator Refinement",
      "Process"
    ]
  },
  {
    "objectID": "pages/refinement_process.html#introduction",
    "href": "pages/refinement_process.html#introduction",
    "title": "Indicator Refinement Process",
    "section": "",
    "text": "To Add:\n\nDescribe dimension meetings\nlink Wiltshire (Wiltshire et al. 2024)\nLink Bene et al 2024 (Béné et al. 2024), describe their process\nWhat is more settled, what is not (social and human)\n\n\n\n\nWhiteboard from economics dimension refinement meeting, November 15th, 2024",
    "crumbs": [
      "Indicator Refinement",
      "Process"
    ]
  },
  {
    "objectID": "pages/refine_environment.html",
    "href": "pages/refine_environment.html",
    "title": "Environment Indicator Refinement",
    "section": "",
    "text": "This page describes the various iterations of indicator sets for the environment dimension. First, we observe the indicators included in the dimension at three points in time. The second section then shows the results of the survey following the indicator refinement meeting. A final set of indicators to incorporate into the next RFP is still in the works!",
    "crumbs": [
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/refine_environment.html#indicator-progression",
    "href": "pages/refine_environment.html#indicator-progression",
    "title": "Environment Indicator Refinement",
    "section": "1 Indicator Progression",
    "text": "1 Indicator Progression\n\n1.1 Wiltshire\nThis graph shows the original framework for the dimension as described in the Wiltshire et al. paper.\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/wiltshire_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.2 Matrix\nHere is the current set of indicators in the matrix, following the Sustainability Metrics workshop in July, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/matrix_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Refinement Meeting\nFinally, the tentative set of indicators following the indicator refinement meeting on November 22nd, 2024\n\n\nCode\n# Use custom function in SMDO repo\nsource('dev/get_dimension_ggraph.R')\nget_dimension_ggraph(\n  csv_path = 'data/trees/env_meeting_tree.csv',\n  dimension_in = 'Environment',\n  y_limits = c(-1.5, 2.1),\n  palette = \"ggthemes::stata_s2color\"\n)",
    "crumbs": [
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/refine_environment.html#survey",
    "href": "pages/refine_environment.html#survey",
    "title": "Environment Indicator Refinement",
    "section": "2 Survey",
    "text": "2 Survey\nThese are the results from the follow-up survey to the economic indicator refinement meeting on November 15th. This feedback will be used to refine the framework for the next RFP.\n\n2.1 Indicators\n\n\nCode\nraw &lt;- read_csv('data/surveys/env_survey.csv')\n\ndat &lt;- raw %&gt;% \n  select(\n    ends_with('GROUP'),\n  ) %&gt;% \n  setNames(c(\n    'indi_must',\n    'indi_probably',\n    'indi_probably_not',\n    'indi_must_not',\n    'idx_must',\n    'idx_probably',\n    'idx_probably_not',\n    'idx_must_not'\n  )) %&gt;% \n  .[-c(1:2), ]\n\nto_df &lt;- function(x) {\n  x %&gt;%\n    str_replace_all('PFAS, PFOS', 'PFAS/PFOS') %&gt;% \n    str_replace_all('soil loss/', 'Soil loss/') %&gt;% \n    str_split(',') %&gt;% \n    unlist() %&gt;% \n    table() %&gt;% \n    as.data.frame() %&gt;% \n    setNames(c('indicator', 'freq')) %&gt;% \n    arrange(desc(freq))\n}\n\nindi_out &lt;- map(dat[1:4], to_df)\nidx_out &lt;- map(dat[5:8], to_df)\n\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nind_tables &lt;- map2(indi_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(ind_tables, ~ {\n  col_name &lt;- str_remove(.y, 'indi_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not * 1e2 + must_not,\n    indicator = fct_reorder(indicator, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(indicator) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\n\n\nCode\nggplot(graph_table, aes(\n  y = reorder(indicator, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Indicator\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_brewer(\n    palette = \"RdBu\", \n    direction = -1,\n    limits = c(\n      \"must\",\n      \"probably\", \n      \"probably_not\", \n      \"must_not\" \n    ),\n    labels = c(\n      \"Must Include\", \n      \"Probably Include\", \n      \"Probably Not Include\", \n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nWe are coding this so “Must Include” is worth 3 points, “Probably Include” is worth 2 points, “Probably Not Include” is worth 1 point, and “Must Not Include” is worth 0 points. Note that the last column is the sum of proportions of “Must Include” and “Probably Include”. You can sort, search, expand, or page through the table below.\n\n\n\n\n\n\n\n\n2.2 Indices\n\n\nCode\n# Add scores by multipliers\nmultipliers &lt;- c(3:0)\nidx_tables &lt;- map2(idx_out, multipliers, ~ {\n  .x %&gt;% \n    mutate(\n      freq = as.numeric(freq),\n      multiplier = .y,\n      score = freq * multiplier,\n    ) %&gt;% \n    select(index = indicator, freq, score)\n})\n\n# Set up DF for color graph \ngraph_table &lt;- imap(idx_tables, ~ {\n  col_name &lt;- str_remove(.y, 'idx_')\n  .x %&gt;% \n    rename(!!sym(col_name) := freq) %&gt;% \n    select(-score)\n}) %&gt;% \n  reduce(full_join) %&gt;% \n  mutate(\n    across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)),\n    sort_key = must * 1e6 + probably * 1e4 + probably_not,\n    sort_key = ifelse(str_detect(index, 'Carbon'), 5e6, sort_key),\n    index = fct_reorder(index, sort_key, .desc = TRUE)\n  ) %&gt;% \n  pivot_longer(\n    cols = must:must_not,\n    names_to = \"category\",\n    values_to = \"count\"\n  ) %&gt;% \n  mutate(\n    category = fct_relevel(\n      category, \n      \"must_not\",\n      \"probably_not\", \n      \"probably\", \n      \"must\"\n    )\n  ) %&gt;%\n  group_by(index) %&gt;%\n  mutate(proportion = count / sum(count)) %&gt;%\n  ungroup()\n\n\ncolors &lt;- RColorBrewer::brewer.pal(4, 'RdBu')\n\nggplot(graph_table, aes(\n  y = reorder(index, sort_key),\n  x = proportion, \n  fill = category\n)) +\n  geom_col(position = \"stack\") +  \n  labs(\n    y = \"Index\",\n    x = \"Proportion\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 20),\n    legend.position = 'top'\n    ) +\n  scale_fill_manual(\n    values = rev(colors),\n    limits = c(\n      \"must\",\n      \"probably\",\n      \"probably_not\",\n      'must_not'\n    ),\n    labels = c(\n      \"Must Include\",\n      \"Probably Include\",\n      \"Probably Not Include\",\n      \"Must Not Include\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nThe indices are going through the same treatment as indicators above - scored from 3 to 0. Note here that the “Carbon ($ GHGs/nutrients)” index seems to be missing a vote. So, it only has 12 points, but the proportion of votes for “Must Include” is 1.",
    "crumbs": [
      "Indicator Refinement",
      "Environment"
    ]
  },
  {
    "objectID": "pages/selection.html",
    "href": "pages/selection.html",
    "title": "Variable Selection and Regression",
    "section": "",
    "text": "On this page we will take our min-max normalized, geometrically averaged scores, which look like the most reliable and approachable so far, and take a deeper dive into variable selection, regression, and PCA. From the dimension meetings, it sounds like we may have some indicators with a couple of metrics, and potentially others with dozens. Because of this, and because of our focus on developing sensible indicators, I think it will be best to do any weighting at the indicator level or above. This also reduces our variable count substantially in relation to our state count of 51, opening more doors for PCA.\nIt is worth emphasizing at the top that the metrics that are making up this secondary data framework are not a great representation of the system. There are some important holes, as well as a heap of metrics that are serving as rather uninspiring proxies. So, extrapolation of these results beyond the confines of the exercise is not recommended. The purpose here is to explore strengths and tradeoffs in methods for aggregating the data. As primary data come in and make up the bulk of the framework and secondary data are used to fill in the gaps, this should start becoming more interpretable.",
    "crumbs": [
      "Analysis",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/selection.html#component-extraction",
    "href": "pages/selection.html#component-extraction",
    "title": "Variable Selection and Regression",
    "section": "2.1 Component Extraction",
    "text": "2.1 Component Extraction\n\n\nCode\n# Filter down to just indicators for PCA\npca_dat &lt;- dat %&gt;% \n  select(starts_with('indic')) %&gt;% \n  setNames(c(str_remove(names(.), 'indic_'))) %&gt;% \n  as.data.frame()\n# get_str(pca_dat)\n\n# Explore how many factors to extract\nVSS(pca_dat, n = 8, fm = 'pc', rotate = 'promax')\n\n\n\n\n\n\n\n\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.53  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.75  with  3  factors\n\nThe Velicer MAP achieves a minimum of 0.04  with  6  factors \n\n\n\nBIC achieves a minimum of  Inf  with    factors\n\n\nSample Size adjusted BIC achieves a minimum of  Inf  with    factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq prob sqresid  fit RMSEA BIC SABIC complex eChisq\n1 0.42 0.00 0.069   0    NA   NA    85.4 0.42    NA  NA    NA      NA     NA\n2 0.53 0.67 0.059   0    NA   NA    48.6 0.67    NA  NA    NA      NA     NA\n3 0.53 0.75 0.055   0    NA   NA    30.7 0.79    NA  NA    NA      NA     NA\n4 0.53 0.75 0.046   0    NA   NA    19.6 0.87    NA  NA    NA      NA     NA\n5 0.47 0.73 0.039   0    NA   NA    13.2 0.91    NA  NA    NA      NA     NA\n6 0.48 0.73 0.039   0    NA   NA    10.5 0.93    NA  NA    NA      NA     NA\n7 0.46 0.72 0.041   0    NA   NA     8.5 0.94    NA  NA    NA      NA     NA\n8 0.47 0.68 0.040   0    NA   NA     6.5 0.96    NA  NA    NA      NA     NA\n  SRMR eCRMS eBIC\n1   NA    NA   NA\n2   NA    NA   NA\n3   NA    NA   NA\n4   NA    NA   NA\n5   NA    NA   NA\n6   NA    NA   NA\n7   NA    NA   NA\n8   NA    NA   NA\n\n\nCode\nset.seed(42)\nfa.parallel(pca_dat, fm = 'ml')\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  5 \n\n\nMAP suggest 5, VSS 3 or 4, PA suggests 4. Not half bad. I think we are justified to go with 5.\n\n\nCode\n# Oblique rotations: promax, oblimin, simplimax, cluster\nrotations &lt;- c(\n  'Promax',\n  'promax',\n  'oblimin',\n  'simplimax',\n  'cluster'\n)\n\npca_outs &lt;- map(rotations, ~ {\n  pca(pca_dat, nfactors = 5, rotate = .x)\n}) %&gt;% \n  setNames(c(rotations))\n\n# Save a version of promax for preso?\npng(\n  filename = 'preso/plots/scree.png',\n  width = 800,\n  height = 600,\n  units = 'px',\n  res = 150\n)\nplot(\n  pca_outs$simplimax$values,\n  xlab = 'Number of Components',\n  ylab = 'Eigen Values'\n)\nabline(h = 1)\ndev.off()\n\n\npng \n  2 \n\n\nCode\n# Now actually show it \nplot(\n  pca_outs$simplimax$values,\n  xlab = 'Number of Components',\n  ylab = 'Eigen Values'\n)\n\n\n\n\n\n\n\n\n\nThe scree plot makes a reasonably convincing case for 5 components, as the slope falls off substantially after the fifth.",
    "crumbs": [
      "Analysis",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/selection.html#run-pca",
    "href": "pages/selection.html#run-pca",
    "title": "Variable Selection and Regression",
    "section": "2.2 Run PCA",
    "text": "2.2 Run PCA\n\n\nCode\npca_tables &lt;- map(pca_outs, ~ {\n  .x$loadings %&gt;% \n    unclass() %&gt;% \n    as.data.frame() %&gt;% \n    select(order(colnames(.))) %&gt;%\n    mutate(\n      across(everything(), ~ round(.x, 3)),\n      across(everything(), ~ case_when(\n        .x &lt; 0.20 ~ '',\n        .x &gt;= 0.20 & .x &lt; 0.32 ~ '.',\n        .x &gt;= 0.32 ~ as.character(.x),\n        .default = NA\n      ))\n    ) %&gt;% \n    rownames_to_column() %&gt;% \n    rename(indicator = 1) %&gt;% \n    mutate(\n      dimension = framework$dimension[match(indicator, framework$indicator)]\n    ) %&gt;% \n    select(indicator, dimension, everything())\n})\nget_str(pca_tables)  \n\n\nList of 5\n $ Promax   :'data.frame':  38 obs. of  7 variables:\n  ..$ indicator: chr [1:38] \"access to land\" \"wealth/income distribution\" \"in\"..\n  ..$ dimension: chr [1:38] \"economics\" \"economics\" \"economics\" \"economics\" \"\"..\n  ..$ RC1      : chr [1:38] \"\" \"\" \"1.013\" \"\" \"\" \"\" \".\" \"0.731\" \"\" \"\" \"\" \"\" \"\"\"..\n  ..$ RC2      : chr [1:38] \"\" \"0.78\" \".\" \".\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"..\n  ..$ RC3      : chr [1:38] \"\" \"\" \"\" \"0.414\" \"\" \"\" \"0.653\" \"\" \"0.825\" \".\" \"\" \"..\n  ..$ RC4      : chr [1:38] \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"0.482\" \"\" \"0.723\" \"0\"..\n  ..$ RC5      : chr [1:38] \".\" \"\" \".\" \"0.357\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"..\n $ promax   :'data.frame':  38 obs. of  7 variables:\n  ..$ indicator: chr [1:38] \"access to land\" \"wealth/income distribution\" \"in\"..\n  ..$ dimension: chr [1:38] \"economics\" \"economics\" \"economics\" \"economics\" \"\"..\n  ..$ RC1      : chr [1:38] \"\" \"\" \"1.013\" \"\" \"\" \"\" \".\" \"0.731\" \"\" \"\" \"\" \"\" \"\"\"..\n  ..$ RC2      : chr [1:38] \"\" \"0.78\" \".\" \".\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"..\n  ..$ RC3      : chr [1:38] \"\" \"\" \"\" \"0.414\" \"\" \"\" \"0.653\" \"\" \"0.825\" \".\" \"\" \"..\n  ..$ RC4      : chr [1:38] \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"0.482\" \"\" \"0.723\" \"0\"..\n  ..$ RC5      : chr [1:38] \".\" \"\" \".\" \"0.357\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"..\n $ oblimin  :'data.frame':  38 obs. of  7 variables:\n  ..$ indicator: chr [1:38] \"access to land\" \"wealth/income distribution\" \"in\"..\n  ..$ dimension: chr [1:38] \"economics\" \"economics\" \"economics\" \"economics\" \"\"..\n  ..$ TC1      : chr [1:38] \"\" \"\" \"0.927\" \"\" \"\" \"\" \".\" \"0.715\" \"\" \"\" \"\" \"\" \"\"\"..\n  ..$ TC2      : chr [1:38] \"\" \"0.757\" \".\" \".\" \"\" \"0.427\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"..\n  ..$ TC3      : chr [1:38] \".\" \"\" \"\" \"0.452\" \".\" \"\" \"0.605\" \"\" \"0.824\" \"\" \".\"..\n  ..$ TC4      : chr [1:38] \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"0.677\" \"\" \"0.805\" \"0\"..\n  ..$ TC5      : chr [1:38] \".\" \"\" \".\" \"0.365\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\"..\n $ simplimax:'data.frame':  38 obs. of  7 variables:\n  ..$ indicator: chr [1:38] \"access to land\" \"wealth/income distribution\" \"in\"..\n  ..$ dimension: chr [1:38] \"economics\" \"economics\" \"economics\" \"economics\" \"\"..\n  ..$ TC1      : chr [1:38] \"0.448\" \"0.43\" \"\" \"0.78\" \"\" \"0.816\" \"\" \"\" \"0.442\"\"..\n  ..$ TC2      : chr [1:38] \"\" \"0.621\" \".\" \"\" \"\" \"\" \".\" \"0.508\" \"0.336\" \"\" \"\"\"..\n  ..$ TC3      : chr [1:38] \"\" \"\" \"\" \".\" \"\" \"\" \"0.539\" \"\" \"0.482\" \"0.464\" \"\" \"..\n  ..$ TC4      : chr [1:38] \"\" \"\" \"\" \"\" \"0.486\" \"\" \".\" \"\" \"0.456\" \".\" \"0.598\"\"..\n  ..$ TC5      : chr [1:38] \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"0.511\" \"\" \"0.589\" \"0\"..\n $ cluster  :'data.frame':  38 obs. of  7 variables:\n  ..$ indicator: chr [1:38] \"access to land\" \"wealth/income distribution\" \"in\"..\n  ..$ dimension: chr [1:38] \"economics\" \"economics\" \"economics\" \"economics\" \"\"..\n  ..$ RC1      : chr [1:38] \".\" \"\" \"\" \"0.44\" \"\" \"0.791\" \"\" \"\" \"\" \"\" \"\" \".\" \".\"..\n  ..$ RC2      : chr [1:38] \"\" \"0.751\" \".\" \".\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\"..\n  ..$ RC3      : chr [1:38] \"\" \"\" \"\" \"0.389\" \"\" \"\" \"0.648\" \"\" \"0.824\" \".\" \"\" \"..\n  ..$ RC4      : chr [1:38] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"0.427\" \"\" \"0.638\" \"0.\"..\n  ..$ RC5      : chr [1:38] \"\" \"\" \"0.33\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\"..\n\n\nCode\n# Save it for preso\nsaveRDS(pca_tables, 'preso/data/pca_tables.rds')\n\n\n\n\nCode\nget_reactable(pca_tables$simplimax)\n\n\n\n\n\n\nAdd interpretation here []",
    "crumbs": [
      "Analysis",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/selection.html#economics",
    "href": "pages/selection.html#economics",
    "title": "Variable Selection and Regression",
    "section": "3.1 Economics",
    "text": "3.1 Economics\n\n3.1.1 Linear Model\nFirst we can try a plain old linear model to see how economics loads onto its indicators.\n\n\nCode\n# Reduce data down to dimen_economics and all indicators\necon_dat &lt;- select(dat, dimen_economics, starts_with('indic')) %&gt;% \n  setNames(c(names(.) %&gt;% str_remove('indic_|dimen_')))\nget_str(econ_dat)\n\n\nrowws_df [50 × 39] (S3: rowwise_df/tbl_df/tbl/data.frame)\n $ economics                            : num [1:50] 0.186 0.299 0.183 0.207 0..\n $ access to land                       : num [1:50] 0.363 0.219 0.198 0.435 0..\n $ wealth/income distribution           : num [1:50] 0.275 0.684 0.373 0.328 0..\n $ income stability                     : num [1:50] 0.08724 0.06626 0.20328 0..\n $ operations diversification           : num [1:50] 0.013147 0.39392 0.05679 ..\n $ use of ag/farm/crop insurance        : num [1:50] 0.2314 0.21962 0 0.29855 ..\n $ carbon fluxes                        : num [1:50] 0.346 0.751 0.879 0.436 0..\n $ carbon stocks                        : num [1:50] 0.1654 0.3674 0.0185 0.32..\n $ embodied carbon                      : num [1:50] 0.05821 0.00615 0.06465 0..\n $ forest health                        : num [1:50] 0.25 0.4275 0.1041 0.3586..\n $ biodiversity                         : num [1:50] 0.572 0.298 0.574 0.489 0..\n $ land use diversity                   : num [1:50] 0.954307 0.587913 0.29319..\n $ sensitive or rare habitats           : num [1:50] 0.642 0.7319 0.6918 0.419..\n $ water quality                        : num [1:50] 0.581 0.585 0.449 0.454 0..\n $ water quantity                       : num [1:50] 0.712 0.915 0.42 0.791 0...\n $ educational attainment               : num [1:50] 0.2323 0.4223 0.4479 0.15..\n $ access to culturally appropriate food: num [1:50] 0.1563 0.3824 0.0818 0.07..\n $ dietary quality                      : num [1:50] 0.36 0.352 0.352 0.768 0...\n $ food access                          : num [1:50] 0.1684 0.2286 0.1061 0.12..\n $ food affordability                   : num [1:50] 0.498 0.526 0.35 0.172 0...\n $ mental health tbd                    : num [1:50] 0.352 0.34 0.312 0.339 0...\n $ access to care                       : num [1:50] 0.198 0.435 0.313 0.247 0..\n $ housing supply and quality           : num [1:50] 0.621 0.616 0.633 0.536 0..\n $ physical health tbd                  : num [1:50] 0.274 0.585 0.507 0.377 0..\n $ total quantity exported              : num [1:50] 0.068031 0 0.068105 0.174..\n $ production species diversity         : num [1:50] 0.5496 0.2917 0.6537 0.37..\n $ production inputs                    : num [1:50] 0.8 1 0.619 0.305 0.533 0..\n $ total quantity food products         : num [1:50] 0.107721 0.000613 0.07753..\n $ total quantity forest products       : num [1:50] 0.635061 0.000087 0.00048..\n $ total quantity non-food ag products  : num [1:50] 0.19143 0.00143 0.02043 0..\n $ value added market                   : num [1:50] 0.01481 0.30841 0.04466 0..\n $ crop failure                         : num [1:50] 0.95 1 0.967 0.391 0.636 ..\n $ social connectedness                 : num [1:50] 0.417 0.406 0.314 0.388 0..\n $ community safety                     : num [1:50] 0.749 0.384 0.896 0.677 0..\n $ diverse representation               : num [1:50] 0.45 0.53 0.638 0.34 0.41..\n $ age diversity                        : num [1:50] 0.3774 0.7547 0.1132 0.58..\n $ gender diversity                     : num [1:50] 0.274 0.9229 1 0.4825 0.4..\n $ racial diversity                     : num [1:50] 0.28454 0.34292 0.82326 0..\n $ participatory governance             : num [1:50] 0.3192 0.514 0.4967 0.011..\n\n\nCode\nlm &lt;- lm(economics ~ ., data = econ_dat)\nsummary(lm)\n\n\n\nCall:\nlm(formula = economics ~ ., data = econ_dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.028597 -0.008780 -0.001355  0.009435  0.033763 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             -0.129261   0.177775  -0.727  0.48235\n`access to land`                         0.407912   0.159224   2.562  0.02643\n`wealth/income distribution`             0.264800   0.100376   2.638  0.02307\n`income stability`                       0.007812   0.108346   0.072  0.94381\n`operations diversification`            -0.080939   0.071452  -1.133  0.28140\n`use of ag/farm/crop insurance`          0.104228   0.027921   3.733  0.00331\n`carbon fluxes`                          0.024741   0.071354   0.347  0.73533\n`carbon stocks`                          0.031166   0.117489   0.265  0.79571\n`embodied carbon`                        0.090569   0.125130   0.724  0.48430\n`forest health`                          0.007066   0.162326   0.044  0.96606\nbiodiversity                             0.019697   0.181185   0.109  0.91539\n`land use diversity`                     0.030419   0.042042   0.724  0.48445\n`sensitive or rare habitats`            -0.060387   0.089124  -0.678  0.51205\n`water quality`                          0.040110   0.076031   0.528  0.60829\n`water quantity`                        -0.006883   0.076736  -0.090  0.93014\n`educational attainment`                 0.053691   0.096731   0.555  0.58997\n`access to culturally appropriate food` -0.071258   0.123987  -0.575  0.57704\n`dietary quality`                       -0.029778   0.067903  -0.439  0.66949\n`food access`                            0.082575   0.128307   0.644  0.53304\n`food affordability`                    -0.085322   0.095028  -0.898  0.38849\n`mental health tbd`                     -0.216905   0.162632  -1.334  0.20926\n`access to care`                         0.059387   0.180315   0.329  0.74808\n`housing supply and quality`             0.087502   0.125383   0.698  0.49974\n`physical health tbd`                   -0.161052   0.165392  -0.974  0.35111\n`total quantity exported`                0.343059   0.163633   2.097  0.05996\n`production species diversity`           0.063850   0.066954   0.954  0.36075\n`production inputs`                      0.055077   0.053327   1.033  0.32386\n`total quantity food products`          -0.315811   0.207882  -1.519  0.15692\n`total quantity forest products`         0.039438   0.057882   0.681  0.50974\n`total quantity non-food ag products`   -0.005035   0.081416  -0.062  0.95180\n`value added market`                     0.319651   0.115996   2.756  0.01870\n`crop failure`                          -0.011586   0.090693  -0.128  0.90065\n`social connectedness`                  -0.049821   0.083092  -0.600  0.56093\n`community safety`                      -0.030003   0.039398  -0.762  0.46235\n`diverse representation`                 0.011552   0.040475   0.285  0.78063\n`age diversity`                          0.035801   0.059580   0.601  0.56009\n`gender diversity`                       0.061339   0.092878   0.660  0.52257\n`racial diversity`                       0.073204   0.099326   0.737  0.47655\n`participatory governance`               0.064864   0.066379   0.977  0.34948\n                                          \n(Intercept)                               \n`access to land`                        * \n`wealth/income distribution`            * \n`income stability`                        \n`operations diversification`              \n`use of ag/farm/crop insurance`         **\n`carbon fluxes`                           \n`carbon stocks`                           \n`embodied carbon`                         \n`forest health`                           \nbiodiversity                              \n`land use diversity`                      \n`sensitive or rare habitats`              \n`water quality`                           \n`water quantity`                          \n`educational attainment`                  \n`access to culturally appropriate food`   \n`dietary quality`                         \n`food access`                             \n`food affordability`                      \n`mental health tbd`                       \n`access to care`                          \n`housing supply and quality`              \n`physical health tbd`                     \n`total quantity exported`               . \n`production species diversity`            \n`production inputs`                       \n`total quantity food products`            \n`total quantity forest products`          \n`total quantity non-food ag products`     \n`value added market`                    * \n`crop failure`                            \n`social connectedness`                    \n`community safety`                        \n`diverse representation`                  \n`age diversity`                           \n`gender diversity`                        \n`racial diversity`                        \n`participatory governance`                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02723 on 11 degrees of freedom\nMultiple R-squared:  0.9703,    Adjusted R-squared:  0.8677 \nF-statistic: 9.461 on 38 and 11 DF,  p-value: 0.0001723\n\n\nWe can see that most of the economics indicators (access to land, wealth and income distribution, income stability) are significant predictors, while operations diversification is close. But some surprises are access to culturally appropriate food (school food authorities serving culturally relevant food), housing supply and quality, as well as a few production indicators, like richness (crop diversity), production inputs, and value-added markets. Social connectedness from the social dimension also makes it on the list. The largest coefficients by a wide margin are for access to land and wealth and income distribution.\n\n\n3.1.2 Splitting Data\nHere we split out data into a 60/40 training/test set for cross validation with GLMnet and Random Forest models. Note that we are pushing the limits of our sample size. But this should help protect against overfitting.\n\n\nCode\npacman::p_load(\n  caret,\n  ranger,\n  glmnet\n)\n\n# Split data 60/40\nset.seed(42)\nindices &lt;- createDataPartition(econ_dat$economics, p = 0.60, list = FALSE)\ntraining_data &lt;- econ_dat[indices, ]\ntesting_data &lt;- econ_dat[-indices,]\n\nmy_folds &lt;- createFolds(training_data$economics, k = 5, list = TRUE)\n\n# Control\nmy_control &lt;- trainControl(\n  method = 'cv',\n  number = 5,\n  verboseIter = TRUE,\n  index = my_folds\n)\n\n# Check for zero variance or near zero variance indicators\nnearZeroVar(dat, names = TRUE, saveMetrics = TRUE)\n# All clear\n\n\n\n\n3.1.3 GLMnet\nHere we use a GLMnet to find an optimal balance between a ridge regression, which penalizes variables based on the magnitude of coefficients, and lasso regression, which adds a penalty based on the absolute value of coefficients. We use a tuning grid to find optimal values of alpha (0 = ridge, 1 = lasso) and lambda (the penalty parameter). Both this and the random forest model are particularly good at prediction, but also provide a metric for variable importance that can help us interpret our indicators.\n\n\nCode\nset.seed(42)\necon_glmnet &lt;- train(\n  economics ~ .,\n  data = training_data, \n  tuneGrid = expand.grid(\n    alpha = seq(0.1, 1, length = 5),\n    lambda = seq(0.0001, 0.1, length = 100)\n  ),\n  method = \"glmnet\",\n  trControl = my_control,\n  preProcess = c('zv', 'center', 'scale')\n)\n\n\n+ Fold1: alpha=0.100, lambda=0.1 \n- Fold1: alpha=0.100, lambda=0.1 \n+ Fold1: alpha=0.325, lambda=0.1 \n- Fold1: alpha=0.325, lambda=0.1 \n+ Fold1: alpha=0.550, lambda=0.1 \n- Fold1: alpha=0.550, lambda=0.1 \n+ Fold1: alpha=0.775, lambda=0.1 \n- Fold1: alpha=0.775, lambda=0.1 \n+ Fold1: alpha=1.000, lambda=0.1 \n- Fold1: alpha=1.000, lambda=0.1 \n+ Fold2: alpha=0.100, lambda=0.1 \n- Fold2: alpha=0.100, lambda=0.1 \n+ Fold2: alpha=0.325, lambda=0.1 \n- Fold2: alpha=0.325, lambda=0.1 \n+ Fold2: alpha=0.550, lambda=0.1 \n- Fold2: alpha=0.550, lambda=0.1 \n+ Fold2: alpha=0.775, lambda=0.1 \n- Fold2: alpha=0.775, lambda=0.1 \n+ Fold2: alpha=1.000, lambda=0.1 \n- Fold2: alpha=1.000, lambda=0.1 \n+ Fold3: alpha=0.100, lambda=0.1 \n- Fold3: alpha=0.100, lambda=0.1 \n+ Fold3: alpha=0.325, lambda=0.1 \n- Fold3: alpha=0.325, lambda=0.1 \n+ Fold3: alpha=0.550, lambda=0.1 \n- Fold3: alpha=0.550, lambda=0.1 \n+ Fold3: alpha=0.775, lambda=0.1 \n- Fold3: alpha=0.775, lambda=0.1 \n+ Fold3: alpha=1.000, lambda=0.1 \n- Fold3: alpha=1.000, lambda=0.1 \n+ Fold4: alpha=0.100, lambda=0.1 \n- Fold4: alpha=0.100, lambda=0.1 \n+ Fold4: alpha=0.325, lambda=0.1 \n- Fold4: alpha=0.325, lambda=0.1 \n+ Fold4: alpha=0.550, lambda=0.1 \n- Fold4: alpha=0.550, lambda=0.1 \n+ Fold4: alpha=0.775, lambda=0.1 \n- Fold4: alpha=0.775, lambda=0.1 \n+ Fold4: alpha=1.000, lambda=0.1 \n- Fold4: alpha=1.000, lambda=0.1 \n+ Fold5: alpha=0.100, lambda=0.1 \n- Fold5: alpha=0.100, lambda=0.1 \n+ Fold5: alpha=0.325, lambda=0.1 \n- Fold5: alpha=0.325, lambda=0.1 \n+ Fold5: alpha=0.550, lambda=0.1 \n- Fold5: alpha=0.550, lambda=0.1 \n+ Fold5: alpha=0.775, lambda=0.1 \n- Fold5: alpha=0.775, lambda=0.1 \n+ Fold5: alpha=1.000, lambda=0.1 \n- Fold5: alpha=1.000, lambda=0.1 \n\n\nAggregating results\nSelecting tuning parameters\nFitting alpha = 1, lambda = 0.0758 on full training set\n\n\nCode\nimportance &lt;- varImp(econ_glmnet, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nCode\n# Predict\n# p &lt;- predict(econ_glmnet, testing_data)\n# postResample(pred = p, obs = testing_data$economics)\n\n\nThe optimal hyperparameters from the tuning grid were alpha = 0.1 (mostly ridge regression) and lambda = 0.00313. The variable importance plot is on a relative scale of 0 (unimportant) to 100 (most important) in terms of predictive power. Curiously, it is showing that the value added market indicator from the production dimension is a better predictor of economics than any economics indicator.\n\n\n3.1.4 Random Forest\nNow we can try a random forest, which is particularly good at handling non-linear relationships. Here we use the RMSE to determine the optimal combination of mtry (the number of variables selected at each node in the decision tree), the split rule, and the minimum node size.\n\n\nCode\nset.seed(42)\necon_rf &lt;- train(\n  economics ~ .,\n  data = training_data, \n  tuneLength = 7,\n  method = \"ranger\",\n  trControl = my_control,\n  importance = 'impurity'\n)\n\n\n+ Fold1: mtry= 2, min.node.size=5, splitrule=variance \n- Fold1: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold1: mtry= 8, min.node.size=5, splitrule=variance \n- Fold1: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold1: mtry=14, min.node.size=5, splitrule=variance \n- Fold1: mtry=14, min.node.size=5, splitrule=variance \n+ Fold1: mtry=20, min.node.size=5, splitrule=variance \n- Fold1: mtry=20, min.node.size=5, splitrule=variance \n+ Fold1: mtry=26, min.node.size=5, splitrule=variance \n- Fold1: mtry=26, min.node.size=5, splitrule=variance \n+ Fold1: mtry=32, min.node.size=5, splitrule=variance \n- Fold1: mtry=32, min.node.size=5, splitrule=variance \n+ Fold1: mtry=38, min.node.size=5, splitrule=variance \n- Fold1: mtry=38, min.node.size=5, splitrule=variance \n+ Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold1: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold1: mtry=38, min.node.size=5, splitrule=extratrees \n- Fold1: mtry=38, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry= 2, min.node.size=5, splitrule=variance \n- Fold2: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold2: mtry= 8, min.node.size=5, splitrule=variance \n- Fold2: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold2: mtry=14, min.node.size=5, splitrule=variance \n- Fold2: mtry=14, min.node.size=5, splitrule=variance \n+ Fold2: mtry=20, min.node.size=5, splitrule=variance \n- Fold2: mtry=20, min.node.size=5, splitrule=variance \n+ Fold2: mtry=26, min.node.size=5, splitrule=variance \n- Fold2: mtry=26, min.node.size=5, splitrule=variance \n+ Fold2: mtry=32, min.node.size=5, splitrule=variance \n- Fold2: mtry=32, min.node.size=5, splitrule=variance \n+ Fold2: mtry=38, min.node.size=5, splitrule=variance \n- Fold2: mtry=38, min.node.size=5, splitrule=variance \n+ Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold2: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold2: mtry=38, min.node.size=5, splitrule=extratrees \n- Fold2: mtry=38, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry= 2, min.node.size=5, splitrule=variance \n- Fold3: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold3: mtry= 8, min.node.size=5, splitrule=variance \n- Fold3: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold3: mtry=14, min.node.size=5, splitrule=variance \n- Fold3: mtry=14, min.node.size=5, splitrule=variance \n+ Fold3: mtry=20, min.node.size=5, splitrule=variance \n- Fold3: mtry=20, min.node.size=5, splitrule=variance \n+ Fold3: mtry=26, min.node.size=5, splitrule=variance \n- Fold3: mtry=26, min.node.size=5, splitrule=variance \n+ Fold3: mtry=32, min.node.size=5, splitrule=variance \n- Fold3: mtry=32, min.node.size=5, splitrule=variance \n+ Fold3: mtry=38, min.node.size=5, splitrule=variance \n- Fold3: mtry=38, min.node.size=5, splitrule=variance \n+ Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold3: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold3: mtry=38, min.node.size=5, splitrule=extratrees \n- Fold3: mtry=38, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry= 2, min.node.size=5, splitrule=variance \n- Fold4: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold4: mtry= 8, min.node.size=5, splitrule=variance \n- Fold4: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold4: mtry=14, min.node.size=5, splitrule=variance \n- Fold4: mtry=14, min.node.size=5, splitrule=variance \n+ Fold4: mtry=20, min.node.size=5, splitrule=variance \n- Fold4: mtry=20, min.node.size=5, splitrule=variance \n+ Fold4: mtry=26, min.node.size=5, splitrule=variance \n- Fold4: mtry=26, min.node.size=5, splitrule=variance \n+ Fold4: mtry=32, min.node.size=5, splitrule=variance \n- Fold4: mtry=32, min.node.size=5, splitrule=variance \n+ Fold4: mtry=38, min.node.size=5, splitrule=variance \n- Fold4: mtry=38, min.node.size=5, splitrule=variance \n+ Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold4: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold4: mtry=38, min.node.size=5, splitrule=extratrees \n- Fold4: mtry=38, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry= 2, min.node.size=5, splitrule=variance \n- Fold5: mtry= 2, min.node.size=5, splitrule=variance \n+ Fold5: mtry= 8, min.node.size=5, splitrule=variance \n- Fold5: mtry= 8, min.node.size=5, splitrule=variance \n+ Fold5: mtry=14, min.node.size=5, splitrule=variance \n- Fold5: mtry=14, min.node.size=5, splitrule=variance \n+ Fold5: mtry=20, min.node.size=5, splitrule=variance \n- Fold5: mtry=20, min.node.size=5, splitrule=variance \n+ Fold5: mtry=26, min.node.size=5, splitrule=variance \n- Fold5: mtry=26, min.node.size=5, splitrule=variance \n+ Fold5: mtry=32, min.node.size=5, splitrule=variance \n- Fold5: mtry=32, min.node.size=5, splitrule=variance \n+ Fold5: mtry=38, min.node.size=5, splitrule=variance \n- Fold5: mtry=38, min.node.size=5, splitrule=variance \n+ Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n- Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry= 8, min.node.size=5, splitrule=extratrees \n- Fold5: mtry= 8, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=14, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=14, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=20, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=20, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=26, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=26, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=32, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=32, min.node.size=5, splitrule=extratrees \n+ Fold5: mtry=38, min.node.size=5, splitrule=extratrees \n- Fold5: mtry=38, min.node.size=5, splitrule=extratrees \n\n\nAggregating results\nSelecting tuning parameters\nFitting mtry = 2, splitrule = extratrees, min.node.size = 5 on full training set\n\n\nCode\n# econ_rf\n# plot(econ_rf)\n\nimportance &lt;- varImp(econ_rf, scale = TRUE)\nplot(importance)\n\n\n\n\n\n\n\n\n\nCode\n# Predict\n# p &lt;- predict(model_mf, testing_data)\n# postResample(pred = p, obs = testing_data$rebl_tpm)\n\n\nThe random forest model is also picking out the value-added market indicator as the best predictor of economics dimension scores, followed closely by operations diversification, wealth and income distribution, and income stability.\nVery curious how value-added markets keep sticking out. The two metrics making up this indicator are both from NASS: the percentage of farms reporting value-added sales, and of those farms, the percentage of value-added sales out of total sales.",
    "crumbs": [
      "Analysis",
      "Selection and Regression"
    ]
  },
  {
    "objectID": "pages/validation.html",
    "href": "pages/validation.html",
    "title": "Validation",
    "section": "",
    "text": "The goal here is to use our five tentative dimension scores as predictors to compare against other established metrics:\n\nFood security index, overall and/or child (Feeding America, Map the Meal Gap)\nHealth outcomes (UW county health rankings)\nLife expectancy, or premature age-adjusted mortality (UW rankings)\nOther ideas: a food affordability index, happiness index, happy planet index?\n\nTo Add:\n\ncite Schneider 2023 (Schneider et al. 2023)\n\nWLS regression to get deviations of region and income group weighted means from global weighted mean\n\n\n\n\nCode\n# Load sm_data\nsm_data &lt;- readRDS('data/sm_data.rds')\n\n# Load state fips key to join other datasets\nstate_key &lt;- sm_data[['state_key']] %&gt;% \n  select(state, state_code)\n\n# Load cleaned aggregated data for all levels of regresion\nminmax_geo &lt;- readRDS('data/minmax_geo_all_levels.rds')\nget_str(minmax_geo)\n\n# Reduce to just dimension scores, and remove prefix\ndimension_scores &lt;- minmax_geo %&gt;% \n  select(state, starts_with('dimen')) %&gt;% \n  setNames(c(str_remove(names(.), 'dimen_')))\nget_str(dimension_scores)\n\n# Pull raw metrics data\nmetrics_df &lt;- readRDS('data/metrics_df.rds')\nmetrics_df &lt;- readRDS('data/valued_rescaled_metrics.rds')\nget_str(metrics_df)\n\n\n# Pull validation variables out of sm_data, wrangle them to match metrics_df\n# Also including covariates, gdp and population\nvalidation_vars &lt;- sm_data$metadata %&gt;% \n  select(variable_name, metric, definition, source) %&gt;% \n  filter(variable_name %in% c(\n    'foodInsecurity',\n    'communityEnvRank',\n    'happinessScore',\n    'wellbeingRank',\n    'workEnvRank',\n    'foodEnvironmentIndex',\n    'lifeExpectancy',\n    'population',\n    'gdpCurrent'\n  )) %&gt;% \n  pull(variable_name)\nvalidation_vars  \n \n# Get subset of metrics for our validation variables, get latest year only\nvalidation_metrics &lt;- sm_data$metrics %&gt;% \n  filter(\n    variable_name %in% validation_vars, \n    !is.na(value), \n    str_length(fips) == 2\n  ) %&gt;% \n  get_latest_year()\nget_str(validation_metrics)\n# All are available in 2024\n\n# Pivot wider, also get rid of trailing year\nvalidation_metrics &lt;- validation_metrics %&gt;% \n  pivot_wider(\n    id_cols = fips,\n    names_from = variable_name,\n    values_from = value\n  ) %&gt;% \n  setNames(c(str_remove(names(.), '_[0-9]{4}'))) %&gt;% \n  mutate(across(!fips, as.numeric))\nget_str(validation_metrics)\n# 00 US is missing a lot obviously\n# 11 DC is the other one with missing data\n# We will just filter down to 50 states to match metrics_df\n\n# Combine validation variables with our dimension scores using state key as the \n# bridge. Also remove DC (don't have validation metrics there)\nkey &lt;- sm_data$state_key %&gt;% \n  select(state, fips = state_code)\ndat &lt;- dimension_scores %&gt;% \n  left_join(key) %&gt;% \n  left_join(validation_metrics) %&gt;% \n  as.data.frame() %&gt;% \n  filter(state != 'DC') %&gt;% \n  select(-fips)\n\n# Make a GDP per capita variable from GDP real and population\n# It was already in millions to begin with\ndat &lt;- dat %&gt;% \n  mutate(gdp_per_cap = ((gdpCurrent / population) * 1e6) / 1000)\nget_str(dat)\n\n# Check it out\nget_str(dat)\nskimr::skim(dat)\n# Looks good\n\n# Save this for other pages\nsaveRDS(dat, 'data/metrics_df_with_vals_and_covars.rds')",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/validation.html#glmnet",
    "href": "pages/validation.html#glmnet",
    "title": "Validation",
    "section": "4.1 GLMnet",
    "text": "4.1 GLMnet\n\n\nCode\nset.seed(42)\nfood_env_glmnet &lt;- train(\n  foodEnvironmentIndex ~ economics + environment + health + production + social + gdp_per_cap,\n  data = training_data, \n  tuneGrid = expand.grid(\n    alpha = seq(0.1, 1, length = 5),\n    lambda = seq(0.0001, 0.1, length = 100)\n  ),\n  method = \"glmnet\",\n  trControl = my_control,\n  preProcess = c('zv', 'center', 'scale')\n)\n\n\n\n\nCode\nimportance &lt;- varImp(food_env_glmnet, scale = TRUE)\nsaveRDS(importance, 'preso/plots/val3_food_env_glmnet_importance.rds')\nplot(importance)\n\n\n\n\n\n\n\n\n\nIt looks like GDP is a better predictor of the Food Environment Index than anything else, but the health dimension is far more influential than any other.",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/validation.html#random-forest",
    "href": "pages/validation.html#random-forest",
    "title": "Validation",
    "section": "4.2 Random Forest",
    "text": "4.2 Random Forest\n\n\nCode\nset.seed(42)\nfood_env_rf &lt;- train(\n  foodEnvironmentIndex ~ production + social + health + economics + environment + gdp_per_cap,\n  data = training_data, \n  tuneLength = 7,\n  method = \"ranger\",\n  trControl = my_control,\n  importance = 'impurity'\n)\n\n\nOOB prediction error (MSE): 1.2861053\n\n\nCode\nimportance &lt;- varImp(food_env_rf, scale = TRUE)\nsaveRDS(importance, 'preso/plots/val3_rf_importance.rds')\nplot(importance)",
    "crumbs": [
      "Analysis",
      "Validation"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#economics",
    "href": "pages/sensitivity.html#economics",
    "title": "Sensitivity and Uncertainty",
    "section": "2.1 Economics",
    "text": "2.1 Economics",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#environment",
    "href": "pages/sensitivity.html#environment",
    "title": "Sensitivity and Uncertainty",
    "section": "2.2 Environment",
    "text": "2.2 Environment",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#health",
    "href": "pages/sensitivity.html#health",
    "title": "Sensitivity and Uncertainty",
    "section": "2.3 Health",
    "text": "2.3 Health",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#production",
    "href": "pages/sensitivity.html#production",
    "title": "Sensitivity and Uncertainty",
    "section": "2.4 Production",
    "text": "2.4 Production",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "pages/sensitivity.html#social",
    "href": "pages/sensitivity.html#social",
    "title": "Sensitivity and Uncertainty",
    "section": "2.5 Social",
    "text": "2.5 Social",
    "crumbs": [
      "Analysis",
      "Sensitivity and Uncertainty"
    ]
  },
  {
    "objectID": "preso/preso.html#getting-up",
    "href": "preso/preso.html#getting-up",
    "title": "Sustainability Metrics Secondary Data Something Something",
    "section": "Getting up",
    "text": "Getting up\n\nTurn off alarm\nGet out of bed"
  },
  {
    "objectID": "preso/preso.html#going-to-sleep",
    "href": "preso/preso.html#going-to-sleep",
    "title": "Sustainability Metrics Secondary Data Something Something",
    "section": "Going to sleep",
    "text": "Going to sleep\n\nGet in bed\nCount sheep\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "preso/preso.html#testing-a-plot",
    "href": "preso/preso.html#testing-a-plot",
    "title": "Sustainability Metrics Secondary Data Something Something",
    "section": "Testing a plot",
    "text": "Testing a plot\n\n\n\nSome points before plot\nanother point before the plot"
  },
  {
    "objectID": "preso/preso.html#testing-plotly",
    "href": "preso/preso.html#testing-plotly",
    "title": "Sustainability Metrics Secondary Data Something Something",
    "section": "Testing Plotly",
    "text": "Testing Plotly\n\n\n\nSome points before plot\nanother point before the plot"
  },
  {
    "objectID": "preso/preso.html#test-loading-plot",
    "href": "preso/preso.html#test-loading-plot",
    "title": "Sustainability Metrics Secondary Data Something Something",
    "section": "Test Loading Plot",
    "text": "Test Loading Plot\n\n\n\nSome points before plot\nanother point before the plot"
  },
  {
    "objectID": "preso/preso.html#test-tabset-for-graphs",
    "href": "preso/preso.html#test-tabset-for-graphs",
    "title": "Sustainability Metrics Secondary Data Something Something",
    "section": "Test Tabset for Graphs",
    "text": "Test Tabset for Graphs\n\nFirst TabSecond Tab"
  },
  {
    "objectID": "preso/preso.html#test-plotly",
    "href": "preso/preso.html#test-plotly",
    "title": "Sustainability Metrics Secondary Data Something Something",
    "section": "Test Plotly",
    "text": "Test Plotly\n\n\n\nSome points before plot\nanother point before the plot\n\n\n\nEcononomicsEnvironment"
  },
  {
    "objectID": "preso/preso.html#sensitivity-by-dimension",
    "href": "preso/preso.html#sensitivity-by-dimension",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Sensitivity by Dimension",
    "text": "Sensitivity by Dimension\n\n\n\nSample from uncertain inputs (OECD 2008)\n\nRescaling methods (5)\nAggregation methods (2)\nLeave out one indicator (39)\n= 390 iterations\n\nHigher ranks are desirable\nSome dimensions are quite unstable (Economics)\nSome are quite stable (Health)\n\n\n\nEconomicsEnvironmentHealthProductionSocial\n\n\nDistribution for Vermont\n\n\n\n\n\n\n\n\n\n\n\nDistribution for Vermont\n\n\n\n\n\n\n\n\n\n\n\nDistribution for Vermont\n\n\n\n\n\n\n\n\n\n\n\nDistribution for Vermont\n\n\n\n\n\n\n\n\n\n\n\nDistribution for Vermont"
  },
  {
    "objectID": "preso/preso.html#indicator-influence",
    "href": "preso/preso.html#indicator-influence",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Indicator Influence",
    "text": "Indicator Influence\n\n\n\nSome points before plot\nanother point before the plot\nsome other important point\n\n\n\nEconomicsEnvironmentHealthProductionSocial"
  },
  {
    "objectID": "preso/preso.html#introduction",
    "href": "preso/preso.html#introduction",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nSecondary data goals:\n\nIdentify existing data and gaps\nExplore methods of aggregating data\n\nGoals for today:\n\nShare methods\nShare preliminary findings\nFeedback and discussion\n\nGuiding topics:\n\nHow well do the data represesent the system?\nNormalization, aggregation, and values\nWhere and how do we incorporate qualitative data?\nInforming the next RFP\n\n\n\n\n\n\nIntervale Farm, Sally McCay, UVM Photo\n\n\n\n\nThe point is to take stock of existing data, use primary research"
  },
  {
    "objectID": "preso/preso.html#indicator-influence-plotlyy",
    "href": "preso/preso.html#indicator-influence-plotlyy",
    "title": "Sustainability Metrics Secondary Data Something Something",
    "section": "Indicator Influence (Plotlyy",
    "text": "Indicator Influence (Plotlyy\n\n\n\nSome points before plot\nanother point before the plot\nsome other important point\n\n\n\nEcononomicsEnvironmentHealthProductionSocial"
  },
  {
    "objectID": "preso/preso.html#indicator-influence-plotly",
    "href": "preso/preso.html#indicator-influence-plotly",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Indicator Influence (Plotly)",
    "text": "Indicator Influence (Plotly)\n\n\n\nSome points before plot\nanother point before the plot\nsome other important point\n\n\n\nEconomicsEnvironmentHealthProductionSocial"
  },
  {
    "objectID": "preso/preso.html#secondary-data",
    "href": "preso/preso.html#secondary-data",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Secondary Data",
    "text": "Secondary Data\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV\n\n\n\n\n\n\n\n\n~650 metrics, plus another thousand NAICS variables\n\nSources\n\nUSDA NASS\nUSDA ERS Farm Income and Wealth Statistics\nUSDA ERS Food Environment Atlas\nUSDA Farm Service Agency Distaster Assistance\nUSDA Food and Nutrition Service\nUSDA Food Safety and Inspection Service\nUniversity of Wisconsin\nUS Census\nEPA State GHG Data\nTreeMap 2016\nNatureServe - biodiversity data"
  },
  {
    "objectID": "preso/preso.html#framework",
    "href": "preso/preso.html#framework",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Framework",
    "text": "Framework\n\nEconomicsEnvironmentHealthProductionSocial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNONE is a placeholder for when I don’t have a metric there\nAll ears for more datasets if you can fill in some NONEs"
  },
  {
    "objectID": "preso/preso.html#metric-distributions",
    "href": "preso/preso.html#metric-distributions",
    "title": "Sustainability Metrics Secondary Data Something Something",
    "section": "Metric Distributions",
    "text": "Metric Distributions\n\nLong figure with metric distributions, scrollable"
  },
  {
    "objectID": "preso/preso.html#indicator-distributions",
    "href": "preso/preso.html#indicator-distributions",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Indicator Distributions",
    "text": "Indicator Distributions\nAn example of indicator distributions with the Min Max + geometric means methods"
  },
  {
    "objectID": "preso/preso.html#indicator-correlations",
    "href": "preso/preso.html#indicator-correlations",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Indicator Correlations",
    "text": "Indicator Correlations\n\nCorrelation MatrixInfluential Indicators\n\n\nMin Max geometric aggregation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighly correlating indicators from different dimensions are a problem (double counting)"
  },
  {
    "objectID": "preso/preso.html#framework-2",
    "href": "preso/preso.html#framework-2",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Framework 2",
    "text": "Framework 2\n\nTable with metadata, desired direction\nTouch on distance from targets? This is a hinky point here"
  },
  {
    "objectID": "preso/preso.html#normalization-and-aggregation",
    "href": "preso/preso.html#normalization-and-aggregation",
    "title": "Sustainability Metrics Secondary Data Something Something",
    "section": "Normalization and Aggregation",
    "text": "Normalization and Aggregation\n\n\n\ncolumns\non the right, formulas and citations for each thing\non left, notes\ntest a citation (Schneider et al. 2023)\n\n\nMin Max (OECD 2008)\\[\\begin{equation}\nI^t_qc = \\frac{x^t_qc - min_c(x^{t_0}_q)}{max_c(x^{t_0}_q)-min_c(x^{t_0}_q)}\n\\end{equation}\\]Where \\(x^t_qc\\) is the metric \\(q\\) for state \\(c\\) at time \\(t\\).Z-Scores (OECD 2008)\\[\\begin{equation}\nI^t_{qc} = \\frac{x^t_{qc}-x^t_{qc=\\overline{c}}}{\\sigma^t_{qc=\\overline{c}}}\n\\end{equation}\\]Box Cox (Bickel and Doksum 1981)\\[\\begin{equation}\n{\\rm For}\\ \\lambda\\neq0,\\ f\\lambda(x) = (sign(x)|x|^\\lambda-1)/\\lambda\n\\end{equation}\\]\\[\\begin{equation}\n{\\rm For}\\ \\lambda = 0,\\ f_0(x) = log(x)\n\\end{equation}\\]Rank OrderWinsorization"
  },
  {
    "objectID": "preso/preso.html#comparisons",
    "href": "preso/preso.html#comparisons",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Comparisons",
    "text": "Comparisons\n\nRank OrderWinsorizationMin MaxZ-ScoreBox Cox"
  },
  {
    "objectID": "preso/preso.html#validation",
    "href": "preso/preso.html#validation",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Validation",
    "text": "Validation\n\nFood InsecurityLife ExpectancyFood Environment IndexHappiness Score\n\n\nOverall food insecurity from Feeding America, Map the Meal Gap\n\n\n\n\n\n\nOverall life expectancy from UW Population Health Institute (2024)\n\n\n\n\n\n\nGLMnet variable importance:\n\n\n\n\n\n\n\n\n\nRandom forest variable importance:"
  },
  {
    "objectID": "preso/preso.html#references",
    "href": "preso/preso.html#references",
    "title": "Sustainability Metrics Secondary Data",
    "section": "References",
    "text": "References\n\n\n\n\nBickel, Peter J., and Kjell A. Doksum. 1981. “An Analysis of Transformations Revisited.” Journal of the American Statistical Association 76 (374): 296–311. https://doi.org/10.1080/01621459.1981.10477649.\n\n\nEsty, Daniel. 2008. “Pilot 2006 Environmental Performance Index.” Yale Center for Environmental Law & Policy, January, 45-2621-45-2621. https://doi.org/10.5860/CHOICE.45-2621.\n\n\nGómez-Limón, José A., and Gabriela Sanchez-Fernandez. 2010. “Empirical Evaluation of Agricultural Sustainability Using Composite Indicators.” Ecological Economics 69 (5): 1062–75. https://doi.org/10.1016/j.ecolecon.2009.11.027.\n\n\nJacobi, Johanna, Stellah Mukhovi, Aymara Llanque, Markus Giger, Adriana Bessa, Christophe Golay, Chinwe Ifejika Speranza, et al. 2020. “A New Understanding and Evaluation of Food Sustainability in Six Different Food Systems in Kenya and Bolivia.” Scientific Reports 10 (1): 19145. https://doi.org/10.1038/s41598-020-76284-y.\n\n\nMayer, Audrey L. 2008. “Strengths and Weaknesses of Common Sustainability Indices for Multidimensional Systems.” Environment International 34 (2): 277–91. https://doi.org/10.1016/j.envint.2007.09.004.\n\n\nOECD. 2008. Handbook on Constructing Composite Indicators: Methodology and User Guide. Paris: Organisation for Economic Co-operation and Development.\n\n\nSchneider, Kate R., Jessica Fanzo, Lawrence Haddad, Mario Herrero, Jose Rosero Moncayo, Anna Herforth, Roseline Remans, et al. 2023. “The State of Food Systems Worldwide in the Countdown to 2030.” Nature Food 4 (12): 1090–110. https://doi.org/10.1038/s43016-023-00885-9.\n\n\nSchneider, Kate R., Roseline Remans, Tesfaye Hailu Bekele, Destan Aytekin, Piero Conforti, Shouro Dasgupta, Fabrice DeClerck, et al. 2025. “Governance and Resilience as Entry Points for Transforming Food Systems in the Countdown to 2030.” Nature Food 6 (1): 105–16. https://doi.org/10.1038/s43016-024-01109-4."
  },
  {
    "objectID": "preso/test.html#slide-with-column-and-tabset",
    "href": "preso/test.html#slide-with-column-and-tabset",
    "title": "Quarto Reveal.js with Panel Tabset in a Column",
    "section": "Slide with Column and Tabset",
    "text": "Slide with Column and Tabset\n\n\nLeft Column\nSome content here…\n\nRight Column with Tabs"
  },
  {
    "objectID": "preso/preso.html#validation-2",
    "href": "preso/preso.html#validation-2",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Validation 2",
    "text": "Validation 2"
  },
  {
    "objectID": "preso/preso.html#normalization",
    "href": "preso/preso.html#normalization",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Normalization",
    "text": "Normalization\n\nRank OrderWinsorMin MaxZ-ScoresBox CoxDistance\n\n\nRank Order\n\nMakes no distributional assumptions (Schneider et al. 2025)\nUseful for comparing other transformation methods\nLoss of information\n\n\n\nWinsorization\n\nReduce all outliers to a percentile (95th and 5th)\nDoesn’t reward overperformance in one area\nUsed in the Environmental Performance Indicator (EPI) before scaling from 0 to 100 (Esty 2008)\nMore robust than leaving keeping outliers (Mayer 2008)\n\n\n\nMin Max (OECD 2008)\n\nScales all data from 0 to 1\nIntuitive (0 is worst, 1 is best)\nLinear transformation\n\n\\[\\begin{equation}\nI^t_qc = \\frac{x^t_qc - min_c(x^{t_0}_q)}{max_c(x^{t_0}_q)-min_c(x^{t_0}_q)}\n\\end{equation}\\]\nWhere \\(x^t_qc\\) is the metric \\(q\\) for state \\(c\\) at time \\(t\\).\n\nSchneider et al. (2023)\n\n\n\nZ-Scores (OECD 2008)\n\nScales data to mean of 0 and standard deviation of 1\nLarger numbers are better, but no limits\nLinear transformation\n\n\\[\\begin{equation}\nI^t_{qc} = \\frac{x^t_{qc}-x^t_{qc=\\overline{c}}}{\\sigma^t_{qc=\\overline{c}}}\n\\end{equation}\\]\n\n\nBox Cox (Bickel and Doksum 1981)\n\nFinds an optimal value of \\(\\lambda\\) to make distribution normal\nMore tractable distribution for analysis\nHarder to interpret\nNon-linear transformation\n\n\\[\\begin{equation}\n{\\rm For}\\ \\lambda\\neq0,\\ f\\lambda(x) = (sign(x)|x|^\\lambda-1)/\\lambda\n\\end{equation}\\] \\[\\begin{equation}\n{\\rm For}\\ \\lambda = 0,\\ f_0(x) = log(x)\n\\end{equation}\\]\n\n\nDistance to Target (OECD 2008)\n\nRatio of the indicator to a reference system or reference value\nUsed with official benchmarks like minimum wage (Jacobi et al. 2020)\nDifficult to set targets in many applications\n\n\\[\\begin{equation}\nI^t_qc = \\frac{x^t_qc}{x^t_{qc=\\overline{c}}}\n\\end{equation}\\]"
  },
  {
    "objectID": "preso/preso.html#secondary-data-all",
    "href": "preso/preso.html#secondary-data-all",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Secondary Data (All)",
    "text": "Secondary Data (All)\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV"
  },
  {
    "objectID": "preso/preso.html#refined-secondary-data",
    "href": "preso/preso.html#refined-secondary-data",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Refined Secondary Data?",
    "text": "Refined Secondary Data?\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV"
  },
  {
    "objectID": "preso/preso.html#secondary-data---refined",
    "href": "preso/preso.html#secondary-data---refined",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Secondary Data - Refined",
    "text": "Secondary Data - Refined\nA set of 130 metrics to match the refined framework\n\n\n\n\n\nShow/hide more columns\n\n\n\nDownload as CSV\n\n\n\n\n\n\n\n\n~650 metrics, plus another thousand NAICS variables\n\nSources\n\nUSDA NASS\nUSDA ERS Farm Income and Wealth Statistics\nUSDA ERS Food Environment Atlas\nUSDA Farm Service Agency Distaster Assistance\nUSDA Food and Nutrition Service\nUSDA Food Safety and Inspection Service\nUniversity of Wisconsin\nUS Census\nEPA State GHG Data\nTreeMap 2016\nNatureServe - biodiversity data"
  },
  {
    "objectID": "pages/pca.html",
    "href": "pages/pca.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "preso/preso.html#validation-regression",
    "href": "preso/preso.html#validation-regression",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Validation: Regression",
    "text": "Validation: Regression\n\nFood InsecurityLife ExpectancyFood Environment Index\n\n\nFood Insecurity Index (UW Population Health Institute 2024)\n\n\n\n\n\n\nLife Expectancy (UW Population Health Institute 2024)\n\n\n\n\n\n\nFood Environment Index (UW Population Health Institute 2024)\nGLMnet variable importance:\n\n\n\n\n\n\n\n\n\nRandom forest variable importance:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFood Environment Index: 0 (worst) 10 (best). Distance to grocery, cost of health diet.\nHappiness (WalletHub):\n\nemotional and physical wellbeing (health index, depression, alcohol use disorder, adequate sleep rate…)\nWork environment (work hours, commute time, income, unemployment…)\nCommunity and environment (ideal weather, leisure time, safety, volunteer rate)"
  },
  {
    "objectID": "preso/preso.html#validation-pca",
    "href": "preso/preso.html#validation-pca",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Validation: PCA",
    "text": "Validation: PCA\n\n\n\nFactor extraction:\n\nParallel Analysis (PA) suggests 5 components\nVelicer MAP suggests 6\nVSS suggests 2 or 3\n\nKey to loadings:\n\nx &lt; 0.2 ~ ’’\nx &lt; 0.32 ~ ‘.’\nx &gt;= 0.32 ~ x\n\n\n\n\nScree PlotSimplimaxPromaxObliminCluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMight be better off with 6 components - sign indicators are not cohesive\nPA: randomize rows, do PCA. Keep PCs that explain significantly more variance than expected by chance\nMAP: get average squared partial correlations for each PC. Keep PCs that lead to lowest average squared partial correlation\nVSS: compare fit of simplified model to original correlations. VSS = 1-sumsquares(r*)/sumsquares(r)Peaks at optimal (most interpretable) number of factors.\nPromax is best interpretation, but not great"
  },
  {
    "objectID": "preso/preso.html#maps",
    "href": "preso/preso.html#maps",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Maps",
    "text": "Maps"
  },
  {
    "objectID": "preso/preso.html#dimension-score-map",
    "href": "preso/preso.html#dimension-score-map",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Dimension Score Map",
    "text": "Dimension Score Map\nUsing Min Max normalization and geometric aggregation\n\nEconomicsEnvironmentHealthProductionSocial"
  },
  {
    "objectID": "preso/preso.html#desirable-directions",
    "href": "preso/preso.html#desirable-directions",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Desirable Directions",
    "text": "Desirable Directions\n\n\n\nOfficial benchmarks where possible, otherwise largest value is 100% (Jacobi et al. 2020)\nEvery indicator gets a good direction and bad direction (Schneider et al. 2023)"
  },
  {
    "objectID": "preso/preso.html#conclusions",
    "href": "preso/preso.html#conclusions",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "preso/preso.html#to-add",
    "href": "preso/preso.html#to-add",
    "title": "Sustainability Metrics Secondary Data",
    "section": "To Add",
    "text": "To Add\n\nMonte Carlo to explore metric numbers\nScenario Analysis with random forest for predictions"
  },
  {
    "objectID": "preso/preso.html#conclusions-and-discussion",
    "href": "preso/preso.html#conclusions-and-discussion",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Conclusions and Discussion",
    "text": "Conclusions and Discussion\n\nConclusions\n\nGaps\nStability and Sensitivity\nFramework complexity\nInfluential indicators\n\nDiscussion\n\nHow well do these data represent the system?\nCan we reasonably assign desirable directions to metrics and indicators?\nWhat are fair and interpretable methods of transformations and aggregation?\n\n\n\n\nGaps:\n\nNASS stats lean toward commodity crops\nSoil health\nFood distribution capacity (have some of this)\nFood loss and waste\nEmbodied Carbon\nCarbon stocks\nforest complexity\nwater quality\nWater quantity\nprecarity, market health indices\ndiversity of Farm Types (this one tricky for VT)\nnutrition\nall of social dimension\n\nSensitivity:\n\nEconomics, environment need to be refined"
  },
  {
    "objectID": "preso/preso.html#aggregation",
    "href": "preso/preso.html#aggregation",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Aggregation",
    "text": "Aggregation\n\n\n\nArithmetic (Jacobi et al. 2020)\nGeometric:\n\n\\[\\begin{equation}\n\\sqrt[n]{x_1 * x_2 * ... * x_n}\n\\end{equation}\\]\n\nCompensatory and non compensatory\nSome did both and compared, came out similar (Gómez-Limón and Sanchez-Fernandez 2010)\n\n\n\nimages here?"
  },
  {
    "objectID": "preso/preso.html#rescaling",
    "href": "preso/preso.html#rescaling",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Rescaling",
    "text": "Rescaling\n\nRank OrderWinsorMin MaxZ-ScoresBox CoxDistance*\n\n\nRank Order\n\nMakes no distributional assumptions (Schneider et al. 2025)\nUseful for comparing other transformation methods\nLoss of information\n\n\n\nWinsorization\n\nReduce all outliers to a percentile (95th and 5th)\nDoesn’t reward overperformance in one area\nUsed in the Environmental Performance Indicator (EPI) before scaling from 0 to 100 (Esty 2008)\nMore robust than leaving keeping outliers (Mayer 2008)\n\n\n\nMin Max (OECD 2008)\n\\[\\begin{equation}\nI^t_qc = \\frac{x^t_qc - min_c(x^{t_0}_q)}{max_c(x^{t_0}_q)-min_c(x^{t_0}_q)}\n\\end{equation}\\]\n\nWhere \\(x^t_qc\\) is the metric \\(q\\) for state \\(c\\) at time \\(t\\).\n\n\nScales all data from 0 to 1\nIntuitive (0 is worst, 1 is best)\nLinear transformation\nSchneider et al. (2023)\n\n\n\nZ-Scores (OECD 2008)\n\\[\\begin{equation}\nI^t_{qc} = \\frac{x^t_{qc}-x^t_{qc=\\overline{c}}}{\\sigma^t_{qc=\\overline{c}}}\n\\end{equation}\\]\n\nScales data to mean of 0 and standard deviation of 1\nLarger numbers are better, but no limits\nLinear transformation\n\n\n\nBox Cox (Bickel and Doksum 1981)\n\\[\\begin{equation}\n{\\rm For}\\ \\lambda\\neq0,\\ f\\lambda(x) = (sign(x)|x|^\\lambda-1)/\\lambda\n\\end{equation}\\] \\[\\begin{equation}\n{\\rm For}\\ \\lambda = 0,\\ f_0(x) = log(x)\n\\end{equation}\\]\n\nFinds an optimal value of \\(\\lambda\\) to make distribution normal\nMore tractable distribution for analysis\nHarder to interpret\nNon-linear transformation\n\n\n\nDistance to Target (OECD 2008)\n\nRatio of the indicator to a reference system or reference value\nUsed with official benchmarks like minimum wage (Jacobi et al. 2020)\nDifficult to set targets in many applications\n\n\\[\\begin{equation}\nI^t_qc = \\frac{x^t_qc}{x^t_{qc=\\overline{c}}}\n\\end{equation}\\]"
  },
  {
    "objectID": "preso/preso.html#dimension-score-maps",
    "href": "preso/preso.html#dimension-score-maps",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Dimension Score Maps",
    "text": "Dimension Score Maps\nUsing Min Max normalization and geometric aggregation\n\nEconomicsEnvironmentHealthProductionSocial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: cannot compare across dimensions\nEnv:\n\nScaling, everything looks good compared to NV\nBias toward biodiverse areas with denser population?\n\nProd: California skews everything\nSocial: Arkansas skews everything"
  },
  {
    "objectID": "preso/preso.html",
    "href": "preso/preso.html",
    "title": "Sustainability Metrics Secondary Data",
    "section": "",
    "text": "Secondary data goals:\n\nIdentify existing data and gaps\nExplore methods of aggregating data\n\nGoals for today:\n\nShare methods\nShare preliminary findings\nFeedback and discussion\n\nGuiding topics:\n\nHow well do the data represesent the system?\nNormalization, aggregation, and values\nWhere and how do we incorporate qualitative data?\nInforming the next RFP\n\n\n\n\n\n\nIntervale Farm, Sally McCay, UVM Photo\n\n\n\n\n\nThe point is to take stock of existing data, use primary research"
  },
  {
    "objectID": "preso/preso.html#uncertainty",
    "href": "preso/preso.html#uncertainty",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Uncertainty",
    "text": "Uncertainty\n\n\n\nnotes here\nis this even interesting?"
  },
  {
    "objectID": "preso/preso.html#metric-uncertainty",
    "href": "preso/preso.html#metric-uncertainty",
    "title": "Sustainability Metrics Secondary Data",
    "section": "Metric Uncertainty",
    "text": "Metric Uncertainty\n\n\n\nnotes here\nis this even interesting?\n\n\n\nDimension ScoresStandard Deviations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScores stay about stable\nSD only goes down for health and economics - both have an indicator with many metrics\nNot sure this is anything"
  }
]