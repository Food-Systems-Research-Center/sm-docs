---
title: 'Economics'
fig-responsive: false
---

```{r}
#| label: setup
#| include: false
pacman::p_load(
 conflicted
)

conflicts_prefer(
 dplyr::select(),
 dplyr::filter(),
 .quiet = TRUE
)
```

## Dimension Overview

Shown in the diagram below are a total of 45 indicators within the economics dimension. Indices are labeled within the diagram. 17 indicators are both included in the Wiltshire et al. framework as well as being studied by one or more teams (red), 9 are included in the Wiltshire et al. but not currently belong studied (green), while 19 were not in the original framework, but have been added by one or more teams (blue).

The points beside each indicator name represent the number of secondary data metrics that have been aggregated for each indicator. Sources include USDA NASS, BLS, ERS, Census Bureau, and others. The quality and appropriateness of these metrics vary widely - I do not mean to suggest that having more of them means an indicator is more accurately better represented. For more information on the data sources, head to the Tables page to see metadata.

One other point to note here is that I removed several dozen metrics from BLS wage labor data broken down by NAICS industry code so as not to inflate that indicator relative to the others.

```{r fig.dim=c(10, 8), fig.align='center'}
#| label: radial_plot
#| fig-cap: Radial dendrogram of Sustainability Metrics framework
#| code-fold: true
#| warning: false

## Load packages
pacman::p_load(
  ggraph,
  igraph,
  dplyr,
  RColorBrewer,
  viridisLite,
  ggrepel,
  stringr
)

conflicted::conflicts_prefer(
  dplyr::as_data_frame(),
  .quiet = TRUE
)

## Load data for tree and metrics
dat <- readRDS('data/trees/econ_tree.rds') %>% 
  select(Dimension:Source)
metadata_all <- readRDS('data/sm_data.rds')[['metadata']]
meta <- metadata_all %>% 
  filter(
    dimension == 'economics'
  )

# Rename metadata so it fits into formatting of tree data
# This is quite not ideal - Note to harmonize this properly later
meta <- meta %>% 
  mutate(
    indicator = str_to_sentence(indicator),
    indicator = case_when(
      str_detect(indicator, '^Assets') ~ 'Balance sheet (assets and liabilities)',
      str_detect(indicator, '^Business failure') ~ 'Business failure rate of food business',
      str_detect(indicator, '^Direct') ~ '% direct-to-consumer sales',
      str_detect(indicator, '^Job avail') ~ 'Availability of good-paying jobs in food systems',
      str_detect(indicator, '^Local sales') ~ '% local sales',
      str_detect(indicator, '^Operator salary') ~ 'Operator salary / wage',
      str_detect(indicator, '^Total sales') ~ 'Total sales / revenue',
      str_detect(indicator, '^Wealth/income') ~ 'Wealth / income distribution',
      TRUE ~ indicator
    )
  ) 

# Join counts of secondary data metrics to original dataset
# Remove the NAICS variables - there are so many of them, don't add much
counts <- meta %>% 
  filter(str_detect(variable_name, '^lq|lvl|Lvl|Naics', negate = TRUE)) %>% 
  group_by(indicator) %>% 
  dplyr::summarize(count = n())


## Make edges
# include groupings by dimension, then combine them
edges <- list()
edges$dim_ind <- dat %>% 
  select(Dimension, Index) %>% 
  unique() %>% 
  dplyr::rename(from = Dimension, to = Index) %>% 
  mutate(group = to)
edges$ind_ind <- dat %>% 
  select(Index, Indicator) %>% 
  unique() %>% 
  dplyr::rename(from = Index, to = Indicator) %>% 
  mutate(group = from)
edges <- bind_rows(edges)

# Add column for use (will use in colors of text?)
edges$group <- c(rep(NA, 10), dat$Source)


## Make vertices
# Each line is a single vertex (dimension, index, or indicator)
# We are just giving them random values to control point size for now
vertices = data.frame(
  name = unique(c(as.character(edges$from), as.character(edges$to)))
) %>% 
  left_join(counts, by = join_by(name == indicator)) %>% 
  dplyr::rename('value' = count)

# Add the dimension groupings to the vertices as well
vertices$group = edges$group[match(vertices$name, edges$to)]

# Calculate the angles to arrange indicator labels
vertices$id = NA
myleaves = which(is.na(match(vertices$name, edges$from)))
nleaves = length(myleaves)
vertices$id[myleaves] = seq(1:nleaves)
vertices$angle = 90 - 360 * vertices$id / nleaves

# Calculate alignment of indicator labels
vertices$hjust <- ifelse(vertices$angle < -90, 1, 0)

# Flip label angles around 180 degrees if they are facing the wrong way
vertices$angle <- ifelse(vertices$angle < -90, vertices$angle + 180, vertices$angle)


## Create graph
# Make ggraph object from edges and vertices
graph <- graph_from_data_frame(edges, vertices = vertices)

# Plot the graph
ggraph(graph, layout = 'dendrogram', circular = TRUE) +
  
  # Color edges by dimension
  geom_edge_diagonal(color = 'black', width = 0.5) +
  
  # Create text for indicators using angles, hjust, and dimension groupings
  geom_node_text(
    aes(
      x = x * 1.15,
      y = y * 1.15,
      filter = leaf,
      label = name,
      angle = angle,
      hjust = hjust,
      colour = group
    ),
    size = 3,
    alpha = 1
  ) +
  
  # Label indices within graph
  geom_label_repel(
    aes(
      x = x,
      y = y,
      label = ifelse(name %in% unique(dat$Index), name, NA)
    ),
    label.padding = unit(0.15, "lines"),
    label.r = unit(0.3, "lines"),
    label.size = 0.05,
    size = 2.25,
    force = 0.1,    
    force_pull = 1, 
    max.overlaps = 10 
  ) +
  
  # Make the points for indicators based on secondary metric count
  geom_node_point(
    aes(
      filter = leaf,
      x = x * 1.07,
      y = y * 1.07,
      colour = group,
      size = value
    ),
    alpha = 0.4
  ) +
  
  # Various formatting options
  scale_colour_manual(values = brewer.pal(3, 'Set1')) +
  # scale_size_continuous(range = c(0.1, 7)) +
  theme_void() +
  theme(
    plot.margin = unit(c(0, 0, 0, 0), "cm")
  ) +
  scale_colour_manual(
    name = "Indicator Use",
    values = brewer.pal(3, 'Set1'),
    labels = c("Both", "Current Only", "Wiltshire Only")
  ) +
  expand_limits(x = c(-2.5, 2.5), y = c(-2.5, 2.5))
```

## Distributions

We are taking out the abundant but largely redundant BLS NAICS wage data variables to leave us with a more approachable set of 46 variables to explore here. First just show univariate distributions by county.


```{r}
#| label: prep
#| warning: false
pacman::p_load(
  dplyr,
  purrr,
  ggplot2,
  rlang,
  ggpubr,
  tidyr
)

source('dev/data_pipeline_functions.R')
source('dev/filter_fips.R')
metrics <- readRDS('data/sm_data.rds')[['metrics']]
metadata <- readRDS('data/sm_data.rds')[['metadata']]

# Use metadata to get help filter by dimension
econ_meta <- metadata %>% 
  filter(dimension == 'economics')

# Filter to economics dimension
econ_metrics <- metrics %>% 
  filter(variable_name %in% econ_meta$variable_name)

# Filter to latest year and new (post-2024) counties
# Also remove NAICS variables to leave us with an approachable number
# And pivot wider so it is easier to get correlations
econ_metrics_latest <- econ_metrics %>%
  filter_fips(scope = 'new') %>% 
  get_latest_year() %>% 
  filter(
    str_detect(
      variable_name, 
      'Naics|NAICS|^lq|^avgEmpLvl|expHiredLaborPercOpExp', 
      negate = TRUE
    )
  )

# Pivot wider for easier correlations below
econ_metrics_latest <- econ_metrics_latest %>% 
  select(fips, variable_name, value) %>% 
  unique() %>% 
  mutate(variable_name = str_split_i(variable_name, '_', 1)) %>% 
  pivot_wider(
    names_from = 'variable_name',
    values_from = 'value'
  ) %>% 
  unnest(!fips) %>% 
  mutate(across(c(civLaborForce:last_col()), as.numeric))
```

```{r}
#| label: distribution_plots
#| fig-cap: Distributions of economic metrics at the county level.
#| fig-height: 25
#| fig-width: 10
#| fig-align: center
#| warning: false
pacman::p_load(
  dplyr,
  purrr,
  ggplot2,
  rlang,
  ggpubr,
  tidyr
)

plots <- map(names(econ_metrics_latest)[-1], \(var){
  if (is.character(econ_metrics_latest[[var]])) {
    econ_metrics_latest %>% 
      ggplot(aes(x = !!sym(var))) + 
      geom_bar(
        fill = 'lightblue',
        color = 'royalblue',
        alpha = 0.5
      ) +
      theme_classic() +
      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))
  } else if (is.numeric(econ_metrics_latest[[var]])) {
    econ_metrics_latest %>% 
      ggplot(aes(x = !!sym(var))) + 
      geom_density(
        fill = 'lightblue',
        color = 'royalblue',
        alpha = 0.5
      ) +
      theme_classic() +
      theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))
  } else {
    return(NULL)
  }
}) 

# Arrange them in 4 columns
ggarrange(
  plotlist = plots,
  ncol = 4,
  nrow = 12
)
```

## Correlation Heatmap

Throwing those same variables into a correlation matrix. Hover to see variable names, Pearson correlation, and p-values.

```{r}
#| label: correlation_plot
#| fig-cap: Interactive Correlation Plot
#| warning: false
pacman::p_load(
  dplyr,
  tidyr,
  tibble,
  stringr,
  purrr,
  tidyr,
  ggplot2,
  plotly,
  reshape,
  Hmisc,
  viridisLite
)

# Arrange variables in some halfway reasonable order
cor_dat <- econ_metrics_latest %>% 
  select(
    matches('Code_|metro'),
    matches('employ|abor|Worker'),
    matches('Sales'),
    matches('Earn|Income'),
    everything(),
    -fips,
    -matches('expHiredLaborPercOpExp') # This one didn't come through
  )

# Make a correlation matrix using all the selected variables
cor <- cor_dat %>% 
  as.matrix() %>% 
  rcorr()

# Melt correlation values and rename columns
cor_r <- melt(cor$r) %>% 
  setNames(c('var_1', 'var_2', 'value'))

# Save p values
cor_p <- melt(cor$P)
p.value <- cor_p$value

# Make heatmap with custom text aesthetic for tooltip
plot <- cor_r %>% 
  ggplot(aes(var_1, var_2, fill = value, text = paste0(
    'Var 1: ', var_1, '\n',
    'Var 2: ', var_2, '\n',
    'Correlation: ', format(round(value, 3), nsmall = 3), '\n',
    'P-Value: ', format(round(p.value, 3), nsmall = 3)
  ))) + 
  geom_tile() + 
  scale_fill_viridis_c() + 
  theme(axis.text.x = element_text(hjust = 1, angle = 45)) +
  labs(
    x = NULL,
    y = NULL,
    fill = 'Correlation'
  )

# Convert to interactive plotly figure with text tooltip
ggplotly(
  plot, 
  tooltip = 'text',
  width = 1000,
  height = 800
)
```


## PCA

PCA is a popular tool in this area for exploring unique variation with many collinear variables. It is a way to reduce the dimensionality of the data into fewer, more interpretable principal components.

It also requires complete data, which we do not have. So we either have to run a probabililistic PCA or run imputations. I'm using a random forest algorithm to impute data here as a first pass [@stekhovenMissForestNonparametricMissing2012a]. This really warrants a deeper dive into the type and severity of missingness though, and PPCA is likely the better option in the end.  

```{r}
#| label: imputation
#| warning: false
pacman::p_load(
  missForest
)

# Wrangle dataset. Need all numeric vars or factor vars. And can't be tibble
# Also removing character vars - can't use these in PCA
dat <- econ_metrics_latest %>%
  select(where(is.numeric)) %>%
  as.data.frame()
# get_str(dat)

# Check missing variables
# skimr::skim(dat)

# Impute missing variables
set.seed(42)
mf_out <- dat %>%
  missForest(
    ntree = 200,
    mtry = 10,
    verbose = FALSE,
    variablewise = FALSE
  )

# Save imputed dataset
imp <- mf_out$ximp

# Print OOB
mf_out$OOBerror

```

Out of bag error is shown as normalized root mean square error. Now we can explore how many composite factors is appropriate for the data.

```{r}
#| label: vss
#| warning: false
pacman::p_load(
  psych
)
VSS(imp)
fa.parallel(imp)
```

VSS gives a wide range from 2 to 8, MAP shows 7, parallel analysis shows 4. I tend to trust PA the most, so let's go with 4.

```{r}
#| label: pca
#| warning: false
(pca_out <- pca(imp, nfactors = 4))

plot(pca_out$values)
abline(h = 1)
```

From the scree plot and eigenvalues it looks like the first three components bear lots of unique variance, but after that there is no clear elbow where a qualitative decision can be made to choose a certain number of components. The Kaiser-Guttman rule suggests keeping any compents with an eigenvalue > 1 (at the horizontal line), but we can see here that this is a rather dubious distinction. 

If we look at the output from the PCA call, we can see how closely each variable (row) correlates with each component (columns 1-4). The variables most associated with Component #1 are the farm labor variables - numbers of workers, labor expenses, etc. They also tend to be raw figures, and probably have more to do with population than anything else. 
Component #2 is made up mostly of generic employment figures - total civilian labor force, total employed, total unemployed. These are not specific to food systems.
Component #3 has a curious collection of median earnings variables and 'per farm' variables like acres per farm, income per farm, and local and direct-to-consumer sales. 
Component #4 does not represent much unique variance, and loooks like a grab bag of variables. 

A couple of early takeaways here are that the raw figures that are tied to population probably shouldn't be mixed with other variables like proportions. We could try normalizing all the variables so that raw variables are not disproportionately weighted. But it might make more sense to avoid raw counts and dollar amounts entirely.

## References

