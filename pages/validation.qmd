---
title: "Validation"
format:
  html:
    fig-dpi: 200
editor_options: 
  chunk_output_type: console
warnings: false
---

```{r}
#| label: prep
#| echo: false
source('dev/data_pipeline_functions.R')
source('dev/get_res_plots.R')
source('dev/get_setup.R')
get_setup()
```

# Introduction

The goal here is to use our five tentative dimension scores as predictors to compare against other established metrics:

- Food security index, overall and/or child (Feeding America, Map the Meal Gap)
- Health outcomes (UW county health rankings)
- Life expectancy, or premature age-adjusted mortality (UW rankings)
- Other ideas: a food affordability index, happiness index, happy planet index?

To Add:

- cite Schneider 2023 [@schneider2023StateFoodSystems]
  - WLS regression to get deviations of region and income group weighted means from global weighted mean
  
  
```{r}
#| label: wrangle
#| output: false
pacman::p_load(
  dplyr,
  purrr,
  stringr,
  tidyr
)

# Load sm_data
sm_data <- readRDS('data/sm_data.rds')

# Load state fips key to join other datasets
state_key <- sm_data[['state_key']] %>% 
  select(state, state_code)

# Load cleaned aggregated data for all levels of regresion
minmax_geo <- readRDS('data/minmax_geo_all_levels.rds')
get_str(minmax_geo)

# Reduce to just dimension scores, and remove prefix
dimension_scores <- minmax_geo %>% 
  select(state, starts_with('dimen')) %>% 
  setNames(c(str_remove(names(.), 'dimen_')))
get_str(dimension_scores)

# Pull raw metrics data
metrics_df <- readRDS('data/metrics_df.rds')
get_str(metrics_df)


# Pull validation variables out of sm_data, wrangle them to match metrics_df
validation_vars <- sm_data$metadata %>% 
  select(variable_name, metric, definition, source) %>% 
  filter(variable_name %in% c(
    'foodInsecurity',
    'communityEnvRank',
    'happinessScore',
    'wellbeingRank',
    'workEnvRank',
    'foodEnvironmentIndex',
    'lifeExpectancy'
  )) %>% 
  pull(variable_name)
validation_vars  
 
# Get subset of metrics for our validation variables, get latest year only
validation_metrics <- sm_data$metrics %>% 
  filter(
    variable_name %in% validation_vars, 
    !is.na(value), 
    str_length(fips) == 2
  ) %>% 
  get_latest_year()
get_str(validation_metrics)
# All are available in 2024

# Pivot wider, also get rid of trailing year
validation_metrics <- validation_metrics %>% 
  pivot_wider(
    id_cols = fips,
    names_from = variable_name,
    values_from = value
  ) %>% 
  setNames(c(str_remove(names(.), '_2024'))) %>% 
  mutate(across(!fips, as.numeric))
get_str(validation_metrics)
# 00 US is missing obviously
# 11 DC is the other one
# We will just filter down to 50 states then

# Combine validation variables with our dimension scores using state key as the 
# bridge. Also remove DC (don't have validation metrics there)
key <- sm_data$state_key %>% 
  select(state, fips = state_code)
dat <- dimension_scores %>% 
  left_join(key) %>% 
  left_join(validation_metrics) %>% 
  as.data.frame() %>% 
  filter(state != 'DC') %>% 
  select(-fips)

# Check it out
get_str(dat)
skimr::skim(dat)
# Looks good
```

# Food Insecurity

```{r}
#| label: food_insecurity
#| fig-width: 8
#| fig-height: 8
#| fig-align: center
#| out-width: 75%
pacman::p_load(
  performance,
  AER,
  sandwich
)
lm1 <- lm(
  foodInsecurity ~ economics + environment + health + production + social,
  data = dat
)
summary(lm1)
check_model(lm1)
```

Not the worst residuals plots I've seen, but there are signs of heteroskedasticity and a scoche of non-linearity. Let's explore heteroskedasticity:

```{r}
#| label: food_insecurity_bptest
lmtest::bptest(lm1)
```

BP test is just barely showing significant result for heteroskedasticity. Let's try using robust standard errors:

```{r}
#| label: food_insecurity_robust
vcov <- vcovHC(lm1, type = 'HC')
coeftest(lm1, vcov. = vcov)
```

Robust standard errors brought economics closer, but none of our dimensions significantly predict food insecurity, nor does the model explain a meaningful amount of variance.  

# Life Expectancy

```{r}
#| label: life_exp
#| fig-width: 8
#| fig-height: 8
#| fig-align: center
#| out-width: 75%
lm2 <- lm(
  lifeExpectancy ~ economics + environment + health + production + social,
  data = dat
)
summary(lm2)
check_model(lm2)
```

The influential observations plot shows that record 48 (West Virginia) is poorly predicted, and has outsized influence on the model. 

It also looks like heteroskedasticity is a bigger issue here. Let's check it and use robust SEs again if it is:

```{r}
#| label: life_exp_bptest
lmtest::bptest(lm2)
vcov <- vcovHC(lm2, type = 'HC')
coeftest(lm2, vcov. = vcov)
```

When using robust errors, the health dimension falls off and we are left with only economics as a significant predictor of life expectancy. That being said, the model is explaining 35% of the variance in life expectancy, which is surprisingly strong. I suspect some strong omitted variable bias here, with GDP being a primary suspect. 

# Food Environment Index

```{r}
#| label: food_env
#| fig-width: 8
#| fig-height: 8
#| fig-align: center
#| out-width: 75%
lm3 <- lm(
  foodEnvironmentIndex ~ economics + environment + health + production + social,
  data = dat
)
summary(lm3)
bptest(lm3)
check_model(lm3)
```

The Food Environment Index Regression does not show heteroskedasticity, but may well have some non-linear relationships given the residual plots. Health and economics are significant predictors, with a pretty healthy $R^2$.

Let's try this one again with a random forest instead of linear model:

```{r}
#| label: food_env_split
#| output: false
pacman::p_load(
  caret,
  ranger,
  glmnet
)

# Split data 60/40
set.seed(42)
indices <- createDataPartition(dat$foodEnvironmentIndex, p = 0.60, list = FALSE)
training_data <- dat[indices, ]
testing_data <- dat[-indices,]

my_folds <- createFolds(training_data$foodEnvironmentIndex, k = 5, list = TRUE)

# Control
my_control <- trainControl(
  method = 'cv',
  number = 5,
  verboseIter = TRUE,
  index = my_folds
)

# Check for zero variance or near zero variance indicators
nearZeroVar(dat, names = TRUE, saveMetrics = TRUE)
# All clear
```

## GLMnet

```{r}
#| label: food_env_glmnet
#| output: false
set.seed(42)
food_env_glmnet <- train(
  foodEnvironmentIndex ~ economics + environment + health + production + social,
  data = training_data, 
  tuneGrid = expand.grid(
    alpha = seq(0.1, 1, length = 5),
    lambda = seq(0.0001, 0.1, length = 100)
  ),
  method = "glmnet",
  trControl = my_control,
  preProcess = c('zv', 'center', 'scale')
)
```


```{r}
#| fig-align: center
#| out-width: 75%
importance <- varImp(food_env_glmnet, scale = TRUE)
plot(importance)
```

## Random Forest

```{r}
#| label: food_env_rf
#| output: false
set.seed(42)
food_env_rf <- train(
  foodEnvironmentIndex ~ production + social + health + economics + environment,
  data = training_data, 
  tuneLength = 7,
  method = "ranger",
  trControl = my_control,
  importance = 'impurity'
)
```

OOB prediction error (MSE): `r food_env_rf$finalModel$prediction.error`

```{r}
#| out-width: 75%
#| fig-align: center
importance <- varImp(food_env_rf, scale = TRUE)
plot(importance)
```

# Happiness Score

```{r}
#| label: happiness
#| fig-width: 8
#| fig-height: 8
#| fig-align: center
#| out-width: 75%
lm4 <- lm(
  happinessScore ~ economics + environment + health + production + social,
  data = dat
)
summary(lm4)
check_model(lm4)
```

The model looks pretty reasonable here. The model does not explain a significant amount of variance on the whole, but economics still shows up as the only significant predictor of the validation variable.

# To Do

- Add GDP and population as covariates in all analyses.
- Add non-linear analyses to each analysis for which linear models are ill-suited
- Package regression outputs more cleanly with stargazer or kable