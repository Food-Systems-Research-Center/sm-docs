---
title: "Metric Distributions"
format:
  html:
    fig-dpi: 200
editor_options: 
  chunk_output_type: inline
warnings: false
---

Explore metric distributions before normalization. Use this to inform how we might want to deal with outliers or normalize data at the metric level. For now, we are leaving the metrics as is, taking arithmetic means, and saving all normalization and weighting for the indicator level.

```{r}
#| label: prep
#| echo: false
source('dev/get_setup.R')
get_setup()
```

Transforming our data from long format to wide and making sure everything came through alright.

```{r}
#| label: wrangle
pacman::p_load(
  dplyr,
  tidyr,
  tibble
)

# Get latest year function
source('dev/data_pipeline_functions.R')

# Load metrics data
sm_data <- readRDS('data/sm_data.rds')

# Load refined framework
raw_tree <- sm_data[['refined_tree']]

# Load refined framework
frame <- readRDS('data/frame.rds')


## Join with metadata to double check the resolution of our metrics
meta <- sm_data$metadata
# get_str(meta)

dat <- frame %>% 
  filter(variable_name != 'NONE') %>% 
  select(variable_name) %>% 
  left_join(meta, by = 'variable_name') %>% 
  unique()
# get_str(dat)

# Pull it from the actual metrics data
metrics <- sm_data$metrics %>% 
  filter(
    variable_name %in% frame$variable_name,
    fips %in% sm_data$state_key$state_code
  )
# get_str(metrics)

# Filter to latest year for each metric, and pivot wider
# Also removing census participation - don't really have data at state level
# Note to aggregate counties for this at some point
metrics_df <- metrics %>%
  mutate(
    value = ifelse(value == 'NaN', NA, value),
    value = str_remove_all(value, ','),
    value = as.numeric(value)
  ) %>%
  get_latest_year() %>% 
  pivot_wider(
    names_from = 'variable_name',
    values_from = 'value'
  ) %>% 
  unnest(cols = !fips) %>%
  unique()
# get_str(metrics_df)

# Let's get rid of the years so they are easier to work with
names(metrics_df) <- str_split_i(names(metrics_df), '_', 1)
# get_str(metrics_df)

# Save this for use in subsequent pages
saveRDS(metrics_df, 'data/metrics_df.rds')
```

# Distributions

Here we explore univariate distributions of each of our metrics. Highly skewed distributions might be good candidates for Box-Cox transformations or Winsorization. The figure below shows metrics with a skew \> 2 in red, while those with a skew \< 2 are in blue.

```{r}
#| label: metric_distributions
#| fig-cap: Distributions of metrics at the state level.
#| fig-height: 45
#| fig-width: 10
#| fig-align: center
pacman::p_load(
  ggplot2,
  purrr,
  ggpubr,
  psych
)

# Get skews of variables
skewed <- psych::describe(metrics_df[, -1]) %>% 
  as.data.frame() %>% 
  rownames_to_column('variable_name') %>% 
  select(variable_name, skew) %>% 
  filter(abs(skew) > 2) %>% 
  pull(variable_name)

plots <- map(names(metrics_df)[-1], \(var){
  # color based on skewness
  if (var %in% skewed) {
    fill <- 'red'
    color <- 'darkred'
  } else {
    fill <- 'lightblue'
    color <- 'royalblue'
  }
  
  # Make plot for variable
  metrics_df %>% 
    ggplot(aes(x = !!sym(var))) + 
    geom_density(
      fill = fill,
      color = color,
      alpha = 0.5
    ) +
    theme_classic() +
    theme(plot.margin = unit(c(rep(0.5, 4)), 'cm'))
}) 

# Arrange them in 4 columns
ggarrange(
  plotlist = plots,
  ncol = 4,
  nrow = 33
)

```

It seems most of our metrics fall along respectable somewhat-normal distributions. `{r} length(skewed)` metrics are skewed out of `{r ncol(df) - 1}` total. They include several variables related to local farm economies (agrotourism sales as a percentage of total sales, direct to consumer sales as a percentage of total sales, and value added sales as a percentage of total sales), as well as a couple of the TreeMap 2016 variables (dead standing carbon and live trees) and GHG emissions from agriculture (CH4 and CO2, with an honorable mention for N2O). Just about the whole collection of ERS metrics are skewed, including importts and exports, indemnities, and real estate expenses. 

There might be a case for transformations here, or it might make more sense to do it at the indicator level. Another option is to weight our metrics to take into account population, number of farms, acres of farmland, GDP, or some other appropriate variable for each metric. @bene2019GlobalMapIndicators used Box Cox transformations for highly skewed indicators before normalizing all indicators with Min Max transformations. 

Putting a pin in this to try it with raw metrics and transformed metrics and then to compare the two. 
