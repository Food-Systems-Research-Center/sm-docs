---
title: "Variable Selection and Regression"
format:
  html:
    fig-dpi: 200
editor_options: 
  chunk_output_type: console
warnings: false
---

```{r}
#| label: prep
#| warnings: false
#| echo: false
pacman::p_load(
  dplyr,
  purrr,
  conflicted,
  psych,
  readr, 
  stringr, 
  caret, 
  purrr, 
  DHARMa, 
  lmtest,
  snakecase,
  tibble,
  reactable
)

source('dev/get_reactable.r')

conflicted::conflicts_prefer(
 dplyr::select(),
 dplyr::filter(),
 dplyr::summarize(),
 dplyr::rename(),
 .quiet = TRUE
)
```

On this page we will take our min-max normalized, geometrically averaged scores, which look like the most reliable and approachable so far, and take a deeper dive into variable selection, regression, and PCA. From the dimension meetings, it sounds like we may have some indicators with a couple of metrics, and potentially others with dozens. Because of this, and because of our focus on developing sensible indicators, I think it will be best to do any weighting at the indicator level or above. This also reduces our variable count substantially in relation to our state count of 51, opening more doors for PCA.

It is worth emphasizing at the top that the metrics that are making up this secondary data framework are not a great representation of the system. There are some important holes, as well as a heap of metrics that are serving as rather uninspiring proxies. So, extrapolation of these results beyond the confines of the exercise is not recommended. The purpose here is to explore strengths and tradeoffs in methods for aggregating the data. As primary data come in and make up the bulk of the framework and secondary data are used to fill in the gaps, this should start becoming more interpretable. 

# Wrangling

First we will combine our indicator, index, and dimension values into one data frame to work with.

```{r}
#| label: wrangle
#| output: false

# Load data
scores <- readRDS('data/state_score_iterations.rds')
names(scores)

# Pull from raw minmax geometric
raw_minmax_geo <- scores$raw_minmax_geometric
get_str(raw_minmax_geo, 3)

# Rename columns to specify what level of framework they are
# then combine them into a single DF
# and remove the aggregate US or NewEng rows
dat <- imap(raw_minmax_geo, ~ {
  new_names <- c(
    paste0(
      str_sub(.y, end = 5),
      '_',
      names(.x[-length(.x)])
    ),
    'state'
  )
  .x %>% 
    setNames(c(new_names))
}) %>% 
  reduce(inner_join) %>% 
  select(state, everything()) %>% 
  filter(str_detect(state, '_', negate = TRUE))
get_str(dat)

# Save this for later use elsewhere
saveRDS(dat, 'data/raw_minmax_geo.rds')

# Get filtered frame, which shows which indicators belong to which dimensions
framework <- readRDS('data/filtered_frame.rds')
```

# PCA

Similar procedure to what we have been doing, but now we will just use our indicators, which have been aggregating with geometric means.

## Component Extraction

```{r}
#| label: extraction
#| warning: false
# Filter down to just indicators for PCA
pca_dat <- dat %>% 
  select(starts_with('indic')) %>% 
  setNames(c(str_remove(names(.), 'indic_'))) %>% 
  as.data.frame()
# get_str(pca_dat)

# Explore how many factors to extract
VSS(pca_dat, n = 8, fm = 'pc', rotate = 'Promax')
set.seed(42)
fa.parallel(pca_dat, fm = 'ml')
```

MAP suggests 6, VSS 2 or 3, PA suggests 5. Not half bad. I think we are justified to go with 5.

```{r}
#| label: scree
# Oblique rotations: promax, oblimin, simplimax, cluster
rotations <- c(
  'Promax',
  'oblimin',
  'simplimax',
  'cluster'
)
pca_outs <- map(rotations, ~ {
  pca_dat %>% 
    # scale() %>% 
    pca(nfactors = 5, rotate = .x)
}) %>% 
  setNames(c(rotations))

# Save a version of promax for preso?
png(
  filename = 'preso/plots/scree.png',
  width = 800,
  height = 600,
  units = 'px',
  res = 150
)
plot(
  pca_outs$simplimax$values,
  xlab = 'Number of Components',
  ylab = 'Eigen Values'
)
abline(h = 1)
dev.off()

# Now actually show it 
plot(
  pca_outs$simplimax$values,
  xlab = 'Number of Components',
  ylab = 'Eigen Values'
)
```

The scree plot makes a reasonably convincing case for 5 components, as the slope falls off substantially after the fifth.

## Run PCA

```{r}
#| label: pca_table_wrangle
pca_tables <- map(pca_outs, ~ {
  .x$loadings %>% 
    unclass() %>% 
    as.data.frame() %>% 
    select(order(colnames(.))) %>%
    mutate(
      across(everything(), ~ round(.x, 3)),
      across(everything(), ~ case_when(
        .x < 0.20 ~ '',
        .x >= 0.20 & .x < 0.32 ~ '.',
        .x >= 0.32 ~ as.character(.x),
        .default = NA
      ))
    ) %>% 
    rownames_to_column() %>% 
    rename(indicator = 1) %>% 
    mutate(
      dimension = framework$dimension[match(indicator, framework$indicator)]
    ) %>% 
    select(indicator, dimension, everything())
})
get_str(pca_tables)  

# Save it for preso
saveRDS(pca_tables, 'preso/data/pca_tables.rds')
```


```{r}
#| label: pca_reactable
#| class: centered
get_reactable(pca_tables$simplimax)
```

Add interpretation here []

# Regression

Now that we have a preliminary framework and set of scores, we can try regressing our dimension scores onto our indicators to see which indicators are the most highly associated with dimensions. We can use this information to help inform weighting. Since we have more indicators per dimension than are easy to interpret, we will use variable selection procedures with GLMnet and Random Forest regressions.

## Economics

### Linear Model

First we can try a plain old linear model to see how economics loads onto its indicators.

```{r}
#| label: econ_lm
# Reduce data down to dimen_economics and all indicators
econ_dat <- select(dat, dimen_economics, starts_with('indic')) %>% 
  setNames(c(names(.) %>% str_remove('indic_|dimen_')))
get_str(econ_dat)

lm <- lm(economics ~ ., data = econ_dat)
summary(lm)
```

We can see that most of the economics indicators (access to land, wealth and income distribution, income stability) are significant predictors, while operations diversification is close. But some surprises are access to culturally appropriate food (school food authorities serving culturally relevant food), housing supply and quality, as well as a few production indicators, like richness (crop diversity), production inputs, and value-added markets. Social connectedness from the social dimension also makes it on the list. The largest coefficients by a wide margin are for access to land and wealth and income distribution.

### Splitting Data

Here we split out data into a 60/40 training/test set for cross validation with GLMnet and Random Forest models. Note that we are pushing the limits of our sample size. But this should help protect against overfitting.

```{r}
#| label: split_econ
#| output: false
pacman::p_load(
  caret,
  ranger,
  glmnet
)

# Split data 60/40
set.seed(42)
indices <- createDataPartition(econ_dat$economics, p = 0.60, list = FALSE)
training_data <- econ_dat[indices, ]
testing_data <- econ_dat[-indices,]

my_folds <- createFolds(training_data$economics, k = 5, list = TRUE)

# Control
my_control <- trainControl(
  method = 'cv',
  number = 5,
  verboseIter = TRUE,
  index = my_folds
)

# Check for zero variance or near zero variance indicators
nearZeroVar(dat, names = TRUE, saveMetrics = TRUE)
# All clear
```

### GLMnet

Here we use a GLMnet to find an optimal balance between a ridge regression, which penalizes variables based on the magnitude of coefficients, and lasso regression, which adds a penalty based on the absolute value of coefficients. We use a tuning grid to find optimal values of alpha (0 = ridge, 1 = lasso) and lambda (the penalty parameter). Both this and the random forest model are particularly good at prediction, but also provide a metric for variable importance that can help us interpret our indicators.

```{r}
#| label: econ_glmnet
set.seed(42)
econ_glmnet <- train(
  economics ~ .,
  data = training_data, 
  tuneGrid = expand.grid(
    alpha = seq(0.1, 1, length = 5),
    lambda = seq(0.0001, 0.1, length = 100)
  ),
  method = "glmnet",
  trControl = my_control,
  preProcess = c('zv', 'center', 'scale')
)

importance <- varImp(econ_glmnet, scale = TRUE)
plot(importance)

# Predict
# p <- predict(econ_glmnet, testing_data)
# postResample(pred = p, obs = testing_data$economics)

```

The optimal hyperparameters from the tuning grid were alpha = 0.1 (mostly ridge regression) and lambda = 0.00313. The variable importance plot is on a relative scale of 0 (unimportant) to 100 (most important) in terms of predictive power. Curiously, it is showing that the value added market indicator from the production dimension is a better predictor of economics than any economics indicator.

### Random Forest

Now we can try a random forest, which is particularly good at handling non-linear relationships. Here we use the RMSE to determine the optimal combination of mtry (the number of variables selected at each node in the decision tree), the split rule, and the minimum node size.

```{r}
#| label: econ_rf
set.seed(42)
econ_rf <- train(
  economics ~ .,
  data = training_data, 
  tuneLength = 7,
  method = "ranger",
  trControl = my_control,
  importance = 'impurity'
)

# econ_rf
# plot(econ_rf)

importance <- varImp(econ_rf, scale = TRUE)
plot(importance)

# Predict
# p <- predict(model_mf, testing_data)
# postResample(pred = p, obs = testing_data$rebl_tpm)
```

The random forest model is also picking out the value-added market indicator as the best predictor of economics dimension scores, followed closely by operations diversification, wealth and income distribution, and income stability.

Very curious how value-added markets keep sticking out. The two metrics making up this indicator are both from NASS: the percentage of farms reporting value-added sales, and of those farms, the percentage of value-added sales out of total sales. 

# Next steps

Lots of directions to go from here, but the plan for now is:

-   Far more visualization and exploration of indicator distributions and normalization methods. Show how VT specifically and New England as a whole compare to national averages in each dimension, index, and key indicators.
-   Explore aggregation using PCA loadings as weights and compare this to our more simply aggregated current frameworks. Might make sense to try PCA within each dimension individually instead of all together, although doing them together does allow us to find unusual behavior of indicators.
-   Continue regression analysis across other four dimensions to help inform indicator weighting. Consider trying double-censored tobit regression in place of the linear model [@gomez-limon2010EmpiricalEvaluationAgricultural].
-   Sensitivity analysis - quantify the effect of different normalization, aggregation methods. Bootstrap confidence intervals around dimension scores. Quantify effect of changing the number of metrics under each indicator. 
