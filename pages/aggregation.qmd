---
title: "Metric Aggregation"
format:
  html:
    fig-dpi: 200
editor_options: 
  chunk_output_type: console
---

Exploring methods of aggregating data into index and dimension scores.

```{r}
#| context: setup
#| label: setup
pacman::p_load(
  dplyr,
  purrr,
  conflicted
)

conflicted::conflicts_prefer(
  dplyr::select(),
  dplyr::summarize(),
  dplyr::filter(),
  .quiet = TRUE
)
```

Explore the data we have, make sure it checks out

```{r}
#| label: explore
pacman::p_load(
  dplyr,
  stringr,
  purrr
)

# Explore our set of state level metrics from refined secondary framework
sm_data <- readRDS('data/sm_data.rds')
raw_tree <- sm_data[['refined_tree']]
get_str(raw_tree)

# Clean up the framework df 
frame <- raw_tree %>% 
  select(dimension:variable_name, resolution, use) %>% 
  filter(use == 'x') %>% 
  select(-use) %>% 
  mutate(
    metric = ifelse(
      str_length(metric) > 50,
      paste0(str_sub(metric, end = 50), '...'),
      metric
    )
  )
get_str(frame)


## Join with metadata to double check the resolution of our metrics
meta <- sm_data$metadata
get_str(meta)

dat <- frame %>% 
  select(variable_name) %>% 
  left_join(meta, by = 'variable_name') %>% 
  unique()
get_str(dat)

# check resolution
dat$resolution
str_detect(dat$resolution, 'state')
# Looks good, everything is at state level

# Pull it from the actual metrics data
sm_data$state_key %>% get_str
metrics <- sm_data$metrics %>% 
  filter(
    variable_name %in% frame$variable_name,
    fips %in% sm_data$state_key$state_code
  )
get_str(metrics)

```


```{r}
#| label: prep_pca
#| warnings: false
pacman::p_load(
  dplyr,
  tidyr,
  tibble
)

# Get latest year function
source('dev/data_pipeline_functions.R')
names(sm_data)
get_str(metrics)

# Filter to latest year for each metric, and pivot wider
# Also removing census participation - don't really have data at state level
# Note to aggregate counties for this at some point
metrics_df <- metrics %>%
  filter(variable_name != 'censusParticipation') %>% 
  mutate(
    value = ifelse(value == 'NaN', NA, value),
    value = as.numeric(value)
  ) %>%
  get_latest_year() %>% 
  pivot_wider(
    names_from = 'variable_name',
    values_from = 'value'
  ) %>% 
  # Note that we are getting dupes here for some reason. Explore this
  unnest(cols = !fips) %>% 
  unique()
get_str(metrics_df)

# Get rid of one variable that didn't come through properly
metrics_df$waterIrrSrcOffFarmExp_2023 <- NULL

get_str(metrics_df)
# Note that we have 75 variables and 51 states. can't have that many.
# 
```



```{r}
#| label: imputation
#| warnings: false
pacman::p_load(
  missForest,
  tibble
)

# Check for missing data
get_str(metrics_df)
skimr::skim(metrics_df)
sum(is.na(metrics_df))
# only 60 missing from value - that's not half bad

# Change fips from column to rowname so we can impute without losing it
metrics_df <- metrics_df %>% 
  column_to_rownames('fips')
# get_str(metrics_df)

# Impute missing variables
set.seed(42)
mf_out <- metrics_df %>%
  missForest(
    ntree = 200,
    mtry = 10,
    verbose = FALSE,
    variablewise = FALSE
  )
# get_str(mf_out)
mf_out$OOBerror
# NRMSE 0.5906362

# Save just imputed data
imp_dat <- mf_out$ximp

# Norming
imp_normed <- imp_dat %>% 
  mutate(across(everything(), ~ as.numeric(scale(.x, scale = TRUE, center = TRUE))))
get_str(imp_normed)
```

```{r}
#| label: extraction
#| warning: false
pacman::p_load(
  psych,
  car
)
cor_matrix <- cor(imp_normed, use = "pairwise.complete.obs")
cor_matrix <- cor(test, use = "pairwise.complete.obs")
print(cor_matrix)

# Check high correlations
threshold <- 0.8
high_corr_indices <- which(abs(cor_matrix) > threshold & abs(cor_matrix) < 1, arr.ind = TRUE)

# Extract variable pairs and correlations
high_corr_pairs <- data.frame(
  Var1 = rownames(cor_matrix)[high_corr_indices[, 1]],
  Var2 = colnames(cor_matrix)[high_corr_indices[, 2]],
  Correlation = cor_matrix[high_corr_indices]
)

# Remove duplicate pairs
high_corr_pairs <- high_corr_pairs[high_corr_pairs$Var1 < high_corr_pairs$Var2, ]

# Check high correlations
high_corr_pairs %>% arrange(abs(Correlation))
# Lets' ditch:
# prematureAgeAdjustedMortality_2024 (0.97 with life expectancy)
# forestStandHeight_2016 (0.97 with forestCarbonDeadDown)
# foodEnvironmentIndex_2024 (cors with several variables)
# Ditch all food insecurity except foodInsecurity_2024 (county health rankings)
# incomeInequality_2024 (already have gini)
# forestLiveTreeVolume_2016 (varies with all other forest variables)
# medianAcresPF_2022 (already have acres per farm)
# 4 medianEarn variables - collinear with each other

## Remove high collinear variables
imp_normed <- imp_normed %>% 
  select(-c(
    prematureAgeAdjustedMortality_2024,
    forestStandHeight_2016,
    foodEnvironmentIndex_2024,
    foodInsecChild_2021,
    foodInsecOverall_2021,
    incomeInequality_2024,
    forestLiveTreeVolume_2016,
    medianAcresPF_2022,
    starts_with('medianEarn'),
    landValPF_2022,
    starts_with('forestCarbonDead'),
    womenEarnPercMaleFood_2021
  ))
get_str(imp_normed)

# Check VIF to find out where the problem is
vif_values <- lm(gini_2022 ~ ., data = imp_normed)
summary(vif_values)
vif_values <- vif(lm(gini_2022 ~ ., data = imp_normed))
print(vif_values)


VSS(imp_normed, fm = 'pc')
fa.parallel(imp_dat)
pca_out <- pca(imp_dat, nfactors = 3, rotate = 'varimax')
plot(pca_out$values)
abline(h = 1)
```

```{r}
#| label: pca
#| warnings: false
# Test it with 49 variables to see if that is the problem
get_str(imp_normed)
small_set <- select(imp_normed, 1:50)
get_str(small_set)

# 
VSS(small_set, n = 10, fm = 'pc')
VSS(small_set)
fa_out <- fa.parallel(small_set, fa = 'pc')
fa.parallel(small_set)

pca(small_set, nfactors = 10, rotate = 'varimax')
```

