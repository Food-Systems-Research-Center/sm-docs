---
title: "Metric Aggregation"
format:
  html:
    fig-dpi: 200
editor_options: 
  chunk_output_type: console
---

Exploring methods of aggregating data into index and dimension scores.

```{r}
#| context: setup
#| label: setup
pacman::p_load(
  dplyr,
  purrr,
  conflicted
)

conflicted::conflicts_prefer(
  dplyr::select(),
  dplyr::summarize(),
  dplyr::filter(),
  .quiet = TRUE
)
```

# Explore

Explore the data we have, make sure it checks out

```{r}
#| label: explore
pacman::p_load(
  dplyr,
  stringr,
  purrr
)

# Explore our set of state level metrics from refined secondary framework
sm_data <- readRDS('data/sm_data_slim.rds')
raw_tree <- sm_data[['refined_tree']]
# get_str(raw_tree)

# Clean up the framework df 
frame <- raw_tree %>% 
  select(dimension:variable_name, resolution, use) %>% 
  filter(use == 'x') %>% 
  select(-use) %>% 
  mutate(
    metric = ifelse(
      str_length(metric) > 50,
      paste0(str_sub(metric, end = 50), '...'),
      metric
    )
  )
# get_str(frame)


## Join with metadata to double check the resolution of our metrics
meta <- sm_data$metadata
# get_str(meta)

dat <- frame %>% 
  select(variable_name) %>% 
  left_join(meta, by = 'variable_name') %>% 
  unique()
# get_str(dat)

# check resolution
# dat$resolution
# str_detect(dat$resolution, 'state')
# Looks good, everything is at state level

# Pull it from the actual metrics data
# sm_data$state_key %>% get_str
metrics <- sm_data$metrics %>% 
  filter(
    variable_name %in% frame$variable_name,
    fips %in% sm_data$state_key$state_code
  )
get_str(metrics)

```

# Prep

Get latest years of each metric, check out missing data, make sure nothing is broken.

```{r}
#| label: prep
#| warnings: false
pacman::p_load(
  dplyr,
  tidyr,
  tibble
)

# Get latest year function
source('dev/data_pipeline_functions.R')
names(sm_data)
get_str(metrics)

# Filter to latest year for each metric, and pivot wider
# Also removing census participation - don't really have data at state level
# Note to aggregate counties for this at some point
metrics_df <- metrics %>%
  filter(variable_name != 'censusParticipation') %>% 
  mutate(
    value = ifelse(value == 'NaN', NA, value),
    value = as.numeric(value)
  ) %>%
  get_latest_year() %>% 
  pivot_wider(
    names_from = 'variable_name',
    values_from = 'value'
  ) %>% 
  # Note that we are getting dupes here for some reason. Explore this
  unnest(cols = !fips) %>% 
  unique()
get_str(metrics_df)

# Get rid of one variable that didn't come through properly
metrics_df$waterIrrSrcOffFarmExp_2023 <- NULL

get_str(metrics_df)
# Note that we have 75 variables and 51 states. Can't do PCA with that

# Let's get rid of the years so they are easier to work with
names(metrics_df) <- str_split_i(names(metrics_df), '_', 1)
```

# Imputation

```{r}
#| label: imputation
#| warnings: false
pacman::p_load(
  missForest,
  tibble
)

# Check for missing data
skimr::skim(metrics_df)
sum(is.na(metrics_df))/(nrow(metrics_df)*(ncol(metrics_df) - 1)) * 100
# 1.59% missing data. 
# only 60 missing from value - that's not half bad

# Change fips from column to rowname so we can impute without losing it
metrics_df <- metrics_df %>% 
  column_to_rownames('fips')
# get_str(metrics_df)

# Impute missing variables
set.seed(42)
mf_out <- metrics_df %>%
  missForest(
    ntree = 200,
    mtry = 10,
    verbose = FALSE,
    variablewise = FALSE
  )
# get_str(mf_out)
mf_out$OOBerror
# NRMSE 0.5906362

# Check missing again
skimr::skim(mf_out$ximp)
# Looks good

# Save just imputed data
imp_dat <- mf_out$ximp
```

# Normalization

Normalizing in three ways: min max, box cox, and z-score standardization. (Could also consider Winsorizing?) Results will be saved to a list of three normalized datasets so we can compare outcomes of each one.

```{r}
#| label: normalization
#| warnings: false
pacman::p_load(
  forecast
)

# List of results
normed <- list()
get_str(imp_dat)

# Z scores
normed$zscore <- imp_dat %>% 
  mutate(across(everything(), ~ as.numeric(scale(.x, scale = TRUE, center = TRUE))))

# Min Max
min_max <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
normed$minmax <- imp_dat %>% 
  mutate(across(everything(), min_max))

# Box Cox. Adding 1 as constant to remove zeroes
normed$boxcox <- imp_dat %>% 
  mutate(across(everything(), ~ BoxCox(.x + 1, lambda = 'auto')))

map(normed, get_str)
```

# Directional Values

Here, we are assuming that each metric is has a directional value. Either more of it is better, or less of it is better. This is rather problematic in that just about any metric becomes negative with too much of it. What might make more sense in the long run would be to consult the expertise of our teams and develop acceptable ranges for some metrics once they are settled. Still, just about every sustainability indicator framework does some variation of this one-way value system [@schneider2023StateFoodSystems; @bene2019GlobalMapIndicators; @nicolettiSummaryIndicatorsProduct2000; @jacobi2020NewUnderstandingEvaluation; @gomez-limon2010EmpiricalEvaluationAgricultural]. Alas, for now, we will invert variables in each of the transformed datasets as necessary so that larger numbers are more sustainable, and smaller numbers are less sustainable.

```{r eval=FALSE}
#| label: directional_values
#| warnings: false
map(normed, get_str)

# Explore variables
names(normed[[1]])
# Higher numbers should be better. Reverse metrics that are the opposite, where lower numbers are better:
reverse <- c(
  'unemploymentRate',
  'gini',
  'lowBirthweight',
  'teenBirths',
  'uninsured',
  'incomeInequality',
  'injuryDeaths',
  'drinkingWaterViolations',
  'prematureAgeAgjustedMortality',
  'infantMortality',
  'foodInsecurity',
  'drugOverdoseDeaths',
  'disconnectedYouth',
  'residentialSegregationBlackWhite',
  'motorVehicleCrashDeaths',
  'severeHousingCostBurden',
  'schoolSegregation',
  'foodInsecOverall',
  'foodInsecChild',
  'droughtMeanPercArea',
  'expChemicalPct',
  'waterIrrSrcOffFarmExpPerAcreFt',
  'ageProducers' # Could be better to use age diversity?
)

# Some are unclear - without clear direction, better to remove:
remove <- c(
  'vacancyRate',
  'salesAnimalPctSales'
)

# Remove from all three datasets
normed <- map(normed, ~ {
  .x %>% 
    select(-matches(remove))
})
map(normed, get_str)

## Do reversals separately for each dataset depending on normalization method
names(normed)

# For z-scores, multiply by -1 
normed$zscore <- normed$zscore %>% 
  mutate(across(matches(reverse), ~ .x * -1))

# For min max, subtract from 1
normed$minmax <- normed$minmax %>% 
  mutate(across(matches(reverse), ~ 1 - .x))

# For BoxCox, multiply by -1, then subtract the minimum
# This should preserve rank and transformation
normed$boxcox <- normed$boxcox %>% 
  mutate(across(matches(reverse), ~ (.x * -1) - min(.x * -1)))

map(normed, get_str)
```

# Aggregation

Here we are combining values in each indicator, index, and dimension using several different methods: arithmetic means, geometric means, and rank order. We might also consider PCA here, but the n:p ratio is not in our favor for PCA as we have more metrics than states.

## Rank Order?

First we will try just ranking states relative to others, as was done by in the Food Systems Global Countdown [@schneider2023StateFoodSystems].

## Indicator Means

This one is pretty straight forward - just take the arithmetic mean of metrics in each indicator, indicators in each index, and indices in each dimension. This is a compensable method, where a good score in one index indicator will make up for a bad score in another.

```{r}
#| label: arithmetic_means
#| warnings: false
# We need to attach these back to framework from metadata
# Filter frame from earlier down to our current metrics
filtered_frame <- frame %>% 
  filter(variable_name %in% names(normed[[1]])) %>% 
  select(variable_name, indicator, index, dimension)
get_str(filtered_frame)

# Make a list where we hold scores for indicators, indices, and dimensions
scores <- list()

# Function for geometric mean
geometric_mean = function(x, na.rm = TRUE){
  exp(sum(log(x[x > 0]), na.rm = na.rm) / length(x))
}

# Get indicator scores across all three normalization methods
indicator_scores <- map(normed, \(df) {
  
  # For each df, calculate indicator means
  indicators_out <- map(unique(filtered_frame$indicator), \(ind) {
  
    # Column name based on indicator
    # ind_snake <- snakecase::to_snake_case(ind)
    ind_snake <- ind
    # ind_arithmetic <- paste0(ind_snake, '_arithmetic')
    # ind_geometric <- paste0(ind_snake, '_geometric')
    
    # Split into groups by indicator, with one or more metrics each
    variables <- filtered_frame %>% 
      filter(indicator == ind) %>% 
      pull(variable_name) %>% 
      unique()
    indicator_metrics <- df %>% 
      select(all_of(variables))
    
    # Get arithmetic and geo means for each indicator
    # indicator_metrics %>%
    #   rowwise() %>%
    #   mutate(
    #     !!sym(ind_arithmetic) := mean(c_across(everything())),
    #     !!sym(ind_geometric) := geometric_mean(c_across(everything()))
    #   ) %>%
    #   select(!!sym(ind_arithmetic), !!sym(ind_geometric))
    dfs <- list()
    dfs$arithmetic <- indicator_metrics %>%
      rowwise() %>%
      mutate(
        !!sym(ind_snake) := mean(c_across(everything())),
      ) %>%
      select(!!sym(ind_snake))
    dfs$geometric <- indicator_metrics %>% 
      rowwise() %>% 
      mutate(
        !!sym(ind_snake) := geometric_mean(c_across(everything())),
      ) %>%
      select(!!sym(ind_snake))
    return(dfs) 
  })
  
  # Rearrange so we put each aggregation method (arith, geo) together
  norm_out <- list()
  norm_out$arithmetic <- map(indicators_out, ~ {
    .x[grep("arithmetic", names(.x))]
  }) %>% 
    bind_cols()
  norm_out$geometric <- map(indicators_out, ~ {
    .x[grep("geometric", names(.x))]
  }) %>% 
    bind_cols()
  return(norm_out) 
})
  
get_str(indicator_scores, 4)
```

## Index Means

```{r}
#| label: index_means
#| warnings: false
# For each set of indicator scores, calculate index scores
get_str(indicator_scores, 4)
indices <- unique(filtered_frame$index)

# Choose aggregation function based on agg_type
agg_function <- function(x, agg_type) {
   if (agg_type == 'geometric') {
    geometric_mean(x)
  } else if (agg_type == 'arithmetic') {
    mean(x)
  }
}

index_scores <- map(indicator_scores, \(norm_type) {
  imap(norm_type, \(agg_df, agg_type) {
    map(indices, \(index_) {
      # Get names of indicators for this index
      index_indicators <- filtered_frame %>% 
        filter(index == index_) %>% 
        pull(indicator) %>% 
        unique()
      # Get DF of indicators for this index
      index_indicator_df <- agg_df %>% 
        select(all_of(index_indicators))
      # Get arithmetic or geometric mean, based on agg_type
      index_indicator_df %>% 
        rowwise() %>% 
        # mutate(mean = across(everything(), agg_function(agg_type)))
        mutate(!!sym(index_) := agg_function(c_across(everything()), agg_type)) %>% 
        select(!!sym(index_))
    }) %>% 
      bind_cols()
  })
})
# get_str(index_scores, 4)
```

# Dimension Means

```{r}
#| label: dimension_means
#| warnings: false
get_str(index_scores, 4)

# Same process for dimensions
dimensions <- unique(filtered_frame$dimension)

dimension_scores <- map(index_scores, \(norm_type) {
  imap(norm_type, \(agg_df, agg_type) {
    map(dimensions, \(dimension_) {
      # Get names of indices for this dimension
      dimension_indices <- filtered_frame %>% 
        filter(dimension == dimension_) %>% 
        pull(index) %>% 
        unique()
      # Get DF of indice for this dimension
      dimension_index_df <- agg_df %>% 
        select(all_of(dimension_indices))
      # Get arithmetic or geometric mean, based on agg_type
      dimension_index_df %>% 
        rowwise() %>% 
        mutate(!!sym(dimension_) := agg_function(
          c_across(everything()), 
          agg_type
        )) %>% 
        select(!!sym(dimension_))
    }) %>% 
      bind_cols()
  })
})
# get_str(dimension_scores, 4)
```

# Wrangle

Organize arithmetic and geometric means for each level of the framework (indicator, index, dimension) in a way that is easier to work with.

```{r}
#| label: wrangle
#| warnings: false
get_str(indicator_scores, 4)
get_str(index_scores, 4)
get_str(dimension_scores, 4)

# Want to end up with 6 lists: 3 norm types * 2 mean types
# Put them all together in one list to work with
all_scores <- mget(c(
  'indicator_scores',
  'index_scores',
  'dimension_scores'
))
get_str(all_scores, 3)

# Function to pull out the pieces we want
# Also put state names back in as a column and with real names, not codes
get_output <- function(norm_type, agg_type) {
  # Get list of each df (dimension, index, indicator) for combo
  dfs <- all_scores %>% 
    map(\(level) level[[norm_type]]) %>% 
    map(\(norm) norm[[agg_type]])
  # Get state back into a proper column for each df
  out <- map(dfs, ~ {
    .x %>% 
      rownames_to_column('state_code') %>% 
      # Leading zeroes lost from rowname transition. Put them back here
      mutate(state_code = case_when(
        str_length(state_code) == 1 ~ paste0('0', state_code),
        .default = state_code
      )) %>% 
      left_join(select(sm_data$state_key, state, state_code)) %>% 
      select(-state_code)
  })
  return(out)
}

# All combinations, also a name
combos <- expand.grid(
  c('zscore', 'minmax', 'boxcox'), 
  c('arithmetic', 'geometric')
) %>% 
  mutate(name = paste0(Var1, '_', Var2))

# Map to pull them all out
final_scores <- map2(combos[[1]], combos[[2]], ~ {
  get_output(.x, .y)
}) %>% 
  setNames(c(combos$name))
get_str(final_scores, 3)
get_str(final_scores, 4)

# Save this for use elsewhere
saveRDS(final_scores, 'data/state_score_iterations.rds')
```

This gives us a list of 6 elements, one for each combination of normalization method and aggregation method. Each element has three data frames, one for indicator, index, and dimension. Now we can compare these 6 outputs to see how the methodological differences affect scores and ranks.

Could consider aggregating the 6 New England states here by population as well? Or by arithmetic / geometric mean. TBD

# PCA

## Extraction

```{r eval=FALSE}
#| label: extraction
#| warning: false
pacman::p_load(
  psych,
  car
)
cor_matrix <- cor(imp_normed, use = "pairwise.complete.obs")
cor_matrix <- cor(test, use = "pairwise.complete.obs")
print(cor_matrix)

# Check high correlations
threshold <- 0.8
high_corr_indices <- which(abs(cor_matrix) > threshold & abs(cor_matrix) < 1, arr.ind = TRUE)

# Extract variable pairs and correlations
high_corr_pairs <- data.frame(
  Var1 = rownames(cor_matrix)[high_corr_indices[, 1]],
  Var2 = colnames(cor_matrix)[high_corr_indices[, 2]],
  Correlation = cor_matrix[high_corr_indices]
)

# Remove duplicate pairs
high_corr_pairs <- high_corr_pairs[high_corr_pairs$Var1 < high_corr_pairs$Var2, ]

# Check high correlations
high_corr_pairs %>% arrange(abs(Correlation))
# Lets' ditch:
# prematureAgeAdjustedMortality_2024 (0.97 with life expectancy)
# forestStandHeight_2016 (0.97 with forestCarbonDeadDown)
# foodEnvironmentIndex_2024 (cors with several variables)
# Ditch all food insecurity except foodInsecurity_2024 (county health rankings)
# incomeInequality_2024 (already have gini)
# forestLiveTreeVolume_2016 (varies with all other forest variables)
# medianAcresPF_2022 (already have acres per farm)
# 4 medianEarn variables - collinear with each other

## Remove high collinear variables
imp_normed <- imp_normed %>% 
  select(-c(
    prematureAgeAdjustedMortality_2024,
    forestStandHeight_2016,
    foodEnvironmentIndex_2024,
    foodInsecChild_2021,
    foodInsecOverall_2021,
    incomeInequality_2024,
    forestLiveTreeVolume_2016,
    medianAcresPF_2022,
    starts_with('medianEarn'),
    landValPF_2022,
    starts_with('forestCarbonDead'),
    womenEarnPercMaleFood_2021
  ))
get_str(imp_normed)

# Check VIF to find out where the problem is
vif_values <- lm(gini_2022 ~ ., data = imp_normed)
summary(vif_values)
vif_values <- vif(lm(gini_2022 ~ ., data = imp_normed))
print(vif_values)


VSS(imp_normed, fm = 'pc')
fa.parallel(imp_dat)
pca_out <- pca(imp_dat, nfactors = 3, rotate = 'varimax')
plot(pca_out$values)
abline(h = 1)
```

## Run PCA

```{r eval=FALSE}
#| label: pca
#| warnings: false
# Test it with 49 variables to see if that is the problem
get_str(imp_normed)
small_set <- select(imp_normed, 1:50)
get_str(small_set)

# 
VSS(small_set, n = 10, fm = 'pc')
VSS(small_set)
fa_out <- fa.parallel(small_set, fa = 'pc')
fa.parallel(small_set)

pca(small_set, nfactors = 10, rotate = 'varimax')
```
